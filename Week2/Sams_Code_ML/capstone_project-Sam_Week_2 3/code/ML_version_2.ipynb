{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c130683c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dependencies\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c84493f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIPS</th>\n",
       "      <th>3/31/21</th>\n",
       "      <th>AREA_SQMI</th>\n",
       "      <th>E_TOTPOP</th>\n",
       "      <th>E_HU</th>\n",
       "      <th>E_HH</th>\n",
       "      <th>E_POV</th>\n",
       "      <th>E_UNEMP</th>\n",
       "      <th>E_PCI</th>\n",
       "      <th>E_NOHSDP</th>\n",
       "      <th>...</th>\n",
       "      <th>Hopefulness</th>\n",
       "      <th>Income Per Capita</th>\n",
       "      <th>Neuroticism</th>\n",
       "      <th>Openness</th>\n",
       "      <th>Religiosity</th>\n",
       "      <th>Risk Taking</th>\n",
       "      <th>Selflessness</th>\n",
       "      <th>Tolerance</th>\n",
       "      <th>Work Ethic</th>\n",
       "      <th>dem_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>6589</td>\n",
       "      <td>594.443459</td>\n",
       "      <td>55200.0</td>\n",
       "      <td>23315.0</td>\n",
       "      <td>21115.0</td>\n",
       "      <td>8422.0</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>29372.0</td>\n",
       "      <td>4204.0</td>\n",
       "      <td>...</td>\n",
       "      <td>91.163142</td>\n",
       "      <td>26168.0</td>\n",
       "      <td>77.925476</td>\n",
       "      <td>78.222354</td>\n",
       "      <td>91.106719</td>\n",
       "      <td>53.333333</td>\n",
       "      <td>82.142857</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>60.380952</td>\n",
       "      <td>27.018365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1003</td>\n",
       "      <td>20505</td>\n",
       "      <td>1589.793007</td>\n",
       "      <td>208107.0</td>\n",
       "      <td>111945.0</td>\n",
       "      <td>78622.0</td>\n",
       "      <td>21653.0</td>\n",
       "      <td>4343.0</td>\n",
       "      <td>31203.0</td>\n",
       "      <td>14310.0</td>\n",
       "      <td>...</td>\n",
       "      <td>82.484017</td>\n",
       "      <td>28069.0</td>\n",
       "      <td>77.232120</td>\n",
       "      <td>80.086368</td>\n",
       "      <td>71.771566</td>\n",
       "      <td>67.272980</td>\n",
       "      <td>75.586018</td>\n",
       "      <td>66.983549</td>\n",
       "      <td>70.972246</td>\n",
       "      <td>22.409030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1005</td>\n",
       "      <td>2227</td>\n",
       "      <td>885.001636</td>\n",
       "      <td>25782.0</td>\n",
       "      <td>11937.0</td>\n",
       "      <td>9186.0</td>\n",
       "      <td>6597.0</td>\n",
       "      <td>918.0</td>\n",
       "      <td>18461.0</td>\n",
       "      <td>4901.0</td>\n",
       "      <td>...</td>\n",
       "      <td>61.927181</td>\n",
       "      <td>17249.0</td>\n",
       "      <td>80.375206</td>\n",
       "      <td>78.783778</td>\n",
       "      <td>73.657368</td>\n",
       "      <td>76.066481</td>\n",
       "      <td>78.753019</td>\n",
       "      <td>65.170377</td>\n",
       "      <td>68.704105</td>\n",
       "      <td>45.788173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1007</td>\n",
       "      <td>2542</td>\n",
       "      <td>622.461089</td>\n",
       "      <td>22527.0</td>\n",
       "      <td>9161.0</td>\n",
       "      <td>6840.0</td>\n",
       "      <td>2863.0</td>\n",
       "      <td>658.0</td>\n",
       "      <td>20199.0</td>\n",
       "      <td>2650.0</td>\n",
       "      <td>...</td>\n",
       "      <td>85.258871</td>\n",
       "      <td>18988.0</td>\n",
       "      <td>80.813736</td>\n",
       "      <td>77.837027</td>\n",
       "      <td>69.974652</td>\n",
       "      <td>75.136154</td>\n",
       "      <td>76.929754</td>\n",
       "      <td>69.859503</td>\n",
       "      <td>67.931677</td>\n",
       "      <td>20.698280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1009</td>\n",
       "      <td>6444</td>\n",
       "      <td>644.830460</td>\n",
       "      <td>57645.0</td>\n",
       "      <td>24222.0</td>\n",
       "      <td>20600.0</td>\n",
       "      <td>8220.0</td>\n",
       "      <td>909.0</td>\n",
       "      <td>22656.0</td>\n",
       "      <td>7861.0</td>\n",
       "      <td>...</td>\n",
       "      <td>79.492703</td>\n",
       "      <td>21033.0</td>\n",
       "      <td>78.764620</td>\n",
       "      <td>78.193105</td>\n",
       "      <td>92.045455</td>\n",
       "      <td>57.603815</td>\n",
       "      <td>79.307632</td>\n",
       "      <td>64.953288</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>9.569378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3053</th>\n",
       "      <td>56037</td>\n",
       "      <td>4022</td>\n",
       "      <td>10426.975725</td>\n",
       "      <td>44117.0</td>\n",
       "      <td>19628.0</td>\n",
       "      <td>15871.0</td>\n",
       "      <td>5237.0</td>\n",
       "      <td>1213.0</td>\n",
       "      <td>32624.0</td>\n",
       "      <td>2549.0</td>\n",
       "      <td>...</td>\n",
       "      <td>82.403142</td>\n",
       "      <td>30945.0</td>\n",
       "      <td>79.384759</td>\n",
       "      <td>79.347081</td>\n",
       "      <td>68.147062</td>\n",
       "      <td>73.938691</td>\n",
       "      <td>76.390464</td>\n",
       "      <td>67.420658</td>\n",
       "      <td>70.956334</td>\n",
       "      <td>22.894957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3054</th>\n",
       "      <td>56039</td>\n",
       "      <td>3609</td>\n",
       "      <td>3996.844622</td>\n",
       "      <td>23059.0</td>\n",
       "      <td>13680.0</td>\n",
       "      <td>9158.0</td>\n",
       "      <td>1619.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>53703.0</td>\n",
       "      <td>958.0</td>\n",
       "      <td>...</td>\n",
       "      <td>84.036899</td>\n",
       "      <td>46499.0</td>\n",
       "      <td>71.547359</td>\n",
       "      <td>80.522872</td>\n",
       "      <td>65.399695</td>\n",
       "      <td>79.598153</td>\n",
       "      <td>79.698193</td>\n",
       "      <td>70.877600</td>\n",
       "      <td>70.938645</td>\n",
       "      <td>66.599040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3055</th>\n",
       "      <td>56041</td>\n",
       "      <td>2128</td>\n",
       "      <td>2081.719807</td>\n",
       "      <td>20609.0</td>\n",
       "      <td>8972.0</td>\n",
       "      <td>7735.0</td>\n",
       "      <td>2552.0</td>\n",
       "      <td>614.0</td>\n",
       "      <td>27009.0</td>\n",
       "      <td>934.0</td>\n",
       "      <td>...</td>\n",
       "      <td>84.089095</td>\n",
       "      <td>25636.0</td>\n",
       "      <td>78.771570</td>\n",
       "      <td>77.859042</td>\n",
       "      <td>67.603416</td>\n",
       "      <td>69.705859</td>\n",
       "      <td>73.332067</td>\n",
       "      <td>67.404487</td>\n",
       "      <td>69.299391</td>\n",
       "      <td>16.819960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3056</th>\n",
       "      <td>56043</td>\n",
       "      <td>891</td>\n",
       "      <td>2238.672972</td>\n",
       "      <td>8129.0</td>\n",
       "      <td>3868.0</td>\n",
       "      <td>3422.0</td>\n",
       "      <td>984.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>27556.0</td>\n",
       "      <td>590.0</td>\n",
       "      <td>...</td>\n",
       "      <td>87.485019</td>\n",
       "      <td>26325.0</td>\n",
       "      <td>76.249370</td>\n",
       "      <td>77.658224</td>\n",
       "      <td>67.412774</td>\n",
       "      <td>82.820701</td>\n",
       "      <td>78.925326</td>\n",
       "      <td>74.628788</td>\n",
       "      <td>70.050103</td>\n",
       "      <td>16.145833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3057</th>\n",
       "      <td>56045</td>\n",
       "      <td>633</td>\n",
       "      <td>2398.003891</td>\n",
       "      <td>7100.0</td>\n",
       "      <td>3565.0</td>\n",
       "      <td>3062.0</td>\n",
       "      <td>1171.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>29152.0</td>\n",
       "      <td>389.0</td>\n",
       "      <td>...</td>\n",
       "      <td>80.975991</td>\n",
       "      <td>29493.0</td>\n",
       "      <td>79.961075</td>\n",
       "      <td>76.977099</td>\n",
       "      <td>71.535519</td>\n",
       "      <td>75.406846</td>\n",
       "      <td>76.730879</td>\n",
       "      <td>68.422269</td>\n",
       "      <td>69.413532</td>\n",
       "      <td>10.112360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3058 rows × 104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       FIPS  3/31/21     AREA_SQMI  E_TOTPOP      E_HU     E_HH    E_POV  \\\n",
       "0      1001     6589    594.443459   55200.0   23315.0  21115.0   8422.0   \n",
       "1      1003    20505   1589.793007  208107.0  111945.0  78622.0  21653.0   \n",
       "2      1005     2227    885.001636   25782.0   11937.0   9186.0   6597.0   \n",
       "3      1007     2542    622.461089   22527.0    9161.0   6840.0   2863.0   \n",
       "4      1009     6444    644.830460   57645.0   24222.0  20600.0   8220.0   \n",
       "...     ...      ...           ...       ...       ...      ...      ...   \n",
       "3053  56037     4022  10426.975725   44117.0   19628.0  15871.0   5237.0   \n",
       "3054  56039     3609   3996.844622   23059.0   13680.0   9158.0   1619.0   \n",
       "3055  56041     2128   2081.719807   20609.0    8972.0   7735.0   2552.0   \n",
       "3056  56043      891   2238.672972    8129.0    3868.0   3422.0    984.0   \n",
       "3057  56045      633   2398.003891    7100.0    3565.0   3062.0   1171.0   \n",
       "\n",
       "      E_UNEMP    E_PCI  E_NOHSDP  ...  Hopefulness  Income Per Capita  \\\n",
       "0      1065.0  29372.0    4204.0  ...    91.163142            26168.0   \n",
       "1      4343.0  31203.0   14310.0  ...    82.484017            28069.0   \n",
       "2       918.0  18461.0    4901.0  ...    61.927181            17249.0   \n",
       "3       658.0  20199.0    2650.0  ...    85.258871            18988.0   \n",
       "4       909.0  22656.0    7861.0  ...    79.492703            21033.0   \n",
       "...       ...      ...       ...  ...          ...                ...   \n",
       "3053   1213.0  32624.0    2549.0  ...    82.403142            30945.0   \n",
       "3054    210.0  53703.0     958.0  ...    84.036899            46499.0   \n",
       "3055    614.0  27009.0     934.0  ...    84.089095            25636.0   \n",
       "3056    253.0  27556.0     590.0  ...    87.485019            26325.0   \n",
       "3057    117.0  29152.0     389.0  ...    80.975991            29493.0   \n",
       "\n",
       "      Neuroticism   Openness  Religiosity  Risk Taking  Selflessness  \\\n",
       "0       77.925476  78.222354    91.106719    53.333333     82.142857   \n",
       "1       77.232120  80.086368    71.771566    67.272980     75.586018   \n",
       "2       80.375206  78.783778    73.657368    76.066481     78.753019   \n",
       "3       80.813736  77.837027    69.974652    75.136154     76.929754   \n",
       "4       78.764620  78.193105    92.045455    57.603815     79.307632   \n",
       "...           ...        ...          ...          ...           ...   \n",
       "3053    79.384759  79.347081    68.147062    73.938691     76.390464   \n",
       "3054    71.547359  80.522872    65.399695    79.598153     79.698193   \n",
       "3055    78.771570  77.859042    67.603416    69.705859     73.332067   \n",
       "3056    76.249370  77.658224    67.412774    82.820701     78.925326   \n",
       "3057    79.961075  76.977099    71.535519    75.406846     76.730879   \n",
       "\n",
       "      Tolerance  Work Ethic    dem_pct  \n",
       "0     70.000000   60.380952  27.018365  \n",
       "1     66.983549   70.972246  22.409030  \n",
       "2     65.170377   68.704105  45.788173  \n",
       "3     69.859503   67.931677  20.698280  \n",
       "4     64.953288   76.000000   9.569378  \n",
       "...         ...         ...        ...  \n",
       "3053  67.420658   70.956334  22.894957  \n",
       "3054  70.877600   70.938645  66.599040  \n",
       "3055  67.404487   69.299391  16.819960  \n",
       "3056  74.628788   70.050103  16.145833  \n",
       "3057  68.422269   69.413532  10.112360  \n",
       "\n",
       "[3058 rows x 104 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read in merged and cleaned data\n",
    "df = pd.read_csv('../modified_data/pol_svi_sc_merged.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3182e6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set index to FIPS\n",
    "df = df.set_index(df['FIPS'])\n",
    "df= df.drop('FIPS', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77087806",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename target column\n",
    "df = df.rename(columns={'3/31/21':'first_year_cases'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b64c46df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FIPS\n",
       "1001    11.936594\n",
       "1003     9.853104\n",
       "1005     8.637809\n",
       "1007    11.284237\n",
       "1009    11.178767\n",
       "Name: case_pct, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create variable for case% for each counties population\n",
    "df['case_pct'] = df['first_year_cases']/df['E_TOTPOP']*100\n",
    "df['case_pct'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78c3a477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3058.000000\n",
       "mean        9.426600\n",
       "std         3.045809\n",
       "min         0.000000\n",
       "25%         7.713422\n",
       "50%         9.466675\n",
       "75%        11.176131\n",
       "max        38.010657\n",
       "Name: case_pct, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['case_pct'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cd247c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FIPS\n",
       "1001      low\n",
       "1003      low\n",
       "1005      low\n",
       "1007      low\n",
       "1009      low\n",
       "         ... \n",
       "56037     low\n",
       "56039    high\n",
       "56041     low\n",
       "56043     low\n",
       "56045     low\n",
       "Name: case_class, Length: 3058, dtype: category\n",
       "Categories (2, object): ['low' < 'high']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bin and cut the case_pct column into 2 classifications\n",
    "# q = df['case_pct'].quantile(.75)\n",
    "q = df['case_pct'].quantile(.9)\n",
    "bins = [0, 12 , 40]\n",
    "labels = ['low','high']\n",
    "df['case_class'] = pd.cut(df['case_pct'], bins, labels = labels)\n",
    "df['case_class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0da9118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "low     2540\n",
       "high     494\n",
       "Name: case_class, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['case_class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16633ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "impact_cols = ['Empathy',\n",
    " 'EPL_GROUPQ',\n",
    " 'Agreeableness',\n",
    " 'Employment Rate',\n",
    " 'EPL_UNEMP',\n",
    " 'Openness',\n",
    " 'E_AGE65',\n",
    " 'EP_AGE17',\n",
    " 'Conscientiousness',\n",
    " 'SPL_THEME4',\n",
    " 'Income Per Capita',\n",
    " 'E_POV',\n",
    " 'first_year_cases',\n",
    " 'E_MINRTY',\n",
    " 'Conflict Awareness',\n",
    " 'Extraversion',\n",
    " 'EP_SNGPNT',\n",
    " 'F_MOBILE',\n",
    " 'Work Ethic',\n",
    " 'E_AGE17',\n",
    " 'EP_PCI',\n",
    " 'Risk Taking',\n",
    " 'E_UNEMP',\n",
    " 'AREA_SQMI',\n",
    " 'E_NOHSDP',\n",
    " 'EPL_PCI',\n",
    " 'E_HU',\n",
    " 'E_NOVEH',\n",
    " 'E_LIMENG',\n",
    " 'EPL_SNGPNT',\n",
    " 'E_UNINSUR',\n",
    " 'EPL_AGE17',\n",
    " 'Belief In Science',\n",
    " 'EPL_LIMENG',\n",
    " 'EPL_NOVEH',\n",
    " 'Entrepreneurship',\n",
    " 'SPL_THEME3',\n",
    " 'E_HH',\n",
    " 'EP_MUNIT',\n",
    " 'EPL_MOBILE',\n",
    " 'Collectivism',\n",
    " 'RPL_THEME2',\n",
    " 'E_MUNIT',\n",
    " 'EPL_POV',\n",
    " 'EP_UNINSUR',\n",
    " 'dem_pct',\n",
    " 'F_SNGPNT',\n",
    " 'EPL_MUNIT',\n",
    " 'EP_NOHSDP',\n",
    " 'EPL_DISABL',\n",
    " 'Hopefulness',\n",
    " 'E_MOBILE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "651223bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impact_cols_reduced = ['Empathy',\n",
    "#  'EPL_GROUPQ',\n",
    "#  'Agreeableness',\n",
    "#  'Employment Rate',\n",
    "#  'EPL_UNEMP',\n",
    "#  'Openness',\n",
    "#  'E_AGE65',\n",
    "#  'EP_AGE17',\n",
    "#  'Conscientiousness',\n",
    "#  'SPL_THEME4',\n",
    "#  'Income Per Capita',\n",
    "#  'E_POV',\n",
    "#  'first_year_cases',\n",
    "#  'E_MINRTY',\n",
    "#  'Conflict Awareness',\n",
    "#  'Extraversion',\n",
    "#  'EP_SNGPNT',\n",
    "#  'F_MOBILE',\n",
    "#  'Work Ethic',\n",
    "#  'E_AGE17',\n",
    "#  'EP_PCI',\n",
    "#  'Risk Taking',\n",
    "#  'E_UNEMP',\n",
    "#  'AREA_SQMI',\n",
    "#  'E_NOHSDP',\n",
    "#  'EPL_PCI',\n",
    "#  'E_HU',\n",
    "#  'E_NOVEH',\n",
    "#  'E_LIMENG',\n",
    "#  'EPL_SNGPNT',\n",
    "#  'E_UNINSUR',\n",
    "#  'EPL_AGE17']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0b8f650",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop unneeded columns\n",
    "df = df.drop('case_pct', axis =1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "664b3ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_year_cases</th>\n",
       "      <th>AREA_SQMI</th>\n",
       "      <th>E_TOTPOP</th>\n",
       "      <th>E_HU</th>\n",
       "      <th>E_HH</th>\n",
       "      <th>E_POV</th>\n",
       "      <th>E_UNEMP</th>\n",
       "      <th>E_PCI</th>\n",
       "      <th>E_NOHSDP</th>\n",
       "      <th>E_AGE65</th>\n",
       "      <th>...</th>\n",
       "      <th>Neuroticism</th>\n",
       "      <th>Openness</th>\n",
       "      <th>Religiosity</th>\n",
       "      <th>Risk Taking</th>\n",
       "      <th>Selflessness</th>\n",
       "      <th>Tolerance</th>\n",
       "      <th>Work Ethic</th>\n",
       "      <th>dem_pct</th>\n",
       "      <th>case_class_low</th>\n",
       "      <th>case_class_high</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FIPS</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>6589</td>\n",
       "      <td>594.443459</td>\n",
       "      <td>55200.0</td>\n",
       "      <td>23315.0</td>\n",
       "      <td>21115.0</td>\n",
       "      <td>8422.0</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>29372.0</td>\n",
       "      <td>4204.0</td>\n",
       "      <td>8050.0</td>\n",
       "      <td>...</td>\n",
       "      <td>77.925476</td>\n",
       "      <td>78.222354</td>\n",
       "      <td>91.106719</td>\n",
       "      <td>53.333333</td>\n",
       "      <td>82.142857</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>60.380952</td>\n",
       "      <td>27.018365</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>20505</td>\n",
       "      <td>1589.793007</td>\n",
       "      <td>208107.0</td>\n",
       "      <td>111945.0</td>\n",
       "      <td>78622.0</td>\n",
       "      <td>21653.0</td>\n",
       "      <td>4343.0</td>\n",
       "      <td>31203.0</td>\n",
       "      <td>14310.0</td>\n",
       "      <td>40665.0</td>\n",
       "      <td>...</td>\n",
       "      <td>77.232120</td>\n",
       "      <td>80.086368</td>\n",
       "      <td>71.771566</td>\n",
       "      <td>67.272980</td>\n",
       "      <td>75.586018</td>\n",
       "      <td>66.983549</td>\n",
       "      <td>70.972246</td>\n",
       "      <td>22.409030</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>2227</td>\n",
       "      <td>885.001636</td>\n",
       "      <td>25782.0</td>\n",
       "      <td>11937.0</td>\n",
       "      <td>9186.0</td>\n",
       "      <td>6597.0</td>\n",
       "      <td>918.0</td>\n",
       "      <td>18461.0</td>\n",
       "      <td>4901.0</td>\n",
       "      <td>4634.0</td>\n",
       "      <td>...</td>\n",
       "      <td>80.375206</td>\n",
       "      <td>78.783778</td>\n",
       "      <td>73.657368</td>\n",
       "      <td>76.066481</td>\n",
       "      <td>78.753019</td>\n",
       "      <td>65.170377</td>\n",
       "      <td>68.704105</td>\n",
       "      <td>45.788173</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>2542</td>\n",
       "      <td>622.461089</td>\n",
       "      <td>22527.0</td>\n",
       "      <td>9161.0</td>\n",
       "      <td>6840.0</td>\n",
       "      <td>2863.0</td>\n",
       "      <td>658.0</td>\n",
       "      <td>20199.0</td>\n",
       "      <td>2650.0</td>\n",
       "      <td>3661.0</td>\n",
       "      <td>...</td>\n",
       "      <td>80.813736</td>\n",
       "      <td>77.837027</td>\n",
       "      <td>69.974652</td>\n",
       "      <td>75.136154</td>\n",
       "      <td>76.929754</td>\n",
       "      <td>69.859503</td>\n",
       "      <td>67.931677</td>\n",
       "      <td>20.698280</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>6444</td>\n",
       "      <td>644.830460</td>\n",
       "      <td>57645.0</td>\n",
       "      <td>24222.0</td>\n",
       "      <td>20600.0</td>\n",
       "      <td>8220.0</td>\n",
       "      <td>909.0</td>\n",
       "      <td>22656.0</td>\n",
       "      <td>7861.0</td>\n",
       "      <td>10233.0</td>\n",
       "      <td>...</td>\n",
       "      <td>78.764620</td>\n",
       "      <td>78.193105</td>\n",
       "      <td>92.045455</td>\n",
       "      <td>57.603815</td>\n",
       "      <td>79.307632</td>\n",
       "      <td>64.953288</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>9.569378</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56037</th>\n",
       "      <td>4022</td>\n",
       "      <td>10426.975725</td>\n",
       "      <td>44117.0</td>\n",
       "      <td>19628.0</td>\n",
       "      <td>15871.0</td>\n",
       "      <td>5237.0</td>\n",
       "      <td>1213.0</td>\n",
       "      <td>32624.0</td>\n",
       "      <td>2549.0</td>\n",
       "      <td>4721.0</td>\n",
       "      <td>...</td>\n",
       "      <td>79.384759</td>\n",
       "      <td>79.347081</td>\n",
       "      <td>68.147062</td>\n",
       "      <td>73.938691</td>\n",
       "      <td>76.390464</td>\n",
       "      <td>67.420658</td>\n",
       "      <td>70.956334</td>\n",
       "      <td>22.894957</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56039</th>\n",
       "      <td>3609</td>\n",
       "      <td>3996.844622</td>\n",
       "      <td>23059.0</td>\n",
       "      <td>13680.0</td>\n",
       "      <td>9158.0</td>\n",
       "      <td>1619.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>53703.0</td>\n",
       "      <td>958.0</td>\n",
       "      <td>3135.0</td>\n",
       "      <td>...</td>\n",
       "      <td>71.547359</td>\n",
       "      <td>80.522872</td>\n",
       "      <td>65.399695</td>\n",
       "      <td>79.598153</td>\n",
       "      <td>79.698193</td>\n",
       "      <td>70.877600</td>\n",
       "      <td>70.938645</td>\n",
       "      <td>66.599040</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56041</th>\n",
       "      <td>2128</td>\n",
       "      <td>2081.719807</td>\n",
       "      <td>20609.0</td>\n",
       "      <td>8972.0</td>\n",
       "      <td>7735.0</td>\n",
       "      <td>2552.0</td>\n",
       "      <td>614.0</td>\n",
       "      <td>27009.0</td>\n",
       "      <td>934.0</td>\n",
       "      <td>2498.0</td>\n",
       "      <td>...</td>\n",
       "      <td>78.771570</td>\n",
       "      <td>77.859042</td>\n",
       "      <td>67.603416</td>\n",
       "      <td>69.705859</td>\n",
       "      <td>73.332067</td>\n",
       "      <td>67.404487</td>\n",
       "      <td>69.299391</td>\n",
       "      <td>16.819960</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56043</th>\n",
       "      <td>891</td>\n",
       "      <td>2238.672972</td>\n",
       "      <td>8129.0</td>\n",
       "      <td>3868.0</td>\n",
       "      <td>3422.0</td>\n",
       "      <td>984.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>27556.0</td>\n",
       "      <td>590.0</td>\n",
       "      <td>1686.0</td>\n",
       "      <td>...</td>\n",
       "      <td>76.249370</td>\n",
       "      <td>77.658224</td>\n",
       "      <td>67.412774</td>\n",
       "      <td>82.820701</td>\n",
       "      <td>78.925326</td>\n",
       "      <td>74.628788</td>\n",
       "      <td>70.050103</td>\n",
       "      <td>16.145833</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56045</th>\n",
       "      <td>633</td>\n",
       "      <td>2398.003891</td>\n",
       "      <td>7100.0</td>\n",
       "      <td>3565.0</td>\n",
       "      <td>3062.0</td>\n",
       "      <td>1171.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>29152.0</td>\n",
       "      <td>389.0</td>\n",
       "      <td>1340.0</td>\n",
       "      <td>...</td>\n",
       "      <td>79.961075</td>\n",
       "      <td>76.977099</td>\n",
       "      <td>71.535519</td>\n",
       "      <td>75.406846</td>\n",
       "      <td>76.730879</td>\n",
       "      <td>68.422269</td>\n",
       "      <td>69.413532</td>\n",
       "      <td>10.112360</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3058 rows × 105 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       first_year_cases     AREA_SQMI  E_TOTPOP      E_HU     E_HH    E_POV  \\\n",
       "FIPS                                                                          \n",
       "1001               6589    594.443459   55200.0   23315.0  21115.0   8422.0   \n",
       "1003              20505   1589.793007  208107.0  111945.0  78622.0  21653.0   \n",
       "1005               2227    885.001636   25782.0   11937.0   9186.0   6597.0   \n",
       "1007               2542    622.461089   22527.0    9161.0   6840.0   2863.0   \n",
       "1009               6444    644.830460   57645.0   24222.0  20600.0   8220.0   \n",
       "...                 ...           ...       ...       ...      ...      ...   \n",
       "56037              4022  10426.975725   44117.0   19628.0  15871.0   5237.0   \n",
       "56039              3609   3996.844622   23059.0   13680.0   9158.0   1619.0   \n",
       "56041              2128   2081.719807   20609.0    8972.0   7735.0   2552.0   \n",
       "56043               891   2238.672972    8129.0    3868.0   3422.0    984.0   \n",
       "56045               633   2398.003891    7100.0    3565.0   3062.0   1171.0   \n",
       "\n",
       "       E_UNEMP    E_PCI  E_NOHSDP  E_AGE65  ...  Neuroticism   Openness  \\\n",
       "FIPS                                        ...                           \n",
       "1001    1065.0  29372.0    4204.0   8050.0  ...    77.925476  78.222354   \n",
       "1003    4343.0  31203.0   14310.0  40665.0  ...    77.232120  80.086368   \n",
       "1005     918.0  18461.0    4901.0   4634.0  ...    80.375206  78.783778   \n",
       "1007     658.0  20199.0    2650.0   3661.0  ...    80.813736  77.837027   \n",
       "1009     909.0  22656.0    7861.0  10233.0  ...    78.764620  78.193105   \n",
       "...        ...      ...       ...      ...  ...          ...        ...   \n",
       "56037   1213.0  32624.0    2549.0   4721.0  ...    79.384759  79.347081   \n",
       "56039    210.0  53703.0     958.0   3135.0  ...    71.547359  80.522872   \n",
       "56041    614.0  27009.0     934.0   2498.0  ...    78.771570  77.859042   \n",
       "56043    253.0  27556.0     590.0   1686.0  ...    76.249370  77.658224   \n",
       "56045    117.0  29152.0     389.0   1340.0  ...    79.961075  76.977099   \n",
       "\n",
       "       Religiosity  Risk Taking  Selflessness  Tolerance  Work Ethic  \\\n",
       "FIPS                                                                   \n",
       "1001     91.106719    53.333333     82.142857  70.000000   60.380952   \n",
       "1003     71.771566    67.272980     75.586018  66.983549   70.972246   \n",
       "1005     73.657368    76.066481     78.753019  65.170377   68.704105   \n",
       "1007     69.974652    75.136154     76.929754  69.859503   67.931677   \n",
       "1009     92.045455    57.603815     79.307632  64.953288   76.000000   \n",
       "...            ...          ...           ...        ...         ...   \n",
       "56037    68.147062    73.938691     76.390464  67.420658   70.956334   \n",
       "56039    65.399695    79.598153     79.698193  70.877600   70.938645   \n",
       "56041    67.603416    69.705859     73.332067  67.404487   69.299391   \n",
       "56043    67.412774    82.820701     78.925326  74.628788   70.050103   \n",
       "56045    71.535519    75.406846     76.730879  68.422269   69.413532   \n",
       "\n",
       "         dem_pct  case_class_low  case_class_high  \n",
       "FIPS                                               \n",
       "1001   27.018365               1                0  \n",
       "1003   22.409030               1                0  \n",
       "1005   45.788173               1                0  \n",
       "1007   20.698280               1                0  \n",
       "1009    9.569378               1                0  \n",
       "...          ...             ...              ...  \n",
       "56037  22.894957               1                0  \n",
       "56039  66.599040               0                1  \n",
       "56041  16.819960               1                0  \n",
       "56043  16.145833               1                0  \n",
       "56045  10.112360               1                0  \n",
       "\n",
       "[3058 rows x 105 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#turn case % classifications into binary \n",
    "df = pd.get_dummies(df, columns = ['case_class'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfb69257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2564\n",
       "1     494\n",
       "Name: case_class_high, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['case_class_high'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3dc4e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperate targets and features\n",
    "## should i drop the number of cases?\n",
    "X = df[impact_cols].values\n",
    "y=df['case_class_high'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe3d3924",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a2d4ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3cb693cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3de65d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 100)               5300      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 80)                8080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 81        \n",
      "                                                                 \n",
      "=================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-08 18:51:29.195755: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 13,461\n",
      "Trainable params: 13,461\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model - deep neural net\n",
    "number_input_features = len(X_train[0])\n",
    "hidden_nodes_layer1 = 100\n",
    "hidden_nodes_layer2 = 80\n",
    "# hidden_nodes_layer1 = 100\n",
    "# hidden_nodes_layer2 = 80\n",
    "# hidden_nodes_layer3 = 80\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(\n",
    "    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\")\n",
    ")\n",
    "\n",
    "# Second hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "# nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer3, activation=\"relu\"))\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Check the structure of the model\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a209d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a670e197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "72/72 [==============================] - 1s 3ms/step - loss: 498.2166 - accuracy: 0.7427\n",
      "Epoch 2/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 129.0733 - accuracy: 0.7833\n",
      "Epoch 3/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 152.1110 - accuracy: 0.7798\n",
      "Epoch 4/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 124.0810 - accuracy: 0.8016\n",
      "Epoch 5/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 189.9255 - accuracy: 0.8020\n",
      "Epoch 6/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 52.7598 - accuracy: 0.8282\n",
      "Epoch 7/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 36.8987 - accuracy: 0.8474\n",
      "Epoch 8/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 92.8280 - accuracy: 0.8094\n",
      "Epoch 9/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 87.3583 - accuracy: 0.8378\n",
      "Epoch 10/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 334.5730 - accuracy: 0.7837\n",
      "Epoch 11/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 224.1774 - accuracy: 0.8011\n",
      "Epoch 12/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 44.6824 - accuracy: 0.8474\n",
      "Epoch 13/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 12.0829 - accuracy: 0.8953\n",
      "Epoch 14/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 105.3089 - accuracy: 0.8334\n",
      "Epoch 15/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 117.8005 - accuracy: 0.7963\n",
      "Epoch 16/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 13.2681 - accuracy: 0.8836\n",
      "Epoch 17/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 11.1554 - accuracy: 0.8945\n",
      "Epoch 18/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 7.7154 - accuracy: 0.8914\n",
      "Epoch 19/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 15.6706 - accuracy: 0.8565\n",
      "Epoch 20/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 7.4037 - accuracy: 0.8831\n",
      "Epoch 21/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 4.3409 - accuracy: 0.9171\n",
      "Epoch 22/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 45.5144 - accuracy: 0.8391\n",
      "Epoch 23/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 11.2199 - accuracy: 0.8927\n",
      "Epoch 24/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 20.3498 - accuracy: 0.8700\n",
      "Epoch 25/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 6.4498 - accuracy: 0.9080\n",
      "Epoch 26/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 10.8520 - accuracy: 0.8805\n",
      "Epoch 27/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 30.9948 - accuracy: 0.8644\n",
      "Epoch 28/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 69.6496 - accuracy: 0.8439\n",
      "Epoch 29/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 56.0742 - accuracy: 0.8443\n",
      "Epoch 30/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 10.2548 - accuracy: 0.8840\n",
      "Epoch 31/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 7.1845 - accuracy: 0.8936\n",
      "Epoch 32/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3.5274 - accuracy: 0.9198\n",
      "Epoch 33/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 9.9137 - accuracy: 0.8975\n",
      "Epoch 34/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 7.4073 - accuracy: 0.9154\n",
      "Epoch 35/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 21.7277 - accuracy: 0.9132\n",
      "Epoch 36/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 100.3980 - accuracy: 0.8447\n",
      "Epoch 37/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 35.5414 - accuracy: 0.8465\n",
      "Epoch 38/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 11.2685 - accuracy: 0.8875\n",
      "Epoch 39/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 6.5525 - accuracy: 0.9062\n",
      "Epoch 40/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 4.7290 - accuracy: 0.9167\n",
      "Epoch 41/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 123.4405 - accuracy: 0.8225\n",
      "Epoch 42/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 56.1876 - accuracy: 0.8290\n",
      "Epoch 43/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 68.1687 - accuracy: 0.8229\n",
      "Epoch 44/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 102.1095 - accuracy: 0.8251\n",
      "Epoch 45/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 17.7802 - accuracy: 0.9001\n",
      "Epoch 46/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 16.1684 - accuracy: 0.8875\n",
      "Epoch 47/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 6.4448 - accuracy: 0.9145\n",
      "Epoch 48/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 4.4200 - accuracy: 0.9198\n",
      "Epoch 49/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3.8934 - accuracy: 0.9206\n",
      "Epoch 50/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 5.0941 - accuracy: 0.9163\n",
      "Epoch 51/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2.1175 - accuracy: 0.9328\n",
      "Epoch 52/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 3.4133 - accuracy: 0.9250\n",
      "Epoch 53/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2.2852 - accuracy: 0.9298\n",
      "Epoch 54/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 5.0257 - accuracy: 0.9097\n",
      "Epoch 55/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 10.5026 - accuracy: 0.9089\n",
      "Epoch 56/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 4.2403 - accuracy: 0.9176\n",
      "Epoch 57/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2.5615 - accuracy: 0.9333\n",
      "Epoch 58/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3.0383 - accuracy: 0.9289\n",
      "Epoch 59/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.8217 - accuracy: 0.9411\n",
      "Epoch 60/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2.3090 - accuracy: 0.9267\n",
      "Epoch 61/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.7014 - accuracy: 0.9285\n",
      "Epoch 62/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3.1939 - accuracy: 0.9145\n",
      "Epoch 63/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.5921 - accuracy: 0.9407\n",
      "Epoch 64/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.9241 - accuracy: 0.9294\n",
      "Epoch 65/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.6826 - accuracy: 0.9263\n",
      "Epoch 66/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2.6894 - accuracy: 0.9263\n",
      "Epoch 67/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3.3466 - accuracy: 0.9241\n",
      "Epoch 68/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2.8713 - accuracy: 0.9246\n",
      "Epoch 69/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2.5875 - accuracy: 0.9163\n",
      "Epoch 70/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 3.5650 - accuracy: 0.9071\n",
      "Epoch 71/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2.5663 - accuracy: 0.9215\n",
      "Epoch 72/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.5510 - accuracy: 0.9341\n",
      "Epoch 73/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.4353 - accuracy: 0.9385\n",
      "Epoch 74/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.1450 - accuracy: 0.9416\n",
      "Epoch 75/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1.0931 - accuracy: 0.9437\n",
      "Epoch 76/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 3.2990 - accuracy: 0.9119\n",
      "Epoch 77/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.4189 - accuracy: 0.9468\n",
      "Epoch 78/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.6656 - accuracy: 0.9311\n",
      "Epoch 79/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1.7785 - accuracy: 0.9341\n",
      "Epoch 80/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 4.2486 - accuracy: 0.9128\n",
      "Epoch 81/150\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 2.7715 - accuracy: 0.9145\n",
      "Epoch 82/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72 [==============================] - 0s 3ms/step - loss: 69.4451 - accuracy: 0.8770\n",
      "Epoch 83/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 42.6792 - accuracy: 0.8386\n",
      "Epoch 84/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3.7168 - accuracy: 0.9036\n",
      "Epoch 85/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1.8326 - accuracy: 0.9263\n",
      "Epoch 86/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 5.2690 - accuracy: 0.8940\n",
      "Epoch 87/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2.3271 - accuracy: 0.9246\n",
      "Epoch 88/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.2134 - accuracy: 0.9350\n",
      "Epoch 89/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2.7531 - accuracy: 0.9145\n",
      "Epoch 90/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1.2460 - accuracy: 0.9368\n",
      "Epoch 91/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 14.7767 - accuracy: 0.8596\n",
      "Epoch 92/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 30.5114 - accuracy: 0.8447\n",
      "Epoch 93/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 7.2439 - accuracy: 0.8831\n",
      "Epoch 94/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2.4567 - accuracy: 0.9132\n",
      "Epoch 95/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1.8558 - accuracy: 0.9171\n",
      "Epoch 96/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.5633 - accuracy: 0.9224\n",
      "Epoch 97/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.2880 - accuracy: 0.9328\n",
      "Epoch 98/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2.1173 - accuracy: 0.9267\n",
      "Epoch 99/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1.5186 - accuracy: 0.9193\n",
      "Epoch 100/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1.4153 - accuracy: 0.9246\n",
      "Epoch 101/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.8847 - accuracy: 0.9381\n",
      "Epoch 102/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.9564 - accuracy: 0.9328\n",
      "Epoch 103/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 10.7428 - accuracy: 0.9054\n",
      "Epoch 104/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 8.7274 - accuracy: 0.8779\n",
      "Epoch 105/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1.7393 - accuracy: 0.9211\n",
      "Epoch 106/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1.9046 - accuracy: 0.9171\n",
      "Epoch 107/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1.3091 - accuracy: 0.9311\n",
      "Epoch 108/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.3822 - accuracy: 0.9246\n",
      "Epoch 109/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2.9579 - accuracy: 0.8932\n",
      "Epoch 110/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.3604 - accuracy: 0.9176\n",
      "Epoch 111/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7547 - accuracy: 0.9394\n",
      "Epoch 112/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6595 - accuracy: 0.9433\n",
      "Epoch 113/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.8552 - accuracy: 0.9381\n",
      "Epoch 114/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7574 - accuracy: 0.9398\n",
      "Epoch 115/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7640 - accuracy: 0.9341\n",
      "Epoch 116/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.5836 - accuracy: 0.9424\n",
      "Epoch 117/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7847 - accuracy: 0.9254\n",
      "Epoch 118/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7720 - accuracy: 0.9324\n",
      "Epoch 119/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7598 - accuracy: 0.9350\n",
      "Epoch 120/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7125 - accuracy: 0.9328\n",
      "Epoch 121/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.0524 - accuracy: 0.9206\n",
      "Epoch 122/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 9.3143 - accuracy: 0.8849\n",
      "Epoch 123/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 23.1834 - accuracy: 0.8369\n",
      "Epoch 124/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 12.4335 - accuracy: 0.8631\n",
      "Epoch 125/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2.5675 - accuracy: 0.9054\n",
      "Epoch 126/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1.1290 - accuracy: 0.9237\n",
      "Epoch 127/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1.0498 - accuracy: 0.9272\n",
      "Epoch 128/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6827 - accuracy: 0.9333\n",
      "Epoch 129/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.9473 - accuracy: 0.9254\n",
      "Epoch 130/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 12.0204 - accuracy: 0.8862\n",
      "Epoch 131/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 53.9389 - accuracy: 0.7850\n",
      "Epoch 132/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 7.7251 - accuracy: 0.8539\n",
      "Epoch 133/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 8.8392 - accuracy: 0.8439\n",
      "Epoch 134/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.8905 - accuracy: 0.8805\n",
      "Epoch 135/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 4.5593 - accuracy: 0.8504\n",
      "Epoch 136/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.0881 - accuracy: 0.8827\n",
      "Epoch 137/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.9062 - accuracy: 0.8862\n",
      "Epoch 138/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.8779 - accuracy: 0.8910\n",
      "Epoch 139/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6915 - accuracy: 0.9027\n",
      "Epoch 140/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.5758 - accuracy: 0.9067\n",
      "Epoch 141/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6797 - accuracy: 0.9010\n",
      "Epoch 142/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.3870 - accuracy: 0.9093\n",
      "Epoch 143/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.4488 - accuracy: 0.9036\n",
      "Epoch 144/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.5732 - accuracy: 0.9027\n",
      "Epoch 145/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.3299 - accuracy: 0.9150\n",
      "Epoch 146/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.3870 - accuracy: 0.9075\n",
      "Epoch 147/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2856 - accuracy: 0.9250\n",
      "Epoch 148/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.3109 - accuracy: 0.9211\n",
      "Epoch 149/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.2922 - accuracy: 0.9206\n",
      "Epoch 150/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.2699 - accuracy: 0.9219\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "fit_model = nn.fit(X_train,y_train,epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d122ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 - 0s - loss: 35.0572 - accuracy: 0.8928 - 264ms/epoch - 11ms/step\n",
      "Loss: 35.05717086791992, Accuracy: 0.8928104639053345\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn.evaluate(X_test,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "581f5f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       "array([[625,   0],\n",
       "       [124,  16]], dtype=int32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=nn.predict(X_test)\n",
    "con_mat = tf.math.confusion_matrix(labels=y_test, predictions=y_pred)\n",
    "con_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b8dd727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.save('../aug_8_reduced_features_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "10b97482",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_imported = tf.keras.models.load_model('../aug_8_reduced_features_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e327dd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 - 0s - loss: 15.6768 - accuracy: 0.9359 - 187ms/epoch - 8ms/step\n",
      "Loss: 15.676847457885742, Accuracy: 0.9359477162361145\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the completed model using the test data\n",
    "model_loss, model_accuracy = nn_imported.evaluate(X_test,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "64070ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       "array([[620,   5],\n",
       "       [ 55,  85]], dtype=int32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=nn_imported.predict(X_test)\n",
    "con_mat = tf.math.confusion_matrix(labels=y_test, predictions=y_pred)\n",
    "con_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "10a68dc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.073171e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.001165e-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.239522e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>9.410870e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761</th>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>4.267087e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>765 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0\n",
       "0    5.073171e-01\n",
       "1    1.001165e-20\n",
       "2    1.000000e+00\n",
       "3    6.239522e-07\n",
       "4    0.000000e+00\n",
       "..            ...\n",
       "760  9.410870e-07\n",
       "761  0.000000e+00\n",
       "762  0.000000e+00\n",
       "763  4.267087e-07\n",
       "764  0.000000e+00\n",
       "\n",
       "[765 rows x 1 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = pd.DataFrame(y_pred)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "efede3fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0\n",
       "2    1.0\n",
       "16   1.0\n",
       "22   1.0\n",
       "36   1.0\n",
       "43   1.0\n",
       "..   ...\n",
       "722  1.0\n",
       "725  1.0\n",
       "727  1.0\n",
       "738  1.0\n",
       "744  1.0\n",
       "\n",
       "[90 rows x 1 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.loc[pred[0]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0076b64c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
