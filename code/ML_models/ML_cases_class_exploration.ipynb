{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c130683c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dependencies\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c84493f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>FIPS</th>\n",
       "      <th>AREA_SQMI</th>\n",
       "      <th>E_TOTPOP</th>\n",
       "      <th>E_HU</th>\n",
       "      <th>E_HH</th>\n",
       "      <th>E_POV</th>\n",
       "      <th>E_UNEMP</th>\n",
       "      <th>E_PCI</th>\n",
       "      <th>E_NOHSDP</th>\n",
       "      <th>...</th>\n",
       "      <th>Income Per Capita</th>\n",
       "      <th>Neuroticism</th>\n",
       "      <th>Openness</th>\n",
       "      <th>Religiosity</th>\n",
       "      <th>Risk Taking</th>\n",
       "      <th>Selflessness</th>\n",
       "      <th>Tolerance</th>\n",
       "      <th>Work Ethic</th>\n",
       "      <th>dem_pct</th>\n",
       "      <th>first_year_cases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1001</td>\n",
       "      <td>594.443459</td>\n",
       "      <td>55200</td>\n",
       "      <td>23315</td>\n",
       "      <td>21115</td>\n",
       "      <td>8422</td>\n",
       "      <td>1065</td>\n",
       "      <td>29372</td>\n",
       "      <td>4204</td>\n",
       "      <td>...</td>\n",
       "      <td>26168.0</td>\n",
       "      <td>77.925476</td>\n",
       "      <td>78.222354</td>\n",
       "      <td>91.106719</td>\n",
       "      <td>53.333333</td>\n",
       "      <td>82.142857</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>60.380952</td>\n",
       "      <td>27.018365</td>\n",
       "      <td>6589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1009</td>\n",
       "      <td>644.830460</td>\n",
       "      <td>57645</td>\n",
       "      <td>24222</td>\n",
       "      <td>20600</td>\n",
       "      <td>8220</td>\n",
       "      <td>909</td>\n",
       "      <td>22656</td>\n",
       "      <td>7861</td>\n",
       "      <td>...</td>\n",
       "      <td>21033.0</td>\n",
       "      <td>78.764620</td>\n",
       "      <td>78.193105</td>\n",
       "      <td>92.045455</td>\n",
       "      <td>57.603815</td>\n",
       "      <td>79.307632</td>\n",
       "      <td>64.953288</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>9.569378</td>\n",
       "      <td>6444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1013</td>\n",
       "      <td>776.838201</td>\n",
       "      <td>20025</td>\n",
       "      <td>10026</td>\n",
       "      <td>6708</td>\n",
       "      <td>4640</td>\n",
       "      <td>567</td>\n",
       "      <td>20430</td>\n",
       "      <td>2141</td>\n",
       "      <td>...</td>\n",
       "      <td>19011.0</td>\n",
       "      <td>78.563680</td>\n",
       "      <td>76.109761</td>\n",
       "      <td>76.623924</td>\n",
       "      <td>69.058104</td>\n",
       "      <td>79.956648</td>\n",
       "      <td>67.920284</td>\n",
       "      <td>72.773953</td>\n",
       "      <td>41.789629</td>\n",
       "      <td>2097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1015</td>\n",
       "      <td>605.867251</td>\n",
       "      <td>115098</td>\n",
       "      <td>53682</td>\n",
       "      <td>45033</td>\n",
       "      <td>20819</td>\n",
       "      <td>4628</td>\n",
       "      <td>24706</td>\n",
       "      <td>12620</td>\n",
       "      <td>...</td>\n",
       "      <td>22231.0</td>\n",
       "      <td>79.439032</td>\n",
       "      <td>79.955121</td>\n",
       "      <td>77.918741</td>\n",
       "      <td>54.063568</td>\n",
       "      <td>76.745724</td>\n",
       "      <td>67.456150</td>\n",
       "      <td>68.292794</td>\n",
       "      <td>29.845243</td>\n",
       "      <td>14224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1017</td>\n",
       "      <td>596.560643</td>\n",
       "      <td>33826</td>\n",
       "      <td>16981</td>\n",
       "      <td>13516</td>\n",
       "      <td>5531</td>\n",
       "      <td>773</td>\n",
       "      <td>22827</td>\n",
       "      <td>4383</td>\n",
       "      <td>...</td>\n",
       "      <td>21532.0</td>\n",
       "      <td>76.995358</td>\n",
       "      <td>78.156771</td>\n",
       "      <td>75.891100</td>\n",
       "      <td>67.343775</td>\n",
       "      <td>79.128558</td>\n",
       "      <td>66.397785</td>\n",
       "      <td>69.554441</td>\n",
       "      <td>41.644857</td>\n",
       "      <td>3488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3053</th>\n",
       "      <td>3053</td>\n",
       "      <td>48131</td>\n",
       "      <td>1793.476183</td>\n",
       "      <td>11355</td>\n",
       "      <td>5592</td>\n",
       "      <td>3511</td>\n",
       "      <td>2751</td>\n",
       "      <td>482</td>\n",
       "      <td>17864</td>\n",
       "      <td>2386</td>\n",
       "      <td>...</td>\n",
       "      <td>19853.0</td>\n",
       "      <td>79.125428</td>\n",
       "      <td>78.895880</td>\n",
       "      <td>76.629575</td>\n",
       "      <td>60.576045</td>\n",
       "      <td>73.670302</td>\n",
       "      <td>64.571017</td>\n",
       "      <td>68.007770</td>\n",
       "      <td>50.959826</td>\n",
       "      <td>1214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3054</th>\n",
       "      <td>3054</td>\n",
       "      <td>48505</td>\n",
       "      <td>998.411980</td>\n",
       "      <td>14369</td>\n",
       "      <td>6388</td>\n",
       "      <td>4405</td>\n",
       "      <td>5609</td>\n",
       "      <td>621</td>\n",
       "      <td>17228</td>\n",
       "      <td>3226</td>\n",
       "      <td>...</td>\n",
       "      <td>16007.0</td>\n",
       "      <td>79.355639</td>\n",
       "      <td>79.572483</td>\n",
       "      <td>74.378252</td>\n",
       "      <td>77.443239</td>\n",
       "      <td>76.386871</td>\n",
       "      <td>74.001471</td>\n",
       "      <td>73.609838</td>\n",
       "      <td>47.134744</td>\n",
       "      <td>1760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3055</th>\n",
       "      <td>3055</td>\n",
       "      <td>48507</td>\n",
       "      <td>1297.406535</td>\n",
       "      <td>12131</td>\n",
       "      <td>4344</td>\n",
       "      <td>3509</td>\n",
       "      <td>4150</td>\n",
       "      <td>421</td>\n",
       "      <td>13350</td>\n",
       "      <td>2719</td>\n",
       "      <td>...</td>\n",
       "      <td>13393.0</td>\n",
       "      <td>78.392216</td>\n",
       "      <td>76.024682</td>\n",
       "      <td>75.848196</td>\n",
       "      <td>76.967659</td>\n",
       "      <td>77.303576</td>\n",
       "      <td>70.010162</td>\n",
       "      <td>71.121990</td>\n",
       "      <td>65.403060</td>\n",
       "      <td>1844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3056</th>\n",
       "      <td>3056</td>\n",
       "      <td>48127</td>\n",
       "      <td>1328.884075</td>\n",
       "      <td>10663</td>\n",
       "      <td>4408</td>\n",
       "      <td>3309</td>\n",
       "      <td>3148</td>\n",
       "      <td>694</td>\n",
       "      <td>18121</td>\n",
       "      <td>2137</td>\n",
       "      <td>...</td>\n",
       "      <td>19528.0</td>\n",
       "      <td>79.457964</td>\n",
       "      <td>79.141556</td>\n",
       "      <td>66.677924</td>\n",
       "      <td>73.140121</td>\n",
       "      <td>71.659882</td>\n",
       "      <td>68.065499</td>\n",
       "      <td>66.378831</td>\n",
       "      <td>61.756683</td>\n",
       "      <td>1707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3057</th>\n",
       "      <td>3057</td>\n",
       "      <td>48247</td>\n",
       "      <td>1136.172598</td>\n",
       "      <td>5282</td>\n",
       "      <td>2523</td>\n",
       "      <td>1710</td>\n",
       "      <td>1431</td>\n",
       "      <td>229</td>\n",
       "      <td>17798</td>\n",
       "      <td>812</td>\n",
       "      <td>...</td>\n",
       "      <td>16637.0</td>\n",
       "      <td>80.157542</td>\n",
       "      <td>79.708963</td>\n",
       "      <td>74.399849</td>\n",
       "      <td>74.024835</td>\n",
       "      <td>77.564505</td>\n",
       "      <td>72.456881</td>\n",
       "      <td>70.160988</td>\n",
       "      <td>58.791749</td>\n",
       "      <td>605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3058 rows × 105 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0   FIPS    AREA_SQMI  E_TOTPOP   E_HU   E_HH  E_POV  E_UNEMP  \\\n",
       "0              0   1001   594.443459     55200  23315  21115   8422     1065   \n",
       "1              1   1009   644.830460     57645  24222  20600   8220      909   \n",
       "2              2   1013   776.838201     20025  10026   6708   4640      567   \n",
       "3              3   1015   605.867251    115098  53682  45033  20819     4628   \n",
       "4              4   1017   596.560643     33826  16981  13516   5531      773   \n",
       "...          ...    ...          ...       ...    ...    ...    ...      ...   \n",
       "3053        3053  48131  1793.476183     11355   5592   3511   2751      482   \n",
       "3054        3054  48505   998.411980     14369   6388   4405   5609      621   \n",
       "3055        3055  48507  1297.406535     12131   4344   3509   4150      421   \n",
       "3056        3056  48127  1328.884075     10663   4408   3309   3148      694   \n",
       "3057        3057  48247  1136.172598      5282   2523   1710   1431      229   \n",
       "\n",
       "      E_PCI  E_NOHSDP  ...  Income Per Capita  Neuroticism   Openness  \\\n",
       "0     29372      4204  ...            26168.0    77.925476  78.222354   \n",
       "1     22656      7861  ...            21033.0    78.764620  78.193105   \n",
       "2     20430      2141  ...            19011.0    78.563680  76.109761   \n",
       "3     24706     12620  ...            22231.0    79.439032  79.955121   \n",
       "4     22827      4383  ...            21532.0    76.995358  78.156771   \n",
       "...     ...       ...  ...                ...          ...        ...   \n",
       "3053  17864      2386  ...            19853.0    79.125428  78.895880   \n",
       "3054  17228      3226  ...            16007.0    79.355639  79.572483   \n",
       "3055  13350      2719  ...            13393.0    78.392216  76.024682   \n",
       "3056  18121      2137  ...            19528.0    79.457964  79.141556   \n",
       "3057  17798       812  ...            16637.0    80.157542  79.708963   \n",
       "\n",
       "      Religiosity  Risk Taking  Selflessness  Tolerance  Work Ethic  \\\n",
       "0       91.106719    53.333333     82.142857  70.000000   60.380952   \n",
       "1       92.045455    57.603815     79.307632  64.953288   76.000000   \n",
       "2       76.623924    69.058104     79.956648  67.920284   72.773953   \n",
       "3       77.918741    54.063568     76.745724  67.456150   68.292794   \n",
       "4       75.891100    67.343775     79.128558  66.397785   69.554441   \n",
       "...           ...          ...           ...        ...         ...   \n",
       "3053    76.629575    60.576045     73.670302  64.571017   68.007770   \n",
       "3054    74.378252    77.443239     76.386871  74.001471   73.609838   \n",
       "3055    75.848196    76.967659     77.303576  70.010162   71.121990   \n",
       "3056    66.677924    73.140121     71.659882  68.065499   66.378831   \n",
       "3057    74.399849    74.024835     77.564505  72.456881   70.160988   \n",
       "\n",
       "        dem_pct  first_year_cases  \n",
       "0     27.018365              6589  \n",
       "1      9.569378              6444  \n",
       "2     41.789629              2097  \n",
       "3     29.845243             14224  \n",
       "4     41.644857              3488  \n",
       "...         ...               ...  \n",
       "3053  50.959826              1214  \n",
       "3054  47.134744              1760  \n",
       "3055  65.403060              1844  \n",
       "3056  61.756683              1707  \n",
       "3057  58.791749               605  \n",
       "\n",
       "[3058 rows x 105 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read in merged and cleaned data\n",
    "df = pd.read_csv('../../data/merged_cleaned_data/cases_merged_data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3182e6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set index to FIPS\n",
    "df = df.set_index(df['FIPS'])\n",
    "df= df.drop(columns = ['FIPS','Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b64c46df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FIPS\n",
       "1001    11.936594\n",
       "1009    11.178767\n",
       "1013    10.471910\n",
       "1015    12.358164\n",
       "1017    10.311595\n",
       "Name: case_pct, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create variable for case% for each counties population\n",
    "df['case_pct'] = df['first_year_cases']/df['E_TOTPOP']*100\n",
    "df['case_pct'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78c3a477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3058.000000\n",
       "mean        9.426600\n",
       "std         3.045809\n",
       "min         0.000000\n",
       "25%         7.713422\n",
       "50%         9.466675\n",
       "75%        11.176131\n",
       "max        38.010657\n",
       "Name: case_pct, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['case_pct'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cd247c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FIPS\n",
       "1001      low\n",
       "1009      low\n",
       "1013      low\n",
       "1015     high\n",
       "1017      low\n",
       "         ... \n",
       "48131     low\n",
       "48505    high\n",
       "48507    high\n",
       "48127    high\n",
       "48247     low\n",
       "Name: case_class, Length: 3058, dtype: category\n",
       "Categories (2, object): ['low' < 'high']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bin and cut the case_pct column into 2 classifications\n",
    "# q = df['case_pct'].quantile(.9)\n",
    "bins = [0, 12 , 40]\n",
    "labels = ['low','high']\n",
    "df['case_class'] = pd.cut(df['case_pct'], bins, labels = labels)\n",
    "df['case_class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0da9118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "low     2540\n",
       "high     494\n",
       "Name: case_class, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['case_class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16633ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impact_cols = ['Empathy',\n",
    "#  'EPL_GROUPQ',\n",
    "#  'Agreeableness',\n",
    "#  'Employment Rate',\n",
    "#  'EPL_UNEMP',\n",
    "#  'Openness',\n",
    "#  'E_AGE65',\n",
    "#  'EP_AGE17',\n",
    "#  'Conscientiousness',\n",
    "#  'SPL_THEME4',\n",
    "#  'Income Per Capita',\n",
    "#  'E_POV',\n",
    "#  'E_MINRTY',\n",
    "#  'Conflict Awareness',\n",
    "#  'Extraversion',\n",
    "#  'EP_SNGPNT',\n",
    "#  'F_MOBILE',\n",
    "#  'Work Ethic',\n",
    "#  'E_AGE17',\n",
    "#  'EP_PCI',\n",
    "#  'Risk Taking',\n",
    "#  'E_UNEMP',\n",
    "#  'AREA_SQMI',\n",
    "#  'E_NOHSDP',\n",
    "#  'EPL_PCI',\n",
    "#  'E_HU',\n",
    "#  'E_NOVEH',\n",
    "#  'E_LIMENG',\n",
    "#  'EPL_SNGPNT',\n",
    "#  'E_UNINSUR',\n",
    "#  'EPL_AGE17',\n",
    "#  'Belief In Science',\n",
    "#  'EPL_LIMENG',\n",
    "#  'EPL_NOVEH',\n",
    "#  'Entrepreneurship',\n",
    "#  'SPL_THEME3',\n",
    "#  'E_HH',\n",
    "#  'EP_MUNIT',\n",
    "#  'EPL_MOBILE',\n",
    "#  'Collectivism',\n",
    "#  'RPL_THEME2',\n",
    "#  'E_MUNIT',\n",
    "#  'EPL_POV',\n",
    "#  'EP_UNINSUR',\n",
    "#  'dem_pct',\n",
    "#  'F_SNGPNT',\n",
    "#  'EPL_MUNIT',\n",
    "#  'EP_NOHSDP',\n",
    "#  'EPL_DISABL',\n",
    "#  'Hopefulness',\n",
    "#  'E_MOBILE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "651223bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impact_cols_reduced = ['Empathy',\n",
    "#  'EPL_GROUPQ',\n",
    "#  'Agreeableness',\n",
    "#  'Employment Rate',\n",
    "#  'EPL_UNEMP',\n",
    "#  'Openness',\n",
    "#  'E_AGE65',\n",
    "#  'EP_AGE17',\n",
    "#  'Conscientiousness',\n",
    "#  'SPL_THEME4',\n",
    "#  'Income Per Capita',\n",
    "#  'E_POV',\n",
    "#  'first_year_cases',\n",
    "#  'E_MINRTY',\n",
    "#  'Conflict Awareness',\n",
    "#  'Extraversion',\n",
    "#  'EP_SNGPNT',\n",
    "#  'F_MOBILE',\n",
    "#  'Work Ethic',\n",
    "#  'E_AGE17',\n",
    "#  'EP_PCI',\n",
    "#  'Risk Taking',\n",
    "#  'E_UNEMP',\n",
    "#  'AREA_SQMI',\n",
    "#  'E_NOHSDP',\n",
    "#  'EPL_PCI',\n",
    "#  'E_HU',\n",
    "#  'E_NOVEH',\n",
    "#  'E_LIMENG',\n",
    "#  'EPL_SNGPNT',\n",
    "#  'E_UNINSUR',\n",
    "#  'EPL_AGE17']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0b8f650",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop unneeded columns\n",
    "df = df.drop('case_pct', axis =1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "664b3ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AREA_SQMI</th>\n",
       "      <th>E_TOTPOP</th>\n",
       "      <th>E_HU</th>\n",
       "      <th>E_HH</th>\n",
       "      <th>E_POV</th>\n",
       "      <th>E_UNEMP</th>\n",
       "      <th>E_PCI</th>\n",
       "      <th>E_NOHSDP</th>\n",
       "      <th>E_AGE65</th>\n",
       "      <th>E_AGE17</th>\n",
       "      <th>...</th>\n",
       "      <th>Openness</th>\n",
       "      <th>Religiosity</th>\n",
       "      <th>Risk Taking</th>\n",
       "      <th>Selflessness</th>\n",
       "      <th>Tolerance</th>\n",
       "      <th>Work Ethic</th>\n",
       "      <th>dem_pct</th>\n",
       "      <th>first_year_cases</th>\n",
       "      <th>case_class_low</th>\n",
       "      <th>case_class_high</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FIPS</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>594.443459</td>\n",
       "      <td>55200</td>\n",
       "      <td>23315</td>\n",
       "      <td>21115</td>\n",
       "      <td>8422</td>\n",
       "      <td>1065</td>\n",
       "      <td>29372</td>\n",
       "      <td>4204</td>\n",
       "      <td>8050</td>\n",
       "      <td>13369</td>\n",
       "      <td>...</td>\n",
       "      <td>78.222354</td>\n",
       "      <td>91.106719</td>\n",
       "      <td>53.333333</td>\n",
       "      <td>82.142857</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>60.380952</td>\n",
       "      <td>27.018365</td>\n",
       "      <td>6589</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>644.830460</td>\n",
       "      <td>57645</td>\n",
       "      <td>24222</td>\n",
       "      <td>20600</td>\n",
       "      <td>8220</td>\n",
       "      <td>909</td>\n",
       "      <td>22656</td>\n",
       "      <td>7861</td>\n",
       "      <td>10233</td>\n",
       "      <td>13468</td>\n",
       "      <td>...</td>\n",
       "      <td>78.193105</td>\n",
       "      <td>92.045455</td>\n",
       "      <td>57.603815</td>\n",
       "      <td>79.307632</td>\n",
       "      <td>64.953288</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>9.569378</td>\n",
       "      <td>6444</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>776.838201</td>\n",
       "      <td>20025</td>\n",
       "      <td>10026</td>\n",
       "      <td>6708</td>\n",
       "      <td>4640</td>\n",
       "      <td>567</td>\n",
       "      <td>20430</td>\n",
       "      <td>2141</td>\n",
       "      <td>3806</td>\n",
       "      <td>4566</td>\n",
       "      <td>...</td>\n",
       "      <td>76.109761</td>\n",
       "      <td>76.623924</td>\n",
       "      <td>69.058104</td>\n",
       "      <td>79.956648</td>\n",
       "      <td>67.920284</td>\n",
       "      <td>72.773953</td>\n",
       "      <td>41.789629</td>\n",
       "      <td>2097</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>605.867251</td>\n",
       "      <td>115098</td>\n",
       "      <td>53682</td>\n",
       "      <td>45033</td>\n",
       "      <td>20819</td>\n",
       "      <td>4628</td>\n",
       "      <td>24706</td>\n",
       "      <td>12620</td>\n",
       "      <td>19386</td>\n",
       "      <td>25196</td>\n",
       "      <td>...</td>\n",
       "      <td>79.955121</td>\n",
       "      <td>77.918741</td>\n",
       "      <td>54.063568</td>\n",
       "      <td>76.745724</td>\n",
       "      <td>67.456150</td>\n",
       "      <td>68.292794</td>\n",
       "      <td>29.845243</td>\n",
       "      <td>14224</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>596.560643</td>\n",
       "      <td>33826</td>\n",
       "      <td>16981</td>\n",
       "      <td>13516</td>\n",
       "      <td>5531</td>\n",
       "      <td>773</td>\n",
       "      <td>22827</td>\n",
       "      <td>4383</td>\n",
       "      <td>6409</td>\n",
       "      <td>7006</td>\n",
       "      <td>...</td>\n",
       "      <td>78.156771</td>\n",
       "      <td>75.891100</td>\n",
       "      <td>67.343775</td>\n",
       "      <td>79.128558</td>\n",
       "      <td>66.397785</td>\n",
       "      <td>69.554441</td>\n",
       "      <td>41.644857</td>\n",
       "      <td>3488</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48131</th>\n",
       "      <td>1793.476183</td>\n",
       "      <td>11355</td>\n",
       "      <td>5592</td>\n",
       "      <td>3511</td>\n",
       "      <td>2751</td>\n",
       "      <td>482</td>\n",
       "      <td>17864</td>\n",
       "      <td>2386</td>\n",
       "      <td>2025</td>\n",
       "      <td>2962</td>\n",
       "      <td>...</td>\n",
       "      <td>78.895880</td>\n",
       "      <td>76.629575</td>\n",
       "      <td>60.576045</td>\n",
       "      <td>73.670302</td>\n",
       "      <td>64.571017</td>\n",
       "      <td>68.007770</td>\n",
       "      <td>50.959826</td>\n",
       "      <td>1214</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48505</th>\n",
       "      <td>998.411980</td>\n",
       "      <td>14369</td>\n",
       "      <td>6388</td>\n",
       "      <td>4405</td>\n",
       "      <td>5609</td>\n",
       "      <td>621</td>\n",
       "      <td>17228</td>\n",
       "      <td>3226</td>\n",
       "      <td>1999</td>\n",
       "      <td>4835</td>\n",
       "      <td>...</td>\n",
       "      <td>79.572483</td>\n",
       "      <td>74.378252</td>\n",
       "      <td>77.443239</td>\n",
       "      <td>76.386871</td>\n",
       "      <td>74.001471</td>\n",
       "      <td>73.609838</td>\n",
       "      <td>47.134744</td>\n",
       "      <td>1760</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48507</th>\n",
       "      <td>1297.406535</td>\n",
       "      <td>12131</td>\n",
       "      <td>4344</td>\n",
       "      <td>3509</td>\n",
       "      <td>4150</td>\n",
       "      <td>421</td>\n",
       "      <td>13350</td>\n",
       "      <td>2719</td>\n",
       "      <td>1665</td>\n",
       "      <td>3583</td>\n",
       "      <td>...</td>\n",
       "      <td>76.024682</td>\n",
       "      <td>75.848196</td>\n",
       "      <td>76.967659</td>\n",
       "      <td>77.303576</td>\n",
       "      <td>70.010162</td>\n",
       "      <td>71.121990</td>\n",
       "      <td>65.403060</td>\n",
       "      <td>1844</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48127</th>\n",
       "      <td>1328.884075</td>\n",
       "      <td>10663</td>\n",
       "      <td>4408</td>\n",
       "      <td>3309</td>\n",
       "      <td>3148</td>\n",
       "      <td>694</td>\n",
       "      <td>18121</td>\n",
       "      <td>2137</td>\n",
       "      <td>1734</td>\n",
       "      <td>3195</td>\n",
       "      <td>...</td>\n",
       "      <td>79.141556</td>\n",
       "      <td>66.677924</td>\n",
       "      <td>73.140121</td>\n",
       "      <td>71.659882</td>\n",
       "      <td>68.065499</td>\n",
       "      <td>66.378831</td>\n",
       "      <td>61.756683</td>\n",
       "      <td>1707</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48247</th>\n",
       "      <td>1136.172598</td>\n",
       "      <td>5282</td>\n",
       "      <td>2523</td>\n",
       "      <td>1710</td>\n",
       "      <td>1431</td>\n",
       "      <td>229</td>\n",
       "      <td>17798</td>\n",
       "      <td>812</td>\n",
       "      <td>932</td>\n",
       "      <td>1659</td>\n",
       "      <td>...</td>\n",
       "      <td>79.708963</td>\n",
       "      <td>74.399849</td>\n",
       "      <td>74.024835</td>\n",
       "      <td>77.564505</td>\n",
       "      <td>72.456881</td>\n",
       "      <td>70.160988</td>\n",
       "      <td>58.791749</td>\n",
       "      <td>605</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3058 rows × 105 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         AREA_SQMI  E_TOTPOP   E_HU   E_HH  E_POV  E_UNEMP  E_PCI  E_NOHSDP  \\\n",
       "FIPS                                                                          \n",
       "1001    594.443459     55200  23315  21115   8422     1065  29372      4204   \n",
       "1009    644.830460     57645  24222  20600   8220      909  22656      7861   \n",
       "1013    776.838201     20025  10026   6708   4640      567  20430      2141   \n",
       "1015    605.867251    115098  53682  45033  20819     4628  24706     12620   \n",
       "1017    596.560643     33826  16981  13516   5531      773  22827      4383   \n",
       "...            ...       ...    ...    ...    ...      ...    ...       ...   \n",
       "48131  1793.476183     11355   5592   3511   2751      482  17864      2386   \n",
       "48505   998.411980     14369   6388   4405   5609      621  17228      3226   \n",
       "48507  1297.406535     12131   4344   3509   4150      421  13350      2719   \n",
       "48127  1328.884075     10663   4408   3309   3148      694  18121      2137   \n",
       "48247  1136.172598      5282   2523   1710   1431      229  17798       812   \n",
       "\n",
       "       E_AGE65  E_AGE17  ...   Openness  Religiosity  Risk Taking  \\\n",
       "FIPS                     ...                                        \n",
       "1001      8050    13369  ...  78.222354    91.106719    53.333333   \n",
       "1009     10233    13468  ...  78.193105    92.045455    57.603815   \n",
       "1013      3806     4566  ...  76.109761    76.623924    69.058104   \n",
       "1015     19386    25196  ...  79.955121    77.918741    54.063568   \n",
       "1017      6409     7006  ...  78.156771    75.891100    67.343775   \n",
       "...        ...      ...  ...        ...          ...          ...   \n",
       "48131     2025     2962  ...  78.895880    76.629575    60.576045   \n",
       "48505     1999     4835  ...  79.572483    74.378252    77.443239   \n",
       "48507     1665     3583  ...  76.024682    75.848196    76.967659   \n",
       "48127     1734     3195  ...  79.141556    66.677924    73.140121   \n",
       "48247      932     1659  ...  79.708963    74.399849    74.024835   \n",
       "\n",
       "       Selflessness  Tolerance  Work Ethic    dem_pct  first_year_cases  \\\n",
       "FIPS                                                                      \n",
       "1001      82.142857  70.000000   60.380952  27.018365              6589   \n",
       "1009      79.307632  64.953288   76.000000   9.569378              6444   \n",
       "1013      79.956648  67.920284   72.773953  41.789629              2097   \n",
       "1015      76.745724  67.456150   68.292794  29.845243             14224   \n",
       "1017      79.128558  66.397785   69.554441  41.644857              3488   \n",
       "...             ...        ...         ...        ...               ...   \n",
       "48131     73.670302  64.571017   68.007770  50.959826              1214   \n",
       "48505     76.386871  74.001471   73.609838  47.134744              1760   \n",
       "48507     77.303576  70.010162   71.121990  65.403060              1844   \n",
       "48127     71.659882  68.065499   66.378831  61.756683              1707   \n",
       "48247     77.564505  72.456881   70.160988  58.791749               605   \n",
       "\n",
       "       case_class_low  case_class_high  \n",
       "FIPS                                    \n",
       "1001                1                0  \n",
       "1009                1                0  \n",
       "1013                1                0  \n",
       "1015                0                1  \n",
       "1017                1                0  \n",
       "...               ...              ...  \n",
       "48131               1                0  \n",
       "48505               0                1  \n",
       "48507               0                1  \n",
       "48127               0                1  \n",
       "48247               1                0  \n",
       "\n",
       "[3058 rows x 105 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#turn case % classifications into binary \n",
    "df = pd.get_dummies(df, columns = ['case_class'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfb69257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2564\n",
       "1     494\n",
       "Name: case_class_high, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['case_class_high'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3dc4e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperate targets and features\n",
    "## should i drop the number of cases?\n",
    "X = df.drop(columns = ['case_class_high','case_class_low','first_year_cases']).values\n",
    "y=df['case_class_high'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe3d3924",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a2d4ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3cb693cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3de65d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 100)               10300     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 80)                8080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 81        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18,461\n",
      "Trainable params: 18,461\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-10 14:59:14.815965: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Define the model - deep neural net\n",
    "number_input_features = len(X_train[0])\n",
    "hidden_nodes_layer1 = 100\n",
    "hidden_nodes_layer2 = 80\n",
    "# hidden_nodes_layer1 = 100\n",
    "# hidden_nodes_layer2 = 80\n",
    "# hidden_nodes_layer3 = 80\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(\n",
    "    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\")\n",
    ")\n",
    "\n",
    "# Second hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "# nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer3, activation=\"relu\"))\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Check the structure of the model\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a209d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a670e197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "72/72 [==============================] - 1s 3ms/step - loss: 0.4691 - accuracy: 0.8064\n",
      "Epoch 2/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3835 - accuracy: 0.8386\n",
      "Epoch 3/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3526 - accuracy: 0.8500\n",
      "Epoch 4/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3283 - accuracy: 0.8622\n",
      "Epoch 5/150\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.3071 - accuracy: 0.8713\n",
      "Epoch 6/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.2864 - accuracy: 0.8735\n",
      "Epoch 7/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.2728 - accuracy: 0.8888\n",
      "Epoch 8/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2608 - accuracy: 0.8958\n",
      "Epoch 9/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2502 - accuracy: 0.8993\n",
      "Epoch 10/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.2269 - accuracy: 0.9080\n",
      "Epoch 11/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2072 - accuracy: 0.9154\n",
      "Epoch 12/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1896 - accuracy: 0.9246\n",
      "Epoch 13/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1820 - accuracy: 0.9302\n",
      "Epoch 14/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1752 - accuracy: 0.9333\n",
      "Epoch 15/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1450 - accuracy: 0.9507\n",
      "Epoch 16/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1369 - accuracy: 0.9498\n",
      "Epoch 17/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1169 - accuracy: 0.9647\n",
      "Epoch 18/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1076 - accuracy: 0.9660\n",
      "Epoch 19/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0990 - accuracy: 0.9717\n",
      "Epoch 20/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0846 - accuracy: 0.9778\n",
      "Epoch 21/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0718 - accuracy: 0.9830\n",
      "Epoch 22/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0656 - accuracy: 0.9852\n",
      "Epoch 23/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0612 - accuracy: 0.9843\n",
      "Epoch 24/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0592 - accuracy: 0.9847\n",
      "Epoch 25/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0474 - accuracy: 0.9908\n",
      "Epoch 26/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0412 - accuracy: 0.9939\n",
      "Epoch 27/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.0330 - accuracy: 0.9939\n",
      "Epoch 28/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0281 - accuracy: 0.9969\n",
      "Epoch 29/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0235 - accuracy: 0.9978\n",
      "Epoch 30/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0217 - accuracy: 0.9983\n",
      "Epoch 31/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0177 - accuracy: 0.9987\n",
      "Epoch 32/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0150 - accuracy: 0.9996\n",
      "Epoch 33/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.0136 - accuracy: 0.9996\n",
      "Epoch 34/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0121 - accuracy: 0.9996\n",
      "Epoch 35/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0107 - accuracy: 0.9996\n",
      "Epoch 36/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0097 - accuracy: 0.9996\n",
      "Epoch 37/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0080 - accuracy: 1.0000\n",
      "Epoch 38/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0071 - accuracy: 1.0000\n",
      "Epoch 39/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0063 - accuracy: 1.0000\n",
      "Epoch 40/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 41/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0053 - accuracy: 1.0000\n",
      "Epoch 42/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 43/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 44/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 45/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 46/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 47/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 48/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 49/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 50/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 51/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 52/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 53/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 54/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 55/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 56/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 57/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 58/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 59/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 9.7047e-04 - accuracy: 1.0000\n",
      "Epoch 60/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 9.1494e-04 - accuracy: 1.0000\n",
      "Epoch 61/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 8.5272e-04 - accuracy: 1.0000\n",
      "Epoch 62/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 7.9357e-04 - accuracy: 1.0000\n",
      "Epoch 63/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 7.4590e-04 - accuracy: 1.0000\n",
      "Epoch 64/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 6.9165e-04 - accuracy: 1.0000\n",
      "Epoch 65/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 6.6583e-04 - accuracy: 1.0000\n",
      "Epoch 66/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 6.1098e-04 - accuracy: 1.0000\n",
      "Epoch 67/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 6.0761e-04 - accuracy: 1.0000\n",
      "Epoch 68/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 5.6392e-04 - accuracy: 1.0000\n",
      "Epoch 69/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 5.2666e-04 - accuracy: 1.0000\n",
      "Epoch 70/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 5.0277e-04 - accuracy: 1.0000\n",
      "Epoch 71/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 4.5440e-04 - accuracy: 1.0000\n",
      "Epoch 72/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 4.3009e-04 - accuracy: 1.0000\n",
      "Epoch 73/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 4.0847e-04 - accuracy: 1.0000\n",
      "Epoch 74/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 3.8313e-04 - accuracy: 1.0000\n",
      "Epoch 75/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 3.6045e-04 - accuracy: 1.0000\n",
      "Epoch 76/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 3.5129e-04 - accuracy: 1.0000\n",
      "Epoch 77/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 3.3147e-04 - accuracy: 1.0000\n",
      "Epoch 78/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 3.1311e-04 - accuracy: 1.0000\n",
      "Epoch 79/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2.9141e-04 - accuracy: 1.0000\n",
      "Epoch 80/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2.7974e-04 - accuracy: 1.0000\n",
      "Epoch 81/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2.6279e-04 - accuracy: 1.0000\n",
      "Epoch 82/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72 [==============================] - 0s 2ms/step - loss: 2.4914e-04 - accuracy: 1.0000\n",
      "Epoch 83/150\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 2.3178e-04 - accuracy: 1.0000\n",
      "Epoch 84/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2.2528e-04 - accuracy: 1.0000\n",
      "Epoch 85/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2.0923e-04 - accuracy: 1.0000\n",
      "Epoch 86/150\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 2.0513e-04 - accuracy: 1.0000\n",
      "Epoch 87/150\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1.8959e-04 - accuracy: 1.0000\n",
      "Epoch 88/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1.8233e-04 - accuracy: 1.0000\n",
      "Epoch 89/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1.6986e-04 - accuracy: 1.0000\n",
      "Epoch 90/150\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1.6247e-04 - accuracy: 1.0000\n",
      "Epoch 91/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1.5517e-04 - accuracy: 1.0000\n",
      "Epoch 92/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.5026e-04 - accuracy: 1.0000\n",
      "Epoch 93/150\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1.4091e-04 - accuracy: 1.0000\n",
      "Epoch 94/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1.3223e-04 - accuracy: 1.0000\n",
      "Epoch 95/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.2742e-04 - accuracy: 1.0000\n",
      "Epoch 96/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.1981e-04 - accuracy: 1.0000\n",
      "Epoch 97/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1.1382e-04 - accuracy: 1.0000\n",
      "Epoch 98/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.0805e-04 - accuracy: 1.0000\n",
      "Epoch 99/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.0334e-04 - accuracy: 1.0000\n",
      "Epoch 100/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 9.8314e-05 - accuracy: 1.0000\n",
      "Epoch 101/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 9.3713e-05 - accuracy: 1.0000\n",
      "Epoch 102/150\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 8.8964e-05 - accuracy: 1.0000\n",
      "Epoch 103/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 8.4389e-05 - accuracy: 1.0000\n",
      "Epoch 104/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 8.0524e-05 - accuracy: 1.0000\n",
      "Epoch 105/150\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 7.7533e-05 - accuracy: 1.0000\n",
      "Epoch 106/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 7.5184e-05 - accuracy: 1.0000\n",
      "Epoch 107/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 7.0763e-05 - accuracy: 1.0000\n",
      "Epoch 108/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 6.8738e-05 - accuracy: 1.0000\n",
      "Epoch 109/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 6.4423e-05 - accuracy: 1.0000\n",
      "Epoch 110/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 6.3871e-05 - accuracy: 1.0000\n",
      "Epoch 111/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 5.7950e-05 - accuracy: 1.0000\n",
      "Epoch 112/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 5.6890e-05 - accuracy: 1.0000\n",
      "Epoch 113/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 5.3425e-05 - accuracy: 1.0000\n",
      "Epoch 114/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 4.9793e-05 - accuracy: 1.0000\n",
      "Epoch 115/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 4.8789e-05 - accuracy: 1.0000\n",
      "Epoch 116/150\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 4.5500e-05 - accuracy: 1.0000\n",
      "Epoch 117/150\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 4.4500e-05 - accuracy: 1.0000\n",
      "Epoch 118/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 4.1567e-05 - accuracy: 1.0000\n",
      "Epoch 119/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 4.0190e-05 - accuracy: 1.0000\n",
      "Epoch 120/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 3.8442e-05 - accuracy: 1.0000\n",
      "Epoch 121/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 3.6558e-05 - accuracy: 1.0000\n",
      "Epoch 122/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3.5214e-05 - accuracy: 1.0000\n",
      "Epoch 123/150\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 3.3612e-05 - accuracy: 1.0000\n",
      "Epoch 124/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3.2651e-05 - accuracy: 1.0000\n",
      "Epoch 125/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3.0497e-05 - accuracy: 1.0000\n",
      "Epoch 126/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2.9384e-05 - accuracy: 1.0000\n",
      "Epoch 127/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2.7791e-05 - accuracy: 1.0000\n",
      "Epoch 128/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2.6838e-05 - accuracy: 1.0000\n",
      "Epoch 129/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2.5577e-05 - accuracy: 1.0000\n",
      "Epoch 130/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2.4936e-05 - accuracy: 1.0000\n",
      "Epoch 131/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2.3119e-05 - accuracy: 1.0000\n",
      "Epoch 132/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2.2879e-05 - accuracy: 1.0000\n",
      "Epoch 133/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2.1365e-05 - accuracy: 1.0000\n",
      "Epoch 134/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2.0706e-05 - accuracy: 1.0000\n",
      "Epoch 135/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2.0040e-05 - accuracy: 1.0000\n",
      "Epoch 136/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.9049e-05 - accuracy: 1.0000\n",
      "Epoch 137/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1.8004e-05 - accuracy: 1.0000\n",
      "Epoch 138/150\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1.6972e-05 - accuracy: 1.0000\n",
      "Epoch 139/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1.6319e-05 - accuracy: 1.0000\n",
      "Epoch 140/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.5511e-05 - accuracy: 1.0000\n",
      "Epoch 141/150\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1.5063e-05 - accuracy: 1.0000\n",
      "Epoch 142/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.4350e-05 - accuracy: 1.0000\n",
      "Epoch 143/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.3672e-05 - accuracy: 1.0000\n",
      "Epoch 144/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.3301e-05 - accuracy: 1.0000\n",
      "Epoch 145/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1.2486e-05 - accuracy: 1.0000\n",
      "Epoch 146/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.2148e-05 - accuracy: 1.0000\n",
      "Epoch 147/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1.1538e-05 - accuracy: 1.0000\n",
      "Epoch 148/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1.0931e-05 - accuracy: 1.0000\n",
      "Epoch 149/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.0403e-05 - accuracy: 1.0000\n",
      "Epoch 150/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.0137e-05 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "fit_model = nn.fit(X_train_scaled,y_train,epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d122ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 - 0s - loss: 2.1169 - accuracy: 0.8379 - 262ms/epoch - 11ms/step\n",
      "Loss: 2.11692476272583, Accuracy: 0.8379085063934326\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "581f5f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       "array([[644,   8],\n",
       "       [108,   5]], dtype=int32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=nn.predict(X_test_scaled)\n",
    "con_mat = tf.math.confusion_matrix(labels=y_test, predictions=y_pred)\n",
    "con_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b8dd727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.save('saved_models/aug_10_reduced_features_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10b97482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn_imported = tf.keras.models.load_model('saved_models/aug_8_reduced_features_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e327dd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate the completed model using the test data\n",
    "# model_loss, model_accuracy = nn_imported.evaluate(X_test,y_test,verbose=2)\n",
    "# print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64070ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred=nn_imported.predict(X_test)\n",
    "# con_mat = tf.math.confusion_matrix(labels=y_test, predictions=y_pred)\n",
    "# con_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "10a68dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred = pd.DataFrame(y_pred)\n",
    "# pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "efede3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred.loc[pred[0]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0076b64c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
