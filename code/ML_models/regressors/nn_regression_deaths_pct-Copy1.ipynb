{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6073e73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xlrd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sqlalchemy import create_engine\n",
    "from config import db_password\n",
    "import psycopg2\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c47c3d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POPUNI</th>\n",
       "      <th>PRED0_PE</th>\n",
       "      <th>PRED12_PE</th>\n",
       "      <th>PRED3_PE</th>\n",
       "      <th>first_yr_deaths</th>\n",
       "      <th>BEDS</th>\n",
       "      <th>dem_pct</th>\n",
       "      <th>Agreeableness</th>\n",
       "      <th>Belief In Science</th>\n",
       "      <th>Collectivism</th>\n",
       "      <th>...</th>\n",
       "      <th>Hopefulness</th>\n",
       "      <th>Income Per Capita</th>\n",
       "      <th>Neuroticism</th>\n",
       "      <th>Openness</th>\n",
       "      <th>Religiosity</th>\n",
       "      <th>Risk Taking</th>\n",
       "      <th>Selflessness</th>\n",
       "      <th>Tolerance</th>\n",
       "      <th>Work Ethic</th>\n",
       "      <th>AREA_SQMI</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FIPS</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>55688</td>\n",
       "      <td>36.94</td>\n",
       "      <td>40.85</td>\n",
       "      <td>22.20</td>\n",
       "      <td>99</td>\n",
       "      <td>85.0</td>\n",
       "      <td>27.018365</td>\n",
       "      <td>86.279655</td>\n",
       "      <td>70.833333</td>\n",
       "      <td>57.142857</td>\n",
       "      <td>...</td>\n",
       "      <td>91.163142</td>\n",
       "      <td>26168.0</td>\n",
       "      <td>77.925476</td>\n",
       "      <td>78.222354</td>\n",
       "      <td>91.106719</td>\n",
       "      <td>53.333333</td>\n",
       "      <td>82.142857</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>60.380952</td>\n",
       "      <td>594.443459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>221898</td>\n",
       "      <td>35.43</td>\n",
       "      <td>40.81</td>\n",
       "      <td>23.76</td>\n",
       "      <td>301</td>\n",
       "      <td>332.0</td>\n",
       "      <td>22.409030</td>\n",
       "      <td>85.603337</td>\n",
       "      <td>63.268161</td>\n",
       "      <td>67.948815</td>\n",
       "      <td>...</td>\n",
       "      <td>82.484017</td>\n",
       "      <td>28069.0</td>\n",
       "      <td>77.232120</td>\n",
       "      <td>80.086368</td>\n",
       "      <td>71.771566</td>\n",
       "      <td>67.272980</td>\n",
       "      <td>75.586018</td>\n",
       "      <td>66.983549</td>\n",
       "      <td>70.972246</td>\n",
       "      <td>1589.793007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>22023</td>\n",
       "      <td>22.81</td>\n",
       "      <td>41.64</td>\n",
       "      <td>35.54</td>\n",
       "      <td>55</td>\n",
       "      <td>74.0</td>\n",
       "      <td>45.788173</td>\n",
       "      <td>87.711609</td>\n",
       "      <td>63.046939</td>\n",
       "      <td>70.099756</td>\n",
       "      <td>...</td>\n",
       "      <td>61.927181</td>\n",
       "      <td>17249.0</td>\n",
       "      <td>80.375206</td>\n",
       "      <td>78.783778</td>\n",
       "      <td>73.657368</td>\n",
       "      <td>76.066481</td>\n",
       "      <td>78.753019</td>\n",
       "      <td>65.170377</td>\n",
       "      <td>68.704105</td>\n",
       "      <td>885.001636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>20393</td>\n",
       "      <td>30.79</td>\n",
       "      <td>44.06</td>\n",
       "      <td>25.14</td>\n",
       "      <td>58</td>\n",
       "      <td>35.0</td>\n",
       "      <td>20.698280</td>\n",
       "      <td>84.830261</td>\n",
       "      <td>63.262028</td>\n",
       "      <td>72.034797</td>\n",
       "      <td>...</td>\n",
       "      <td>85.258871</td>\n",
       "      <td>18988.0</td>\n",
       "      <td>80.813736</td>\n",
       "      <td>77.837027</td>\n",
       "      <td>69.974652</td>\n",
       "      <td>75.136154</td>\n",
       "      <td>76.929754</td>\n",
       "      <td>69.859503</td>\n",
       "      <td>67.931677</td>\n",
       "      <td>622.461089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>57697</td>\n",
       "      <td>31.53</td>\n",
       "      <td>41.51</td>\n",
       "      <td>26.97</td>\n",
       "      <td>131</td>\n",
       "      <td>40.0</td>\n",
       "      <td>9.569378</td>\n",
       "      <td>85.548096</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>79.492703</td>\n",
       "      <td>21033.0</td>\n",
       "      <td>78.764620</td>\n",
       "      <td>78.193105</td>\n",
       "      <td>92.045455</td>\n",
       "      <td>57.603815</td>\n",
       "      <td>79.307632</td>\n",
       "      <td>64.953288</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>644.830460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56037</th>\n",
       "      <td>41888</td>\n",
       "      <td>40.53</td>\n",
       "      <td>42.45</td>\n",
       "      <td>17.02</td>\n",
       "      <td>37</td>\n",
       "      <td>115.0</td>\n",
       "      <td>22.894957</td>\n",
       "      <td>83.811791</td>\n",
       "      <td>68.303853</td>\n",
       "      <td>68.673956</td>\n",
       "      <td>...</td>\n",
       "      <td>82.403142</td>\n",
       "      <td>30945.0</td>\n",
       "      <td>79.384759</td>\n",
       "      <td>79.347081</td>\n",
       "      <td>68.147062</td>\n",
       "      <td>73.938691</td>\n",
       "      <td>76.390464</td>\n",
       "      <td>67.420658</td>\n",
       "      <td>70.956334</td>\n",
       "      <td>10426.975725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56039</th>\n",
       "      <td>23390</td>\n",
       "      <td>31.00</td>\n",
       "      <td>49.45</td>\n",
       "      <td>19.55</td>\n",
       "      <td>9</td>\n",
       "      <td>48.0</td>\n",
       "      <td>66.599040</td>\n",
       "      <td>82.886955</td>\n",
       "      <td>73.489916</td>\n",
       "      <td>63.115088</td>\n",
       "      <td>...</td>\n",
       "      <td>84.036899</td>\n",
       "      <td>46499.0</td>\n",
       "      <td>71.547359</td>\n",
       "      <td>80.522872</td>\n",
       "      <td>65.399695</td>\n",
       "      <td>79.598153</td>\n",
       "      <td>79.698193</td>\n",
       "      <td>70.877600</td>\n",
       "      <td>70.938645</td>\n",
       "      <td>3996.844622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56041</th>\n",
       "      <td>20183</td>\n",
       "      <td>38.37</td>\n",
       "      <td>46.31</td>\n",
       "      <td>15.32</td>\n",
       "      <td>12</td>\n",
       "      <td>42.0</td>\n",
       "      <td>16.819960</td>\n",
       "      <td>84.272810</td>\n",
       "      <td>67.029022</td>\n",
       "      <td>67.552392</td>\n",
       "      <td>...</td>\n",
       "      <td>84.089095</td>\n",
       "      <td>25636.0</td>\n",
       "      <td>78.771570</td>\n",
       "      <td>77.859042</td>\n",
       "      <td>67.603416</td>\n",
       "      <td>69.705859</td>\n",
       "      <td>73.332067</td>\n",
       "      <td>67.404487</td>\n",
       "      <td>69.299391</td>\n",
       "      <td>2081.719807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56043</th>\n",
       "      <td>7738</td>\n",
       "      <td>33.61</td>\n",
       "      <td>41.55</td>\n",
       "      <td>24.84</td>\n",
       "      <td>26</td>\n",
       "      <td>18.0</td>\n",
       "      <td>16.145833</td>\n",
       "      <td>80.773973</td>\n",
       "      <td>68.663949</td>\n",
       "      <td>66.701109</td>\n",
       "      <td>...</td>\n",
       "      <td>87.485019</td>\n",
       "      <td>26325.0</td>\n",
       "      <td>76.249370</td>\n",
       "      <td>77.658224</td>\n",
       "      <td>67.412774</td>\n",
       "      <td>82.820701</td>\n",
       "      <td>78.925326</td>\n",
       "      <td>74.628788</td>\n",
       "      <td>70.050103</td>\n",
       "      <td>2238.672972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56045</th>\n",
       "      <td>6664</td>\n",
       "      <td>27.87</td>\n",
       "      <td>50.53</td>\n",
       "      <td>21.61</td>\n",
       "      <td>5</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.112360</td>\n",
       "      <td>88.251855</td>\n",
       "      <td>67.710969</td>\n",
       "      <td>69.084172</td>\n",
       "      <td>...</td>\n",
       "      <td>80.975991</td>\n",
       "      <td>29493.0</td>\n",
       "      <td>79.961075</td>\n",
       "      <td>76.977099</td>\n",
       "      <td>71.535519</td>\n",
       "      <td>75.406846</td>\n",
       "      <td>76.730879</td>\n",
       "      <td>68.422269</td>\n",
       "      <td>69.413532</td>\n",
       "      <td>2398.003891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3058 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       POPUNI  PRED0_PE  PRED12_PE  PRED3_PE  first_yr_deaths   BEDS  \\\n",
       "FIPS                                                                   \n",
       "1001    55688     36.94      40.85     22.20               99   85.0   \n",
       "1003   221898     35.43      40.81     23.76              301  332.0   \n",
       "1005    22023     22.81      41.64     35.54               55   74.0   \n",
       "1007    20393     30.79      44.06     25.14               58   35.0   \n",
       "1009    57697     31.53      41.51     26.97              131   40.0   \n",
       "...       ...       ...        ...       ...              ...    ...   \n",
       "56037   41888     40.53      42.45     17.02               37  115.0   \n",
       "56039   23390     31.00      49.45     19.55                9   48.0   \n",
       "56041   20183     38.37      46.31     15.32               12   42.0   \n",
       "56043    7738     33.61      41.55     24.84               26   18.0   \n",
       "56045    6664     27.87      50.53     21.61                5   12.0   \n",
       "\n",
       "         dem_pct  Agreeableness  Belief In Science  Collectivism  ...  \\\n",
       "FIPS                                                              ...   \n",
       "1001   27.018365      86.279655          70.833333     57.142857  ...   \n",
       "1003   22.409030      85.603337          63.268161     67.948815  ...   \n",
       "1005   45.788173      87.711609          63.046939     70.099756  ...   \n",
       "1007   20.698280      84.830261          63.262028     72.034797  ...   \n",
       "1009    9.569378      85.548096          33.333333     80.000000  ...   \n",
       "...          ...            ...                ...           ...  ...   \n",
       "56037  22.894957      83.811791          68.303853     68.673956  ...   \n",
       "56039  66.599040      82.886955          73.489916     63.115088  ...   \n",
       "56041  16.819960      84.272810          67.029022     67.552392  ...   \n",
       "56043  16.145833      80.773973          68.663949     66.701109  ...   \n",
       "56045  10.112360      88.251855          67.710969     69.084172  ...   \n",
       "\n",
       "       Hopefulness  Income Per Capita  Neuroticism   Openness  Religiosity  \\\n",
       "FIPS                                                                         \n",
       "1001     91.163142            26168.0    77.925476  78.222354    91.106719   \n",
       "1003     82.484017            28069.0    77.232120  80.086368    71.771566   \n",
       "1005     61.927181            17249.0    80.375206  78.783778    73.657368   \n",
       "1007     85.258871            18988.0    80.813736  77.837027    69.974652   \n",
       "1009     79.492703            21033.0    78.764620  78.193105    92.045455   \n",
       "...            ...                ...          ...        ...          ...   \n",
       "56037    82.403142            30945.0    79.384759  79.347081    68.147062   \n",
       "56039    84.036899            46499.0    71.547359  80.522872    65.399695   \n",
       "56041    84.089095            25636.0    78.771570  77.859042    67.603416   \n",
       "56043    87.485019            26325.0    76.249370  77.658224    67.412774   \n",
       "56045    80.975991            29493.0    79.961075  76.977099    71.535519   \n",
       "\n",
       "       Risk Taking  Selflessness  Tolerance  Work Ethic     AREA_SQMI  \n",
       "FIPS                                                                   \n",
       "1001     53.333333     82.142857  70.000000   60.380952    594.443459  \n",
       "1003     67.272980     75.586018  66.983549   70.972246   1589.793007  \n",
       "1005     76.066481     78.753019  65.170377   68.704105    885.001636  \n",
       "1007     75.136154     76.929754  69.859503   67.931677    622.461089  \n",
       "1009     57.603815     79.307632  64.953288   76.000000    644.830460  \n",
       "...            ...           ...        ...         ...           ...  \n",
       "56037    73.938691     76.390464  67.420658   70.956334  10426.975725  \n",
       "56039    79.598153     79.698193  70.877600   70.938645   3996.844622  \n",
       "56041    69.705859     73.332067  67.404487   69.299391   2081.719807  \n",
       "56043    82.820701     78.925326  74.628788   70.050103   2238.672972  \n",
       "56045    75.406846     76.730879  68.422269   69.413532   2398.003891  \n",
       "\n",
       "[3058 rows x 27 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deaths_df = pd.read_csv('../../../data/cleaned_data/cre_svi_death_pct.csv', index_col = 'FIPS')\n",
    "deaths_df = deaths_df.rename(columns = {'3/31/21':'first_yr_deaths'})\n",
    "deaths_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e97997a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # use regex to split svi columns into distinct categories\n",
    "# cols = df.columns.to_list()\n",
    "# col_series = pd.Series(cols)\n",
    "# pct_str = r'^[ERS]P+.'\n",
    "# pct_form = col_series.str.contains(pct_str)\n",
    "# pct_col = col_series[pct_form].to_list()\n",
    "# flag_str = r'^F+.'\n",
    "# flag_form = col_series.str.contains(flag_str)\n",
    "# flag_col = col_series[flag_form].to_list()\n",
    "# val_str = r'^E_+.'\n",
    "# val_form = col_series.str.contains(val_str)\n",
    "# val_col = col_series[val_form].to_list()\n",
    "# non_svi = col_series[~pct_form & ~flag_form & ~val_form].to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f26935a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # filter cases_df to only have svi estimated percentage columns\n",
    "# cases_df = df.drop(columns = flag_col)\n",
    "# cases_df = cases_df.drop(columns = val_col)\n",
    "# cases_df.columns.to_list()\n",
    "# cases_df = cases_df.merge(df['E_TOTPOP'], how = 'left', left_index=True, right_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3286057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['POPUNI',\n",
       " 'PRED0_PE',\n",
       " 'PRED12_PE',\n",
       " 'PRED3_PE',\n",
       " 'first_yr_deaths',\n",
       " 'BEDS',\n",
       " 'dem_pct',\n",
       " 'Agreeableness',\n",
       " 'Belief In Science',\n",
       " 'Collectivism',\n",
       " 'Conflict Awareness',\n",
       " 'Conscientiousness',\n",
       " 'Empathy',\n",
       " 'Employment Rate',\n",
       " 'Entrepreneurship',\n",
       " 'Extraversion',\n",
       " 'Gender Equality',\n",
       " 'Hopefulness',\n",
       " 'Income Per Capita',\n",
       " 'Neuroticism',\n",
       " 'Openness',\n",
       " 'Religiosity',\n",
       " 'Risk Taking',\n",
       " 'Selflessness',\n",
       " 'Tolerance',\n",
       " 'Work Ethic',\n",
       " 'AREA_SQMI']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deaths_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdcc35d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POPUNI</th>\n",
       "      <th>PRED0_PE</th>\n",
       "      <th>PRED12_PE</th>\n",
       "      <th>PRED3_PE</th>\n",
       "      <th>first_yr_deaths</th>\n",
       "      <th>BEDS</th>\n",
       "      <th>dem_pct</th>\n",
       "      <th>Agreeableness</th>\n",
       "      <th>Belief In Science</th>\n",
       "      <th>Collectivism</th>\n",
       "      <th>...</th>\n",
       "      <th>Hopefulness</th>\n",
       "      <th>Income Per Capita</th>\n",
       "      <th>Neuroticism</th>\n",
       "      <th>Openness</th>\n",
       "      <th>Religiosity</th>\n",
       "      <th>Risk Taking</th>\n",
       "      <th>Selflessness</th>\n",
       "      <th>Tolerance</th>\n",
       "      <th>Work Ethic</th>\n",
       "      <th>AREA_SQMI</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FIPS</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>55688</td>\n",
       "      <td>36.94</td>\n",
       "      <td>40.85</td>\n",
       "      <td>22.20</td>\n",
       "      <td>99</td>\n",
       "      <td>85.0</td>\n",
       "      <td>27.018365</td>\n",
       "      <td>86.279655</td>\n",
       "      <td>70.833333</td>\n",
       "      <td>57.142857</td>\n",
       "      <td>...</td>\n",
       "      <td>91.163142</td>\n",
       "      <td>26168.0</td>\n",
       "      <td>77.925476</td>\n",
       "      <td>78.222354</td>\n",
       "      <td>91.106719</td>\n",
       "      <td>53.333333</td>\n",
       "      <td>82.142857</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>60.380952</td>\n",
       "      <td>594.443459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>221898</td>\n",
       "      <td>35.43</td>\n",
       "      <td>40.81</td>\n",
       "      <td>23.76</td>\n",
       "      <td>301</td>\n",
       "      <td>332.0</td>\n",
       "      <td>22.409030</td>\n",
       "      <td>85.603337</td>\n",
       "      <td>63.268161</td>\n",
       "      <td>67.948815</td>\n",
       "      <td>...</td>\n",
       "      <td>82.484017</td>\n",
       "      <td>28069.0</td>\n",
       "      <td>77.232120</td>\n",
       "      <td>80.086368</td>\n",
       "      <td>71.771566</td>\n",
       "      <td>67.272980</td>\n",
       "      <td>75.586018</td>\n",
       "      <td>66.983549</td>\n",
       "      <td>70.972246</td>\n",
       "      <td>1589.793007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>22023</td>\n",
       "      <td>22.81</td>\n",
       "      <td>41.64</td>\n",
       "      <td>35.54</td>\n",
       "      <td>55</td>\n",
       "      <td>74.0</td>\n",
       "      <td>45.788173</td>\n",
       "      <td>87.711609</td>\n",
       "      <td>63.046939</td>\n",
       "      <td>70.099756</td>\n",
       "      <td>...</td>\n",
       "      <td>61.927181</td>\n",
       "      <td>17249.0</td>\n",
       "      <td>80.375206</td>\n",
       "      <td>78.783778</td>\n",
       "      <td>73.657368</td>\n",
       "      <td>76.066481</td>\n",
       "      <td>78.753019</td>\n",
       "      <td>65.170377</td>\n",
       "      <td>68.704105</td>\n",
       "      <td>885.001636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>20393</td>\n",
       "      <td>30.79</td>\n",
       "      <td>44.06</td>\n",
       "      <td>25.14</td>\n",
       "      <td>58</td>\n",
       "      <td>35.0</td>\n",
       "      <td>20.698280</td>\n",
       "      <td>84.830261</td>\n",
       "      <td>63.262028</td>\n",
       "      <td>72.034797</td>\n",
       "      <td>...</td>\n",
       "      <td>85.258871</td>\n",
       "      <td>18988.0</td>\n",
       "      <td>80.813736</td>\n",
       "      <td>77.837027</td>\n",
       "      <td>69.974652</td>\n",
       "      <td>75.136154</td>\n",
       "      <td>76.929754</td>\n",
       "      <td>69.859503</td>\n",
       "      <td>67.931677</td>\n",
       "      <td>622.461089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>57697</td>\n",
       "      <td>31.53</td>\n",
       "      <td>41.51</td>\n",
       "      <td>26.97</td>\n",
       "      <td>131</td>\n",
       "      <td>40.0</td>\n",
       "      <td>9.569378</td>\n",
       "      <td>85.548096</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>79.492703</td>\n",
       "      <td>21033.0</td>\n",
       "      <td>78.764620</td>\n",
       "      <td>78.193105</td>\n",
       "      <td>92.045455</td>\n",
       "      <td>57.603815</td>\n",
       "      <td>79.307632</td>\n",
       "      <td>64.953288</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>644.830460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56037</th>\n",
       "      <td>41888</td>\n",
       "      <td>40.53</td>\n",
       "      <td>42.45</td>\n",
       "      <td>17.02</td>\n",
       "      <td>37</td>\n",
       "      <td>115.0</td>\n",
       "      <td>22.894957</td>\n",
       "      <td>83.811791</td>\n",
       "      <td>68.303853</td>\n",
       "      <td>68.673956</td>\n",
       "      <td>...</td>\n",
       "      <td>82.403142</td>\n",
       "      <td>30945.0</td>\n",
       "      <td>79.384759</td>\n",
       "      <td>79.347081</td>\n",
       "      <td>68.147062</td>\n",
       "      <td>73.938691</td>\n",
       "      <td>76.390464</td>\n",
       "      <td>67.420658</td>\n",
       "      <td>70.956334</td>\n",
       "      <td>10426.975725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56039</th>\n",
       "      <td>23390</td>\n",
       "      <td>31.00</td>\n",
       "      <td>49.45</td>\n",
       "      <td>19.55</td>\n",
       "      <td>9</td>\n",
       "      <td>48.0</td>\n",
       "      <td>66.599040</td>\n",
       "      <td>82.886955</td>\n",
       "      <td>73.489916</td>\n",
       "      <td>63.115088</td>\n",
       "      <td>...</td>\n",
       "      <td>84.036899</td>\n",
       "      <td>46499.0</td>\n",
       "      <td>71.547359</td>\n",
       "      <td>80.522872</td>\n",
       "      <td>65.399695</td>\n",
       "      <td>79.598153</td>\n",
       "      <td>79.698193</td>\n",
       "      <td>70.877600</td>\n",
       "      <td>70.938645</td>\n",
       "      <td>3996.844622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56041</th>\n",
       "      <td>20183</td>\n",
       "      <td>38.37</td>\n",
       "      <td>46.31</td>\n",
       "      <td>15.32</td>\n",
       "      <td>12</td>\n",
       "      <td>42.0</td>\n",
       "      <td>16.819960</td>\n",
       "      <td>84.272810</td>\n",
       "      <td>67.029022</td>\n",
       "      <td>67.552392</td>\n",
       "      <td>...</td>\n",
       "      <td>84.089095</td>\n",
       "      <td>25636.0</td>\n",
       "      <td>78.771570</td>\n",
       "      <td>77.859042</td>\n",
       "      <td>67.603416</td>\n",
       "      <td>69.705859</td>\n",
       "      <td>73.332067</td>\n",
       "      <td>67.404487</td>\n",
       "      <td>69.299391</td>\n",
       "      <td>2081.719807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56043</th>\n",
       "      <td>7738</td>\n",
       "      <td>33.61</td>\n",
       "      <td>41.55</td>\n",
       "      <td>24.84</td>\n",
       "      <td>26</td>\n",
       "      <td>18.0</td>\n",
       "      <td>16.145833</td>\n",
       "      <td>80.773973</td>\n",
       "      <td>68.663949</td>\n",
       "      <td>66.701109</td>\n",
       "      <td>...</td>\n",
       "      <td>87.485019</td>\n",
       "      <td>26325.0</td>\n",
       "      <td>76.249370</td>\n",
       "      <td>77.658224</td>\n",
       "      <td>67.412774</td>\n",
       "      <td>82.820701</td>\n",
       "      <td>78.925326</td>\n",
       "      <td>74.628788</td>\n",
       "      <td>70.050103</td>\n",
       "      <td>2238.672972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56045</th>\n",
       "      <td>6664</td>\n",
       "      <td>27.87</td>\n",
       "      <td>50.53</td>\n",
       "      <td>21.61</td>\n",
       "      <td>5</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.112360</td>\n",
       "      <td>88.251855</td>\n",
       "      <td>67.710969</td>\n",
       "      <td>69.084172</td>\n",
       "      <td>...</td>\n",
       "      <td>80.975991</td>\n",
       "      <td>29493.0</td>\n",
       "      <td>79.961075</td>\n",
       "      <td>76.977099</td>\n",
       "      <td>71.535519</td>\n",
       "      <td>75.406846</td>\n",
       "      <td>76.730879</td>\n",
       "      <td>68.422269</td>\n",
       "      <td>69.413532</td>\n",
       "      <td>2398.003891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       POPUNI  PRED0_PE  PRED12_PE  PRED3_PE  first_yr_deaths   BEDS  \\\n",
       "FIPS                                                                   \n",
       "1001    55688     36.94      40.85     22.20               99   85.0   \n",
       "1003   221898     35.43      40.81     23.76              301  332.0   \n",
       "1005    22023     22.81      41.64     35.54               55   74.0   \n",
       "1007    20393     30.79      44.06     25.14               58   35.0   \n",
       "1009    57697     31.53      41.51     26.97              131   40.0   \n",
       "...       ...       ...        ...       ...              ...    ...   \n",
       "56037   41888     40.53      42.45     17.02               37  115.0   \n",
       "56039   23390     31.00      49.45     19.55                9   48.0   \n",
       "56041   20183     38.37      46.31     15.32               12   42.0   \n",
       "56043    7738     33.61      41.55     24.84               26   18.0   \n",
       "56045    6664     27.87      50.53     21.61                5   12.0   \n",
       "\n",
       "         dem_pct  Agreeableness  Belief In Science  Collectivism  ...  \\\n",
       "FIPS                                                              ...   \n",
       "1001   27.018365      86.279655          70.833333     57.142857  ...   \n",
       "1003   22.409030      85.603337          63.268161     67.948815  ...   \n",
       "1005   45.788173      87.711609          63.046939     70.099756  ...   \n",
       "1007   20.698280      84.830261          63.262028     72.034797  ...   \n",
       "1009    9.569378      85.548096          33.333333     80.000000  ...   \n",
       "...          ...            ...                ...           ...  ...   \n",
       "56037  22.894957      83.811791          68.303853     68.673956  ...   \n",
       "56039  66.599040      82.886955          73.489916     63.115088  ...   \n",
       "56041  16.819960      84.272810          67.029022     67.552392  ...   \n",
       "56043  16.145833      80.773973          68.663949     66.701109  ...   \n",
       "56045  10.112360      88.251855          67.710969     69.084172  ...   \n",
       "\n",
       "       Hopefulness  Income Per Capita  Neuroticism   Openness  Religiosity  \\\n",
       "FIPS                                                                         \n",
       "1001     91.163142            26168.0    77.925476  78.222354    91.106719   \n",
       "1003     82.484017            28069.0    77.232120  80.086368    71.771566   \n",
       "1005     61.927181            17249.0    80.375206  78.783778    73.657368   \n",
       "1007     85.258871            18988.0    80.813736  77.837027    69.974652   \n",
       "1009     79.492703            21033.0    78.764620  78.193105    92.045455   \n",
       "...            ...                ...          ...        ...          ...   \n",
       "56037    82.403142            30945.0    79.384759  79.347081    68.147062   \n",
       "56039    84.036899            46499.0    71.547359  80.522872    65.399695   \n",
       "56041    84.089095            25636.0    78.771570  77.859042    67.603416   \n",
       "56043    87.485019            26325.0    76.249370  77.658224    67.412774   \n",
       "56045    80.975991            29493.0    79.961075  76.977099    71.535519   \n",
       "\n",
       "       Risk Taking  Selflessness  Tolerance  Work Ethic     AREA_SQMI  \n",
       "FIPS                                                                   \n",
       "1001     53.333333     82.142857  70.000000   60.380952    594.443459  \n",
       "1003     67.272980     75.586018  66.983549   70.972246   1589.793007  \n",
       "1005     76.066481     78.753019  65.170377   68.704105    885.001636  \n",
       "1007     75.136154     76.929754  69.859503   67.931677    622.461089  \n",
       "1009     57.603815     79.307632  64.953288   76.000000    644.830460  \n",
       "...            ...           ...        ...         ...           ...  \n",
       "56037    73.938691     76.390464  67.420658   70.956334  10426.975725  \n",
       "56039    79.598153     79.698193  70.877600   70.938645   3996.844622  \n",
       "56041    69.705859     73.332067  67.404487   69.299391   2081.719807  \n",
       "56043    82.820701     78.925326  74.628788   70.050103   2238.672972  \n",
       "56045    75.406846     76.730879  68.422269   69.413532   2398.003891  \n",
       "\n",
       "[3000 rows x 27 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop counties with 0 cases \n",
    "## these seem to be errors- mostly in Utah, some counties with large populations\n",
    "zeros = deaths_df.loc[deaths_df['first_yr_deaths']==0]\n",
    "deaths_df = deaths_df.drop(index = zeros.index)\n",
    "deaths_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c055ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "deaths_df['pop_density'] = deaths_df['POPUNI']/deaths_df['AREA_SQMI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "106bfab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FIPS\n",
       "1001    0.177776\n",
       "1003    0.135648\n",
       "1005    0.249739\n",
       "1007    0.284411\n",
       "1009    0.227048\n",
       "Name: case_pct, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create variable for case% for each counties population\n",
    "deaths_df['case_pct'] = deaths_df['first_yr_deaths']/deaths_df['POPUNI']*100\n",
    "deaths_df['case_pct'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c977cd6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3000.000000\n",
       "mean        0.197046\n",
       "std         0.113159\n",
       "min         0.001390\n",
       "25%         0.117239\n",
       "50%         0.178936\n",
       "75%         0.252381\n",
       "max         0.865801\n",
       "Name: case_pct, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deaths_df['case_pct'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73aa10ce",
   "metadata": {},
   "source": [
    "## RF Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7384c664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 29)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deaths_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8242a445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature set (x)\n",
    "X = deaths_df.drop(['first_yr_deaths','case_pct','AREA_SQMI','POPUNI'], axis=1).values\n",
    "\n",
    "#Define (y)\n",
    "y= deaths_df['case_pct'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "426b3d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check columns list\n",
    "# X.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44b3e403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# frequency histogram to see how many counties are high risk and low risk\n",
    "plt.hist(deaths_df[\"case_pct\"])\n",
    "plt.xlabel(\"High Risk Counties\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd26af29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data for training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3 ,random_state= 42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb794166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2100, 25)\n",
      "(900, 25)\n",
      "(2100,)\n",
      "(900,)\n"
     ]
    }
   ],
   "source": [
    "# Determine the shape of our training and testing sets.\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ab34a44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0a2871d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a StandardScaler instance.\n",
    "scaler = StandardScaler()\n",
    "# Fitting the Standard Scaler with the training data.\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scaling the data.\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8a29ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-17 18:45:44.525301: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 128)               3328      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28,161\n",
      "Trainable params: 28,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model - deep neural net\n",
    "number_input_features = len(X_train[0])\n",
    "hidden_nodes_layer1 = 128\n",
    "hidden_nodes_layer2 = 128\n",
    "hidden_nodes_layer3 = 64\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(\n",
    "    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\"))\n",
    "# Second hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer3, activation=\"relu\"))\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation=\"linear\"))\n",
    "\n",
    "# Check the structure of the model\n",
    "nn.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "077c762e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "nn.compile(loss=\"mean_absolute_error\", optimizer=\"adam\", metrics=[\"mean_absolute_error\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a8951c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "66/66 [==============================] - 1s 2ms/step - loss: 0.1145 - mean_absolute_error: 0.1145\n",
      "Epoch 2/300\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 0.0818 - mean_absolute_error: 0.0818\n",
      "Epoch 3/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0749 - mean_absolute_error: 0.0749\n",
      "Epoch 4/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0678 - mean_absolute_error: 0.0678\n",
      "Epoch 5/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0651 - mean_absolute_error: 0.0651\n",
      "Epoch 6/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0594 - mean_absolute_error: 0.0594\n",
      "Epoch 7/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0569 - mean_absolute_error: 0.0569\n",
      "Epoch 8/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0540 - mean_absolute_error: 0.0540\n",
      "Epoch 9/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0537 - mean_absolute_error: 0.0537\n",
      "Epoch 10/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0515 - mean_absolute_error: 0.0515\n",
      "Epoch 11/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0484 - mean_absolute_error: 0.0484\n",
      "Epoch 12/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0493 - mean_absolute_error: 0.0493\n",
      "Epoch 13/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0466 - mean_absolute_error: 0.0466\n",
      "Epoch 14/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0437 - mean_absolute_error: 0.0437\n",
      "Epoch 15/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0436 - mean_absolute_error: 0.0436\n",
      "Epoch 16/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0439 - mean_absolute_error: 0.0439\n",
      "Epoch 17/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0427 - mean_absolute_error: 0.0427\n",
      "Epoch 18/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0398 - mean_absolute_error: 0.0398\n",
      "Epoch 19/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0385 - mean_absolute_error: 0.0385\n",
      "Epoch 20/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0376 - mean_absolute_error: 0.0376\n",
      "Epoch 21/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0373 - mean_absolute_error: 0.0373\n",
      "Epoch 22/300\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 0.0364 - mean_absolute_error: 0.0364\n",
      "Epoch 23/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0354 - mean_absolute_error: 0.0354\n",
      "Epoch 24/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0332 - mean_absolute_error: 0.0332\n",
      "Epoch 25/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0342 - mean_absolute_error: 0.0342\n",
      "Epoch 26/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0327 - mean_absolute_error: 0.0327\n",
      "Epoch 27/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0317 - mean_absolute_error: 0.0317\n",
      "Epoch 28/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0316 - mean_absolute_error: 0.0316\n",
      "Epoch 29/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0302 - mean_absolute_error: 0.0302\n",
      "Epoch 30/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0306 - mean_absolute_error: 0.0306\n",
      "Epoch 31/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0295 - mean_absolute_error: 0.0295\n",
      "Epoch 32/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0280 - mean_absolute_error: 0.0280\n",
      "Epoch 33/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0287 - mean_absolute_error: 0.0287\n",
      "Epoch 34/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0277 - mean_absolute_error: 0.0277\n",
      "Epoch 35/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0264 - mean_absolute_error: 0.0264\n",
      "Epoch 36/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0276 - mean_absolute_error: 0.0276\n",
      "Epoch 37/300\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 0.0266 - mean_absolute_error: 0.0266\n",
      "Epoch 38/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0258 - mean_absolute_error: 0.0258\n",
      "Epoch 39/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0261 - mean_absolute_error: 0.0261\n",
      "Epoch 40/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0262 - mean_absolute_error: 0.0262\n",
      "Epoch 41/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0251 - mean_absolute_error: 0.0251\n",
      "Epoch 42/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0231 - mean_absolute_error: 0.0231\n",
      "Epoch 43/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0242 - mean_absolute_error: 0.0242\n",
      "Epoch 44/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0228 - mean_absolute_error: 0.0228\n",
      "Epoch 45/300\n",
      "66/66 [==============================] - 1s 9ms/step - loss: 0.0246 - mean_absolute_error: 0.0246\n",
      "Epoch 46/300\n",
      "66/66 [==============================] - 0s 5ms/step - loss: 0.0220 - mean_absolute_error: 0.0220\n",
      "Epoch 47/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0236 - mean_absolute_error: 0.0236\n",
      "Epoch 48/300\n",
      "66/66 [==============================] - 0s 5ms/step - loss: 0.0225 - mean_absolute_error: 0.0225\n",
      "Epoch 49/300\n",
      "66/66 [==============================] - 0s 7ms/step - loss: 0.0223 - mean_absolute_error: 0.0223\n",
      "Epoch 50/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0230 - mean_absolute_error: 0.0230\n",
      "Epoch 51/300\n",
      "66/66 [==============================] - 0s 5ms/step - loss: 0.0217 - mean_absolute_error: 0.0217\n",
      "Epoch 52/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0218 - mean_absolute_error: 0.0218\n",
      "Epoch 53/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0213 - mean_absolute_error: 0.0213\n",
      "Epoch 54/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0205 - mean_absolute_error: 0.0205\n",
      "Epoch 55/300\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 0.0205 - mean_absolute_error: 0.0205\n",
      "Epoch 56/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0208 - mean_absolute_error: 0.0208\n",
      "Epoch 57/300\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 0.0211 - mean_absolute_error: 0.0211\n",
      "Epoch 58/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0202 - mean_absolute_error: 0.0202\n",
      "Epoch 59/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0203 - mean_absolute_error: 0.0203\n",
      "Epoch 60/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0211 - mean_absolute_error: 0.0211\n",
      "Epoch 61/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0203 - mean_absolute_error: 0.0203\n",
      "Epoch 62/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0194 - mean_absolute_error: 0.0194\n",
      "Epoch 63/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0191 - mean_absolute_error: 0.0191\n",
      "Epoch 64/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0188 - mean_absolute_error: 0.0188\n",
      "Epoch 65/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0182 - mean_absolute_error: 0.0182\n",
      "Epoch 66/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0175 - mean_absolute_error: 0.0175\n",
      "Epoch 67/300\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 0.0180 - mean_absolute_error: 0.0180\n",
      "Epoch 68/300\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 0.0183 - mean_absolute_error: 0.0183\n",
      "Epoch 69/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0190 - mean_absolute_error: 0.0190\n",
      "Epoch 70/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0174 - mean_absolute_error: 0.0174\n",
      "Epoch 71/300\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 0.0176 - mean_absolute_error: 0.0176\n",
      "Epoch 72/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0173 - mean_absolute_error: 0.0173\n",
      "Epoch 73/300\n",
      "66/66 [==============================] - 0s 7ms/step - loss: 0.0174 - mean_absolute_error: 0.0174\n",
      "Epoch 74/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0173 - mean_absolute_error: 0.0173\n",
      "Epoch 75/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0180 - mean_absolute_error: 0.0180\n",
      "Epoch 76/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0167 - mean_absolute_error: 0.0167\n",
      "Epoch 77/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0169 - mean_absolute_error: 0.0169\n",
      "Epoch 78/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0177 - mean_absolute_error: 0.0177\n",
      "Epoch 79/300\n",
      "66/66 [==============================] - 0s 6ms/step - loss: 0.0158 - mean_absolute_error: 0.0158\n",
      "Epoch 80/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0164 - mean_absolute_error: 0.0164\n",
      "Epoch 81/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0171 - mean_absolute_error: 0.0171\n",
      "Epoch 82/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0156 - mean_absolute_error: 0.0156\n",
      "Epoch 83/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0159 - mean_absolute_error: 0.0159\n",
      "Epoch 84/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0172 - mean_absolute_error: 0.0172\n",
      "Epoch 85/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0156 - mean_absolute_error: 0.0156\n",
      "Epoch 86/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0173 - mean_absolute_error: 0.0173\n",
      "Epoch 87/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0172 - mean_absolute_error: 0.0172\n",
      "Epoch 88/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0168 - mean_absolute_error: 0.0168\n",
      "Epoch 89/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0160 - mean_absolute_error: 0.0160\n",
      "Epoch 90/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0155 - mean_absolute_error: 0.0155\n",
      "Epoch 91/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0156 - mean_absolute_error: 0.0156\n",
      "Epoch 92/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0152 - mean_absolute_error: 0.0152\n",
      "Epoch 93/300\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 0.0155 - mean_absolute_error: 0.0155\n",
      "Epoch 94/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0154 - mean_absolute_error: 0.0154\n",
      "Epoch 95/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0152 - mean_absolute_error: 0.0152\n",
      "Epoch 96/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0146 - mean_absolute_error: 0.0146\n",
      "Epoch 97/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0160 - mean_absolute_error: 0.0160\n",
      "Epoch 98/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0155 - mean_absolute_error: 0.0155\n",
      "Epoch 99/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0153 - mean_absolute_error: 0.0153\n",
      "Epoch 100/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0146 - mean_absolute_error: 0.0146\n",
      "Epoch 101/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0148 - mean_absolute_error: 0.0148\n",
      "Epoch 102/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0150 - mean_absolute_error: 0.0150\n",
      "Epoch 103/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0150 - mean_absolute_error: 0.0150\n",
      "Epoch 104/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0149 - mean_absolute_error: 0.0149\n",
      "Epoch 105/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0139 - mean_absolute_error: 0.0139\n",
      "Epoch 106/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0147 - mean_absolute_error: 0.0147\n",
      "Epoch 107/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0148 - mean_absolute_error: 0.0148\n",
      "Epoch 108/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0143 - mean_absolute_error: 0.0143\n",
      "Epoch 109/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0149 - mean_absolute_error: 0.0149\n",
      "Epoch 110/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0147 - mean_absolute_error: 0.0147\n",
      "Epoch 111/300\n",
      "66/66 [==============================] - 0s 7ms/step - loss: 0.0148 - mean_absolute_error: 0.0148\n",
      "Epoch 112/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0145 - mean_absolute_error: 0.0145\n",
      "Epoch 113/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0139 - mean_absolute_error: 0.0139\n",
      "Epoch 114/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0140 - mean_absolute_error: 0.0140\n",
      "Epoch 115/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0147 - mean_absolute_error: 0.0147\n",
      "Epoch 116/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0138 - mean_absolute_error: 0.0138\n",
      "Epoch 117/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0142 - mean_absolute_error: 0.0142\n",
      "Epoch 118/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0141 - mean_absolute_error: 0.0141\n",
      "Epoch 119/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0140 - mean_absolute_error: 0.0140\n",
      "Epoch 120/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0138 - mean_absolute_error: 0.0138\n",
      "Epoch 121/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0143 - mean_absolute_error: 0.0143\n",
      "Epoch 122/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0137 - mean_absolute_error: 0.0137\n",
      "Epoch 123/300\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 0.0138 - mean_absolute_error: 0.0138\n",
      "Epoch 124/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0134 - mean_absolute_error: 0.0134\n",
      "Epoch 125/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0139 - mean_absolute_error: 0.0139\n",
      "Epoch 126/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0135 - mean_absolute_error: 0.0135\n",
      "Epoch 127/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0141 - mean_absolute_error: 0.0141\n",
      "Epoch 128/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0136 - mean_absolute_error: 0.0136\n",
      "Epoch 129/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0136 - mean_absolute_error: 0.0136\n",
      "Epoch 130/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0131 - mean_absolute_error: 0.0131\n",
      "Epoch 131/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0134 - mean_absolute_error: 0.0134\n",
      "Epoch 132/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0136 - mean_absolute_error: 0.0136\n",
      "Epoch 133/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0130 - mean_absolute_error: 0.0130\n",
      "Epoch 134/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0132 - mean_absolute_error: 0.0132\n",
      "Epoch 135/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0129 - mean_absolute_error: 0.0129\n",
      "Epoch 136/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0128 - mean_absolute_error: 0.0128\n",
      "Epoch 137/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0136 - mean_absolute_error: 0.0136\n",
      "Epoch 138/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0128 - mean_absolute_error: 0.0128\n",
      "Epoch 139/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0127 - mean_absolute_error: 0.0127\n",
      "Epoch 140/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0125 - mean_absolute_error: 0.0125\n",
      "Epoch 141/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0128 - mean_absolute_error: 0.0128\n",
      "Epoch 142/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0130 - mean_absolute_error: 0.0130\n",
      "Epoch 143/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0131 - mean_absolute_error: 0.0131\n",
      "Epoch 144/300\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 0.0136 - mean_absolute_error: 0.0136\n",
      "Epoch 145/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0126 - mean_absolute_error: 0.0126\n",
      "Epoch 146/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0127 - mean_absolute_error: 0.0127\n",
      "Epoch 147/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0127 - mean_absolute_error: 0.0127\n",
      "Epoch 148/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0128 - mean_absolute_error: 0.0128\n",
      "Epoch 149/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0120 - mean_absolute_error: 0.0120\n",
      "Epoch 150/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0125 - mean_absolute_error: 0.0125\n",
      "Epoch 151/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0129 - mean_absolute_error: 0.0129\n",
      "Epoch 152/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0120 - mean_absolute_error: 0.0120\n",
      "Epoch 153/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0124 - mean_absolute_error: 0.0124\n",
      "Epoch 154/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0118 - mean_absolute_error: 0.0118\n",
      "Epoch 155/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0122 - mean_absolute_error: 0.0122\n",
      "Epoch 156/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0119 - mean_absolute_error: 0.0119\n",
      "Epoch 157/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0135 - mean_absolute_error: 0.0135\n",
      "Epoch 158/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0118 - mean_absolute_error: 0.0118\n",
      "Epoch 159/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0125 - mean_absolute_error: 0.0125\n",
      "Epoch 160/300\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 0.0122 - mean_absolute_error: 0.0122\n",
      "Epoch 161/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0122 - mean_absolute_error: 0.0122\n",
      "Epoch 162/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0117 - mean_absolute_error: 0.0117\n",
      "Epoch 163/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0117 - mean_absolute_error: 0.0117\n",
      "Epoch 164/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0114 - mean_absolute_error: 0.0114\n",
      "Epoch 165/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0117 - mean_absolute_error: 0.0117\n",
      "Epoch 166/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0118 - mean_absolute_error: 0.0118\n",
      "Epoch 167/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0119 - mean_absolute_error: 0.0119\n",
      "Epoch 168/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0121 - mean_absolute_error: 0.0121\n",
      "Epoch 169/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0115 - mean_absolute_error: 0.0115\n",
      "Epoch 170/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0118 - mean_absolute_error: 0.0118\n",
      "Epoch 171/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0118 - mean_absolute_error: 0.0118\n",
      "Epoch 172/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0114 - mean_absolute_error: 0.0114\n",
      "Epoch 173/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0119 - mean_absolute_error: 0.0119\n",
      "Epoch 174/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0121 - mean_absolute_error: 0.0121\n",
      "Epoch 175/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0119 - mean_absolute_error: 0.0119\n",
      "Epoch 176/300\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 0.0121 - mean_absolute_error: 0.0121\n",
      "Epoch 177/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0108 - mean_absolute_error: 0.0108\n",
      "Epoch 178/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0119 - mean_absolute_error: 0.0119\n",
      "Epoch 179/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0112 - mean_absolute_error: 0.0112\n",
      "Epoch 180/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0114 - mean_absolute_error: 0.0114\n",
      "Epoch 181/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0115 - mean_absolute_error: 0.0115\n",
      "Epoch 182/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0106 - mean_absolute_error: 0.0106\n",
      "Epoch 183/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0110 - mean_absolute_error: 0.0110\n",
      "Epoch 184/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0110 - mean_absolute_error: 0.0110\n",
      "Epoch 185/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0105 - mean_absolute_error: 0.0105\n",
      "Epoch 186/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0117 - mean_absolute_error: 0.0117\n",
      "Epoch 187/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0119 - mean_absolute_error: 0.0119\n",
      "Epoch 188/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0110 - mean_absolute_error: 0.0110\n",
      "Epoch 189/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0111 - mean_absolute_error: 0.0111\n",
      "Epoch 190/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0117 - mean_absolute_error: 0.0117\n",
      "Epoch 191/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0119 - mean_absolute_error: 0.0119\n",
      "Epoch 192/300\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 0.0112 - mean_absolute_error: 0.0112\n",
      "Epoch 193/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0108 - mean_absolute_error: 0.0108\n",
      "Epoch 194/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0109 - mean_absolute_error: 0.0109\n",
      "Epoch 195/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0110 - mean_absolute_error: 0.0110\n",
      "Epoch 196/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0103 - mean_absolute_error: 0.0103\n",
      "Epoch 197/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0105 - mean_absolute_error: 0.0105\n",
      "Epoch 198/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0113 - mean_absolute_error: 0.0113\n",
      "Epoch 199/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0109 - mean_absolute_error: 0.0109\n",
      "Epoch 200/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0108 - mean_absolute_error: 0.0108\n",
      "Epoch 201/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0105 - mean_absolute_error: 0.0105\n",
      "Epoch 202/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0103 - mean_absolute_error: 0.0103\n",
      "Epoch 203/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0106 - mean_absolute_error: 0.0106\n",
      "Epoch 204/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0110 - mean_absolute_error: 0.0110\n",
      "Epoch 205/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0109 - mean_absolute_error: 0.0109\n",
      "Epoch 206/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0110 - mean_absolute_error: 0.0110\n",
      "Epoch 207/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0109 - mean_absolute_error: 0.0109\n",
      "Epoch 208/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0105 - mean_absolute_error: 0.0105\n",
      "Epoch 209/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0105 - mean_absolute_error: 0.0105\n",
      "Epoch 210/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0103 - mean_absolute_error: 0.0103\n",
      "Epoch 211/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0107 - mean_absolute_error: 0.0107\n",
      "Epoch 212/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0108 - mean_absolute_error: 0.0108\n",
      "Epoch 213/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0106 - mean_absolute_error: 0.0106\n",
      "Epoch 214/300\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 0.0103 - mean_absolute_error: 0.0103\n",
      "Epoch 215/300\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 0.0106 - mean_absolute_error: 0.0106\n",
      "Epoch 216/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0101 - mean_absolute_error: 0.0101\n",
      "Epoch 217/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0102 - mean_absolute_error: 0.0102\n",
      "Epoch 218/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0103 - mean_absolute_error: 0.0103\n",
      "Epoch 219/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0102 - mean_absolute_error: 0.0102\n",
      "Epoch 220/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0101 - mean_absolute_error: 0.0101\n",
      "Epoch 221/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0104 - mean_absolute_error: 0.0104\n",
      "Epoch 222/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0101 - mean_absolute_error: 0.0101\n",
      "Epoch 223/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0103 - mean_absolute_error: 0.0103\n",
      "Epoch 224/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0109 - mean_absolute_error: 0.0109\n",
      "Epoch 225/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0100 - mean_absolute_error: 0.0100\n",
      "Epoch 226/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0099 - mean_absolute_error: 0.0099\n",
      "Epoch 227/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0108 - mean_absolute_error: 0.0108\n",
      "Epoch 228/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0105 - mean_absolute_error: 0.0105\n",
      "Epoch 229/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0098 - mean_absolute_error: 0.0098\n",
      "Epoch 230/300\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 0.0103 - mean_absolute_error: 0.0103\n",
      "Epoch 231/300\n",
      "66/66 [==============================] - 0s 5ms/step - loss: 0.0102 - mean_absolute_error: 0.0102\n",
      "Epoch 232/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0104 - mean_absolute_error: 0.0104\n",
      "Epoch 233/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0105 - mean_absolute_error: 0.0105\n",
      "Epoch 234/300\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 0.0105 - mean_absolute_error: 0.0105\n",
      "Epoch 235/300\n",
      "66/66 [==============================] - 0s 5ms/step - loss: 0.0105 - mean_absolute_error: 0.0105\n",
      "Epoch 236/300\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 0.0095 - mean_absolute_error: 0.0095\n",
      "Epoch 237/300\n",
      "66/66 [==============================] - 0s 5ms/step - loss: 0.0100 - mean_absolute_error: 0.0100\n",
      "Epoch 238/300\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 0.0100 - mean_absolute_error: 0.0100\n",
      "Epoch 239/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0100 - mean_absolute_error: 0.0100\n",
      "Epoch 240/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0099 - mean_absolute_error: 0.0099\n",
      "Epoch 241/300\n",
      "66/66 [==============================] - 0s 5ms/step - loss: 0.0100 - mean_absolute_error: 0.0100\n",
      "Epoch 242/300\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 0.0101 - mean_absolute_error: 0.0101\n",
      "Epoch 243/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0095 - mean_absolute_error: 0.0095\n",
      "Epoch 244/300\n",
      "66/66 [==============================] - 0s 5ms/step - loss: 0.0096 - mean_absolute_error: 0.0096\n",
      "Epoch 245/300\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 0.0095 - mean_absolute_error: 0.0095\n",
      "Epoch 246/300\n",
      "66/66 [==============================] - 0s 5ms/step - loss: 0.0100 - mean_absolute_error: 0.0100\n",
      "Epoch 247/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0103 - mean_absolute_error: 0.0103\n",
      "Epoch 248/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0097 - mean_absolute_error: 0.0097\n",
      "Epoch 249/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0100 - mean_absolute_error: 0.0100\n",
      "Epoch 250/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0093 - mean_absolute_error: 0.0093\n",
      "Epoch 251/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0094 - mean_absolute_error: 0.0094\n",
      "Epoch 252/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0100 - mean_absolute_error: 0.0100\n",
      "Epoch 253/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0096 - mean_absolute_error: 0.0096\n",
      "Epoch 254/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0097 - mean_absolute_error: 0.0097\n",
      "Epoch 255/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0095 - mean_absolute_error: 0.0095\n",
      "Epoch 256/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0095 - mean_absolute_error: 0.0095\n",
      "Epoch 257/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0094 - mean_absolute_error: 0.0094\n",
      "Epoch 258/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0107 - mean_absolute_error: 0.0107\n",
      "Epoch 259/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0096 - mean_absolute_error: 0.0096\n",
      "Epoch 260/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0100 - mean_absolute_error: 0.0100\n",
      "Epoch 261/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0098 - mean_absolute_error: 0.0098\n",
      "Epoch 262/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0094 - mean_absolute_error: 0.0094\n",
      "Epoch 263/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0095 - mean_absolute_error: 0.0095\n",
      "Epoch 264/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0095 - mean_absolute_error: 0.0095\n",
      "Epoch 265/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0094 - mean_absolute_error: 0.0094\n",
      "Epoch 266/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0097 - mean_absolute_error: 0.0097\n",
      "Epoch 267/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0097 - mean_absolute_error: 0.0097\n",
      "Epoch 268/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0096 - mean_absolute_error: 0.0096\n",
      "Epoch 269/300\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 0.0095 - mean_absolute_error: 0.0095\n",
      "Epoch 270/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0095 - mean_absolute_error: 0.0095\n",
      "Epoch 271/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0092 - mean_absolute_error: 0.0092\n",
      "Epoch 272/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0094 - mean_absolute_error: 0.0094\n",
      "Epoch 273/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0098 - mean_absolute_error: 0.0098\n",
      "Epoch 274/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0095 - mean_absolute_error: 0.0095\n",
      "Epoch 275/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0090 - mean_absolute_error: 0.0090\n",
      "Epoch 276/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0089 - mean_absolute_error: 0.0089\n",
      "Epoch 277/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0093 - mean_absolute_error: 0.0093\n",
      "Epoch 278/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0095 - mean_absolute_error: 0.0095\n",
      "Epoch 279/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0089 - mean_absolute_error: 0.0089\n",
      "Epoch 280/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0090 - mean_absolute_error: 0.0090\n",
      "Epoch 281/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0098 - mean_absolute_error: 0.0098\n",
      "Epoch 282/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0100 - mean_absolute_error: 0.0100\n",
      "Epoch 283/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0090 - mean_absolute_error: 0.0090\n",
      "Epoch 284/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0093 - mean_absolute_error: 0.0093\n",
      "Epoch 285/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0092 - mean_absolute_error: 0.0092\n",
      "Epoch 286/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0092 - mean_absolute_error: 0.0092\n",
      "Epoch 287/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 288/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0086 - mean_absolute_error: 0.0086\n",
      "Epoch 289/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0090 - mean_absolute_error: 0.0090\n",
      "Epoch 290/300\n",
      "66/66 [==============================] - 0s 7ms/step - loss: 0.0096 - mean_absolute_error: 0.0096\n",
      "Epoch 291/300\n",
      "66/66 [==============================] - 0s 7ms/step - loss: 0.0092 - mean_absolute_error: 0.0092\n",
      "Epoch 292/300\n",
      "66/66 [==============================] - 1s 8ms/step - loss: 0.0089 - mean_absolute_error: 0.0089\n",
      "Epoch 293/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 0s 4ms/step - loss: 0.0087 - mean_absolute_error: 0.0087\n",
      "Epoch 294/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0092 - mean_absolute_error: 0.0092\n",
      "Epoch 295/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 296/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0094 - mean_absolute_error: 0.0094\n",
      "Epoch 297/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0090 - mean_absolute_error: 0.0090\n",
      "Epoch 298/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0089 - mean_absolute_error: 0.0089\n",
      "Epoch 299/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0089 - mean_absolute_error: 0.0089\n",
      "Epoch 300/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0090 - mean_absolute_error: 0.0090\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "fit_model = nn.fit(X_train_scaled,y_train,epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c394d261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 - 0s - loss: 0.0767 - mean_absolute_error: 0.0767 - 210ms/epoch - 7ms/step\n",
      "Loss: 0.07665082067251205, Accuracy: 0.07665082067251205\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65045f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>pred_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.507846</td>\n",
       "      <td>513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.479137</td>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.472565</td>\n",
       "      <td>282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.461983</td>\n",
       "      <td>297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.452145</td>\n",
       "      <td>506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>0.022415</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>0.013792</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>0.012752</td>\n",
       "      <td>651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>0.001200</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>-0.000381</td>\n",
       "      <td>326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         pred  pred_rank\n",
       "0    0.507846        513\n",
       "1    0.479137        339\n",
       "2    0.472565        282\n",
       "3    0.461983        297\n",
       "4    0.452145        506\n",
       "..        ...        ...\n",
       "895  0.022415         44\n",
       "896  0.013792        137\n",
       "897  0.012752        651\n",
       "898  0.001200        186\n",
       "899 -0.000381        326\n",
       "\n",
       "[900 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# turn model predictions into dataframe\n",
    "preds = pd.DataFrame(nn.predict(X_test_scaled), columns = ['pred'])\n",
    "preds['pred_rank'] = preds.index\n",
    "preds.sort_values(by = ['pred'], ascending = False, ignore_index = True, inplace = True)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e18f381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>act</th>\n",
       "      <th>act_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.716846</td>\n",
       "      <td>662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.704225</td>\n",
       "      <td>489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.662589</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.650195</td>\n",
       "      <td>710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.638613</td>\n",
       "      <td>578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>0.017095</td>\n",
       "      <td>777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>0.014209</td>\n",
       "      <td>883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>0.008065</td>\n",
       "      <td>638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>0.007495</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>0.007141</td>\n",
       "      <td>420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          act  act_rank\n",
       "0    0.716846       662\n",
       "1    0.704225       489\n",
       "2    0.662589       187\n",
       "3    0.650195       710\n",
       "4    0.638613       578\n",
       "..        ...       ...\n",
       "895  0.017095       777\n",
       "896  0.014209       883\n",
       "897  0.008065       638\n",
       "898  0.007495       125\n",
       "899  0.007141       420\n",
       "\n",
       "[900 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#turn test values into dataframe\n",
    "actuals = pd.DataFrame(y_test, columns = ['act'])\n",
    "actuals['act_rank'] = actuals.index\n",
    "actuals.sort_values(by = ['act'],ascending = False, ignore_index = True, inplace =True)\n",
    "actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "442b1b05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>act</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>pred_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.716846</td>\n",
       "      <td>662</td>\n",
       "      <td>0.507846</td>\n",
       "      <td>513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.704225</td>\n",
       "      <td>489</td>\n",
       "      <td>0.479137</td>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.662589</td>\n",
       "      <td>187</td>\n",
       "      <td>0.472565</td>\n",
       "      <td>282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.650195</td>\n",
       "      <td>710</td>\n",
       "      <td>0.461983</td>\n",
       "      <td>297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.638613</td>\n",
       "      <td>578</td>\n",
       "      <td>0.452145</td>\n",
       "      <td>506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>0.280261</td>\n",
       "      <td>712</td>\n",
       "      <td>0.244187</td>\n",
       "      <td>499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>0.279680</td>\n",
       "      <td>97</td>\n",
       "      <td>0.243547</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>0.279395</td>\n",
       "      <td>615</td>\n",
       "      <td>0.243232</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>0.278818</td>\n",
       "      <td>198</td>\n",
       "      <td>0.242507</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>0.278533</td>\n",
       "      <td>354</td>\n",
       "      <td>0.242452</td>\n",
       "      <td>783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          act  act_rank      pred  pred_rank\n",
       "0    0.716846       662  0.507846        513\n",
       "1    0.704225       489  0.479137        339\n",
       "2    0.662589       187  0.472565        282\n",
       "3    0.650195       710  0.461983        297\n",
       "4    0.638613       578  0.452145        506\n",
       "..        ...       ...       ...        ...\n",
       "175  0.280261       712  0.244187        499\n",
       "176  0.279680        97  0.243547          9\n",
       "177  0.279395       615  0.243232        168\n",
       "178  0.278818       198  0.242507        306\n",
       "179  0.278533       354  0.242452        783\n",
       "\n",
       "[180 rows x 4 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merege acutal and predicted dataframes\n",
    "ranked_results = actuals.merge(preds, how = 'inner', left_index=True, right_index=True)\n",
    "ranked = ranked_results.head(180)\n",
    "ranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96f4e93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "correct_index = []\n",
    "\n",
    "for index, row in ranked.iterrows():\n",
    "    if row['pred_rank'] in ranked['act_rank'].values:\n",
    "        correct = correct+1\n",
    "        correct_index.append(row['pred_rank'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5d65c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "012d75f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>act</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>pred_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.716846</td>\n",
       "      <td>662</td>\n",
       "      <td>0.410980</td>\n",
       "      <td>662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.704225</td>\n",
       "      <td>489</td>\n",
       "      <td>0.114952</td>\n",
       "      <td>489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.662589</td>\n",
       "      <td>187</td>\n",
       "      <td>0.291988</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.650195</td>\n",
       "      <td>710</td>\n",
       "      <td>0.188471</td>\n",
       "      <td>710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.638613</td>\n",
       "      <td>578</td>\n",
       "      <td>0.237382</td>\n",
       "      <td>578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>0.017095</td>\n",
       "      <td>777</td>\n",
       "      <td>0.059597</td>\n",
       "      <td>777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>0.014209</td>\n",
       "      <td>883</td>\n",
       "      <td>0.157941</td>\n",
       "      <td>883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>0.008065</td>\n",
       "      <td>638</td>\n",
       "      <td>0.037196</td>\n",
       "      <td>638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>0.007495</td>\n",
       "      <td>125</td>\n",
       "      <td>0.177465</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>0.007141</td>\n",
       "      <td>420</td>\n",
       "      <td>0.098781</td>\n",
       "      <td>420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          act  act_rank      pred  pred_rank\n",
       "0    0.716846       662  0.410980        662\n",
       "1    0.704225       489  0.114952        489\n",
       "2    0.662589       187  0.291988        187\n",
       "3    0.650195       710  0.188471        710\n",
       "4    0.638613       578  0.237382        578\n",
       "..        ...       ...       ...        ...\n",
       "895  0.017095       777  0.059597        777\n",
       "896  0.014209       883  0.157941        883\n",
       "897  0.008065       638  0.037196        638\n",
       "898  0.007495       125  0.177465        125\n",
       "899  0.007141       420  0.098781        420\n",
       "\n",
       "[900 rows x 4 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = actuals.merge(preds, how = 'inner', left_on='act_rank', right_on='pred_rank')\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2f03e8ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>act</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>pred_rank</th>\n",
       "      <th>error</th>\n",
       "      <th>abs_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.716846</td>\n",
       "      <td>662</td>\n",
       "      <td>0.410980</td>\n",
       "      <td>662</td>\n",
       "      <td>0.305866</td>\n",
       "      <td>0.305866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.704225</td>\n",
       "      <td>489</td>\n",
       "      <td>0.114952</td>\n",
       "      <td>489</td>\n",
       "      <td>0.589273</td>\n",
       "      <td>0.589273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.662589</td>\n",
       "      <td>187</td>\n",
       "      <td>0.291988</td>\n",
       "      <td>187</td>\n",
       "      <td>0.370601</td>\n",
       "      <td>0.370601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.650195</td>\n",
       "      <td>710</td>\n",
       "      <td>0.188471</td>\n",
       "      <td>710</td>\n",
       "      <td>0.461724</td>\n",
       "      <td>0.461724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.638613</td>\n",
       "      <td>578</td>\n",
       "      <td>0.237382</td>\n",
       "      <td>578</td>\n",
       "      <td>0.401231</td>\n",
       "      <td>0.401231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>0.017095</td>\n",
       "      <td>777</td>\n",
       "      <td>0.059597</td>\n",
       "      <td>777</td>\n",
       "      <td>-0.042503</td>\n",
       "      <td>0.042503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>0.014209</td>\n",
       "      <td>883</td>\n",
       "      <td>0.157941</td>\n",
       "      <td>883</td>\n",
       "      <td>-0.143733</td>\n",
       "      <td>0.143733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>0.008065</td>\n",
       "      <td>638</td>\n",
       "      <td>0.037196</td>\n",
       "      <td>638</td>\n",
       "      <td>-0.029131</td>\n",
       "      <td>0.029131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>0.007495</td>\n",
       "      <td>125</td>\n",
       "      <td>0.177465</td>\n",
       "      <td>125</td>\n",
       "      <td>-0.169970</td>\n",
       "      <td>0.169970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>0.007141</td>\n",
       "      <td>420</td>\n",
       "      <td>0.098781</td>\n",
       "      <td>420</td>\n",
       "      <td>-0.091639</td>\n",
       "      <td>0.091639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          act  act_rank      pred  pred_rank     error  abs_error\n",
       "0    0.716846       662  0.410980        662  0.305866   0.305866\n",
       "1    0.704225       489  0.114952        489  0.589273   0.589273\n",
       "2    0.662589       187  0.291988        187  0.370601   0.370601\n",
       "3    0.650195       710  0.188471        710  0.461724   0.461724\n",
       "4    0.638613       578  0.237382        578  0.401231   0.401231\n",
       "..        ...       ...       ...        ...       ...        ...\n",
       "895  0.017095       777  0.059597        777 -0.042503   0.042503\n",
       "896  0.014209       883  0.157941        883 -0.143733   0.143733\n",
       "897  0.008065       638  0.037196        638 -0.029131   0.029131\n",
       "898  0.007495       125  0.177465        125 -0.169970   0.169970\n",
       "899  0.007141       420  0.098781        420 -0.091639   0.091639\n",
       "\n",
       "[900 rows x 6 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check how big the errors are on each prediction\n",
    "results['error'] = results['act']-results['pred']\n",
    "results['abs_error'] = abs(results['error'])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "98f83f3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>act</th>\n",
       "      <th>act_rank</th>\n",
       "      <th>pred</th>\n",
       "      <th>pred_rank</th>\n",
       "      <th>error</th>\n",
       "      <th>abs_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.007141</td>\n",
       "      <td>420</td>\n",
       "      <td>0.098781</td>\n",
       "      <td>420</td>\n",
       "      <td>-0.091639</td>\n",
       "      <td>0.091639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.007495</td>\n",
       "      <td>125</td>\n",
       "      <td>0.177465</td>\n",
       "      <td>125</td>\n",
       "      <td>-0.169970</td>\n",
       "      <td>0.169970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.008065</td>\n",
       "      <td>638</td>\n",
       "      <td>0.037196</td>\n",
       "      <td>638</td>\n",
       "      <td>-0.029131</td>\n",
       "      <td>0.029131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.014209</td>\n",
       "      <td>883</td>\n",
       "      <td>0.157941</td>\n",
       "      <td>883</td>\n",
       "      <td>-0.143733</td>\n",
       "      <td>0.143733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.017095</td>\n",
       "      <td>777</td>\n",
       "      <td>0.059597</td>\n",
       "      <td>777</td>\n",
       "      <td>-0.042503</td>\n",
       "      <td>0.042503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>0.638613</td>\n",
       "      <td>578</td>\n",
       "      <td>0.237382</td>\n",
       "      <td>578</td>\n",
       "      <td>0.401231</td>\n",
       "      <td>0.401231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>0.650195</td>\n",
       "      <td>710</td>\n",
       "      <td>0.188471</td>\n",
       "      <td>710</td>\n",
       "      <td>0.461724</td>\n",
       "      <td>0.461724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>0.662589</td>\n",
       "      <td>187</td>\n",
       "      <td>0.291988</td>\n",
       "      <td>187</td>\n",
       "      <td>0.370601</td>\n",
       "      <td>0.370601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>0.704225</td>\n",
       "      <td>489</td>\n",
       "      <td>0.114952</td>\n",
       "      <td>489</td>\n",
       "      <td>0.589273</td>\n",
       "      <td>0.589273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>0.716846</td>\n",
       "      <td>662</td>\n",
       "      <td>0.410980</td>\n",
       "      <td>662</td>\n",
       "      <td>0.305866</td>\n",
       "      <td>0.305866</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          act  act_rank      pred  pred_rank     error  abs_error\n",
       "0    0.007141       420  0.098781        420 -0.091639   0.091639\n",
       "1    0.007495       125  0.177465        125 -0.169970   0.169970\n",
       "2    0.008065       638  0.037196        638 -0.029131   0.029131\n",
       "3    0.014209       883  0.157941        883 -0.143733   0.143733\n",
       "4    0.017095       777  0.059597        777 -0.042503   0.042503\n",
       "..        ...       ...       ...        ...       ...        ...\n",
       "895  0.638613       578  0.237382        578  0.401231   0.401231\n",
       "896  0.650195       710  0.188471        710  0.461724   0.461724\n",
       "897  0.662589       187  0.291988        187  0.370601   0.370601\n",
       "898  0.704225       489  0.114952        489  0.589273   0.589273\n",
       "899  0.716846       662  0.410980        662  0.305866   0.305866\n",
       "\n",
       "[900 rows x 6 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sort_values(by= ['act'], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a751ba9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f8c4516da50>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD4CAYAAADvsV2wAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAug0lEQVR4nO2df5Ac5Znfv8/MjsSsODNSLN/BgpBMsAgKltbsGflUSQy+O2FjYAPYAoPPvvKFclV8ZWTXnhebO2SXU+ii2JA/7HNRnBOnIEaAyFpGXITPyEkFFxjJK6ETIB/mh9CIi2XQYkc7sLO7T/6Y6VVPT7/9e2Z6ur+fKpV2enr6x9vdz/v28zzv9xFVBSGEkOxT6PUBEEII6Q40+IQQkhNo8AkhJCfQ4BNCSE6gwSeEkJww0OsD8OKd73ynrly5steHQQghfcO+fft+rarL3b5LtcFfuXIl9u7d2+vDIISQvkFEXjF9R5cOIYTkBBp8QgjJCTT4hBCSExIx+CJyuYgcFpEXRGTcsM4HRWS/iBwSkf+VxH4JIYQEJ3bQVkSKAL4F4I8AHAXwtIjsVNVnbetUAHwbwOWqekRE3hV3v4QQQsKRRJbO+wG8oKovAoCI3A/gagDP2tb5BICHVfUIAKjqrxLYrysTk1Vs230Yx6ZqOKtSxtjG1RgdHurU7gghpG9IwqUzBOBV2+ejzWV23gNgqYj8RET2icifmDYmIjeLyF4R2Xv8+PFQBzIxWcWtDx9EdaoGBVCdquHWhw9iYrIaajuEEJJFkjD44rLMqbk8AOBiAFcA2AjgL0XkPW4bU9W7VXVEVUeWL3edO2Bk2+7DqNXnWpbV6nPYtvtwqO0QQkgWScKlcxTAObbPZwM45rLOr1X1JICTIvK/AawF8IsE9r/AsalaqOWEEJInkhjhPw3gfBFZJSKLAFwPYKdjnR8A+FciMiAigwAuAfBcAvtu4axKOdRyQgjJE7ENvqrOAvgcgN1oGPEHVPWQiHxWRD7bXOc5AP8TwDMAfgbgHlX9h7j7djK2cTXKpWLLsnKpiLGNq5PeFSGE9B2S5hKHIyMjGlZLh1k6hJA8IyL7VHXE7btUi6dFYXR4iAaeEEJcoLQCIYTkBBp8QgjJCTT4hBCSE2jwCSEkJ9DgE0JITqDBJ4SQnECDTwghOYEGnxBCcgINPiGE5AQafEIIyQk0+IQQkhNo8AkhJCfQ4BNCSE6gwSeEkJxAg08IITmBBp8QQnICDT4hhOSEzFW8ssNyh4QQcopMGvyJySpuffgZ1OrzC8uqUzXc+vBBAKDRJ4Tkksy5dCYmqxh78ECLsbeo1eewbffhHhwVIYT0nswZ/G27D6M+r8bvj03Vung0hBCSHjJn8P0M+lmVcpeOhBBC0kXmDL6XQRcAYxtXd+9gCCEkRWTO4F96wXLjdzeuX8GALSEkt2TO4O95/rjr8nKpgK+PXtTloyGEkPSQOYNv8uHX6vOYmKx2+WgIISQ9ZM7ge/nwmZJJCMkzmTP4XkFZpmQSQvJM5gz+6PAQlg6WXL9jSiYhJM9kzuADwO1XrkG5VGxZVi4VmZJJCMk1iRh8EblcRA6LyAsiMu6x3u+LyJyIXJfEfr1YPHDq1JYOlnDHNRcxJZMQkmtiG3wRKQL4FoAPA7gQwA0icqFhvb8GsDvuPr1oCKcdxFStvrDsLRddHUIIyRtJjPDfD+AFVX1RVWcA3A/gapf1/hzADgC/SmCfRrbtPoxafa5lGUXTCCEkGYM/BOBV2+ejzWULiMgQgH8L4Dt+GxORm0Vkr4jsPX7cfRKVF6ZMnOpUDRu2Ps5cfEJIbknC4IvLMqdc5V0AvqSqcy7rtv5Q9W5VHVHVkeXLzTIJJrwycSxNfBp9QkgeScLgHwVwju3z2QCOOdYZAXC/iLwM4DoA3xaR0QT23cbYxtVtGTp2avU53LJ9P0f7hJDckUTFq6cBnC8iqwBUAVwP4BP2FVR1lfW3iPxXAI+o6kQC+3Zl8UChzY/vhBWwCCF5I/YIX1VnAXwOjeyb5wA8oKqHROSzIvLZuNsPg1uGjhcM5hJC8kQiNW1V9VEAjzqWuQZoVfXTSezTDbcMHT8ot0AIyQuZmmkbxXhTboEQkhcyZfDDGm/KLRBC8kSmDL5bhk6pICgV2zNHKbdACMkbifjw04JlvLftPoxjUzWcUS5BBDgxXUdRBHOqGKqUMbZxNQ09ISR3iKpzjlR6GBkZ0b1790b6rZWxYw/iChozwmj8CSFZRUT2qeqI23eZcunYccvYsbq2uWYnx5m3hJA8kVmDHzRjh7n4hJC8kFmDHyZjh7n4hJA8kFmDP7ZxtWt2jhsFEbp1CCGZJ1NZOm0EjEfPqYbW1ZmYrC5kA53F4C8hpA/InMG3DHE1pJvG8uUHMdrODCAKsRFC+oFMuXQsQ+xl7AXuAv5AcF8+q2oRQvqRTBn8IOJpZ1XKxoBu0ECvqWNg8JcQkmYyZfD9DK6lneMmwRBGVyduh0EIIb0gUwbfz+CeVmqc7ujwEO645iIMVcoQAEOVcihdnbgdBiGE9IJMBW3HNq5uk1Owc2K63hJcjRpgdWr2MEuHENIPZMrgOw1xoamZY8ceXI1jsON0GIQQ0gsyK54GAKvGdxlT8S0hNYtyqUi5ZEJI35NL8TTA26fv7AiYVkkIyTqZNvhuwVUvqlM1SiwQQjJLpg2+WzaOH5RLJoRklUwFbd1wBldXju/yXN9LYoH6OYSQfibzBt9JpVzCVK3uuY7bBK4o+jnsIAghaSLTLh03tly1BqWCt2yyW7A3rH6OXddHEb661sRkFRu2Po5V47uwYevjdDMRQmKTqxG+NeKuz+tCXVu39MyxjavbRucmQTaTnINXB+E3yqcaJyGkE+RmhO9U0pxTRblUxI3rV7RJLABoG52b3glMqZ9xBNaoxkkI6QS5GeGbjOh9Tx0BFDijXML0zCw2b9/vOkNX4T5Zy6SfY3orCCKwRjVOQkgnyMUIf2KyanTJqDaM+FStjhPTdSjQZuwX1kUj6GthibG5EUdgjWqchJBOkHmDPzFZxdhDBxLZVqVcwtuz8wufLTE2t4BqHEVOqnESQjpB5l06X/kfB1Gfi68XVCoKRBAqEBtVYI1qnISQTpBpgz8xWcXJGe8KWEGZnVecmHbP3++Eb51qnISQpEnEpSMil4vIYRF5QUTGXb6/UUSeaf77qYisTWK/fiSZ1eIlKur0rTOHnhCSRmKP8EWkCOBbAP4IwFEAT4vITlV91rbaSwD+jaqeEJEPA7gbwCVx9+1HN7JanL51rxx6gG4aQkjvSMKl834AL6jqiwAgIvcDuBrAgsFX1Z/a1n8SwNkJ7NcXrwlTSTDkYrRN6Z+bt+/HQFEW4gmcTEUI6TZJuHSGALxq+3y0uczEZwD8nelLEblZRPaKyN7jx4/HOrCw8shhGKqU8cT4ZW3G2vRWoUBb8JiTqQgh3SQJg+82CdXV4y0il6Jh8L9k2piq3q2qI6o6snz58lgH5kyNrJRLKBW9dXSC4JYiafntw+YDcTIVIaRbJOHSOQrgHNvnswEcc64kIu8FcA+AD6vq6wnsNxDObBdLIyeqq8fNjeP024eBk6kIId0iCYP/NIDzRWQVgCqA6wF8wr6CiKwA8DCAT6rqLxLYZ2SsDmDD1sdDG/2iCI5N1RbcMPZ8+SjG3iTUlqZgbpqPjRASjtgGX1VnReRzAHYDKAL4rqoeEpHPNr//DoC/AvDPAHxbRABg1lRkt1uMbVwdelRuSS44A65R3DJLB0u4/co1AJBaZcygqp3sFAjpDxLJw1fVR1X1Pap6nqr+h+ay7zSNPVT1z1R1qaqua/7rqbG3DFSUUbmFPeBqcstUyiUUxT1mMLhoAKPDQ4GUMXuV1x/02OLo/hNCukfmtXScOGWS42Btw6R9s+WqNZg3zNiqTtU83UrVqRpWje/Cuq8+hrGHDvTEoAZR7aSUMyH9Q+4MftCRfaVcWsjuMQzSAQDDX3sMANqE0q69uDF698ra8dLZB06pePYqnTOIaielnAnpH3Jn8IMYonKpiI+uPRNAw+h6ySpYipkA8MT4ZXhp6xUY27gaO/aZJZntWDr7YenkhDKgqUP09mzbcmdKataknCmLQbJMpsXT3DDNvi2KYF4VZ1XKuPSC5dixrxrYx+9UzAwbH1A03gqONd02QTDFBtxwBlUvvWA59jx/3BhkNaWZWoFm+7puwe9+lXJmaUmSdXI3wnfztwsaGTiW8dvz/PHQAV3LJz8xWQ3tzlg6WFp4OxgKODK2Mob8RqRuQdV7nzzS8vmW7fsx/LXHFn5r6rCsQLOdOLr/aYPxCJJ1cjfCt4/CLR+6NaquTtUw9uAB1Oej6edbvxfxdgM5sa8bNF10qFIONCIN+rZhd02F9ctnRcqZ8QiSdXJj8Ccmq9iy8xCmag1N+6WDJSwdLLVp3Ec19nF+P1WrY2Ky2mI4/WYDT8/M4pbt+9uWO91LYYyV9ds49Xj7maTPm/MTSNrIhUtnYrKKsQcPLBh7oDGiNRU06QWbt+/HyqZbBmgEgO31c514HbvdyIc1VsemakbRuanpmUwHMZMsLcn5CSSN5MLgb9t9OPbIPS5+qp12t5JlGLZctQalQvgcHgWwcnwXhr/2GC69YHkoxdCzKmWMDg/h2ovbR6InZ+Yw9tCBzBqtJOMRjAeQNCIaxtncZUZGRnTv3r2xt7NqfFdoFcskEQA3rl+BPc8fD5xOackv290CBZGFYG1QSkXBpt8/Z2Hf9piFk3KpiGsvHvI9zkq5hCWLB3rmqugHV4npnhMAL229otuHQ3KEiOwzqRnkwoff6UIofiiAPc8fXzDgX3zggK/httwydr/+yvFdofddn1M8cuA17L/9jwEAt00cxH1PHmkzRgLgfSvOwPanX/Ut+j5Vqy+4x7qdumgKVO995Q3PVNNuHJe9E6q4xIeA7MdBSLrJhcEf27g6VvZNElSnalj31cda4gheuNXJ9Rqde2EFhfe+8oarsUdzu0/88o0IW28PFHcSk6vEfl5p6IRKBUHJVuEM6N/5CSQ75MLgWw+9PUunF4TZ96UXtBZ/8ZNp8MMtoydJrHkInR5he1UUs9PrTqg+rz13fWWJfnDj9QO5MPhAq2uk1z79IHz/qVcxcu6ySOmVnWKwVMB0fd71O8EpuYckR9hBXSVudKvNTPt5s1ZfcKWR6HAGdHLkIkvHST/4UedUWzJiTiv17lJVyiW8vPUKLF2y2LiOaYQdB7fUxv/31mxbmUpTHlO3rnPW9ITSBjOekiOXBj9McfOlg+Zc+E5Tn1N8+eFnAABvz7qPrDtNQYAtV63BxGQwMTg7cUfYJlfJkkUDLamTN65fkVj+fBTC5O9TnC08pvuol4kY/UpuXDp2gs5mLRWl55OzpuvzmJisolfx5qIIHtx7xDOga5KSiDvCDeIqsWZQ2zuGpYMlXPHeM7Fl56GF2IWb8FtS2O8nLx9zVNeE3a11RrkEEWBqup4bX7Ypy06AhRnqJBi5yMP3Ikpt224zVCnjn958K3QOvhcCYNFAIZE3h8FSAQoJpK7pxE3J85EDr3kGuO1zFNyyr6y5as5OsiDANz++rmcGwnSvWefjhkm51KJcKnZVrK4XwdOJySo2b9/vGnfzaru84pWHnxuXjulVOox7JyxRdO7dODZVww2XnJPQ1k6RlJuoVp/HHddc1CYFYQmymdwWJiVPL2Nvd5WYZlDPa7uxt5Zv2Xko+IklTBRxNj/xu276st2ul1NptROMDg8ZkyzSPlhLG7lw6QR5lQ4yGSoM9tm1UWfJWlgTtzactyxyrrzbNpNC0TS+c+0dSK0+hy8+cGDhs310OD0zG1qGuiAN3SE/d5yJXqblRhFnCxIH6VY2kqnzsSutdmq0P0S3TiLkYoTvF+UfHR4y1p6NigIYOXfZgs593O1Xp2r42UsncNP6FW1ZKmmgOlXDyRl34z2nilu278ct2/e3jA6jxEdOzswt/L7bWG+JK8d3YdX4Lqxs/gs6wo0izhYkDtKtbCCvjqXTbxpjG1e7vjFbgw0SjFwY/CCv0p14aOzujCS2X59X7Nh31Dg8T183kD4GS4VIWTJ2dwbQeglOTNddReWcbkSgvfaxn//dz+XYzWwkv3u4k28aXm6dsPvNc6ZULgx+kDzpIA9NmLKCQGPUs2XnoYVgXRIGuVafd/VbF0VSP5ksDdTnteUtY/P2/bht4qDv7/x86fU5bRlpmuSRgVO1j58Yv8zXFeFU8KyUG3UcelFdzK/z6fSbhqkaXJj9ul0XuzR51o1/Lnz4Qequjg4P4as/POTqZqiUSy1pgEEqUlnYhcY6aZCTjD9kFQHahOEUwL1PHsGuZ17zzCgK60v3ciOODg+FynbpVEWxsBk3XhIl3XjTcHuOBe0yJF64XRcvDaasSTrkwuAHzZO+/co1rh3DlqvWuG6LGQL9hVeXeGK6js3NOMOQy/0RRHHVPtL0ciN2SyrAMlbVqRqKzaQB69wARDoGq/PplCH02u7o8FCbAKAC2LGv2iJD4oVfx+3slJO+Tr3uQHKfh+8kzAUJO9r3olIu4e3ZOdQMWjWkuzjz22+bOIh7nzxiXN9ed8ArK0ukca2DvEnGmWzldW+WS0UUBK5B9k7ktXs9U87zPDkz26Ywar8OUecyhKkpYdUsiLIvL9yuSSfmUXjl4dPgx8RZK9dCAAwuKro+VE6ZY+uiJ50aSuJhGWC/GgYFAT7w7mX4+ZE3Y3f+N61f4TvxrFQQnH7agGcHMPy1xyJlQXkVaHGbJOdXg8DNyFnHf2K6Hkjy225gwxaWiTIos/aXdBGbpDsQE7kvgNIJTIbeQgFjKuYfnLcML79ea3tQvCSMo2rhk+hM1epYOb7Lt+3nFfjpL99I5PqY6hXYqc/rgjE3+Z2jSoKYAqDOWc3WJDkL03G4dZT24w/SZnYjGXYugynYbrm43AZflssr6aL2USbeJU0usnS8iJKi5VYU3Q2Te+bl12sLmRpjG1dj2+7DWOVTzerG9Ss8i5p3i6FKGXdtWtfrw+gqQYxSUp1xlO3U6nMtM17jzCaenpl1fQa27DzkW0DInotvjayTeGO1JlcB4ecymIypZezPsGU9VcolnFYqYPP2/diw9XHXetDOfYWxH2lQVc21wTelzvkZ/bhF0a1iIbdNHGzZv4kN5y3DyLnL8Nu3ZiPvMymmZ2axeft+hMxQJV3gxHQdt2zfH2s2sUkOI+g2LQPrl8YaBufkqsUDp8zW0sGSpw/cy5gqGuf1Vn0eN65fgbdn53Fiur5gC3bsq+Lai4eM8ybC2g+vzqpbcwNy7cOP6lNLqoBKUDdNxSWYRUgUgt5zzmcgaD3lQlM5Nek7VQDcuWmdMegJuGfhBfXhmxRf7e3gjGFMz8y6us6CBpCt4wSQaDC340FbEbkcwH8GUARwj6pudXwvze8/AmAawKdV9ed+2+20wY8alOmGwma5VExshCQA/vm7luCFX51sOd+k4wKLioKBghirYmWFUkEAac/pTzsFcReV88JK4/zSjmd6VpMBaPjc31EeMGY3vT0735afr2gcvxVcjvLMWrbApMzq9RsvgmQORQ3mdlQtU0SKAL4F4MMALgRwg4hc6FjtwwDOb/67GcDfxN1vEkT1qY1tXN146B2UioK7Nq0LPSPXjTjGvlSQltmYd25ah+mZ+TbjnrS5mplT1OcVG85blvCWe491RYcqZWz72Fpsu26tceZnWonihbQUMbtp7N2enjlVYyB6qlb3nEy1Y1/V1R8fhDOacbMgMQyLsyplTxeN0xVkinN0IpibRJbO+wG8oKovAoCI3A/gagDP2ta5GsB/08brxJMiUhGRM1X1tQT2H5kgM3DdcJtxaNd+73TBcD9OP22gbdbo5i4dU31O8eSLJ7qyr25iPZLTM404ijUB6bxbH2UqbcIk3Zq1+hy+/9Srka7TyWYQO2gMo1wq4tILlrdN2PrC9v34wgP7Q3W6nQjmJmHwhwC8avt8FMAlAdYZAtBm8EXkZjTeArBixYoEDs9M0Bm4pt+6rTcxWV1I+YpK3N+fmK5j7MED+OoPDy3kaocp/h2XLBtASygNaNwDWT7XfqBcKuK0UsH33o56nZwaSW4MVcot9sMtYD0PhOrJSkXpiFRFEgbfpFoadp3GQtW7AdwNNHz48Q7NnyR1SpJIRRMAN1xyDnbsq8Zy6zhztUsFQakofed3TiP1OcWWnYf6WlMlC9hlIkwVsZKgOlXDUsOAqSBoGyQm8Ta9ZNFAR+6vJNIyjwKwl2M6G8CxCOv0PUmkoimAr49ehGsvHkpU7the/JvEZ6pWz7yyYqdZsqiIm9ZHe4u3AprWgC2IsXc+T9bnIDG3K957pmsdinlFWypmEq6YNztUqCcJg/80gPNFZJWILAJwPYCdjnV2AvgTabAewJu99t93giSCLNbkqj3PH098xPJmrY4nxi8zGv1KuZSKyV39Qq9jNf3OzGzDtx4WtzhbkIGMlbVjT2Z4OWBxokcOvIYli9wdInYZ9FXju3Dy7Vm45HSEoiDSkQFFbJeOqs6KyOcA7EYjLfO7qnpIRD7b/P47AB5FIyXzBTTSMv807n7TSBBFRQtT3q812OhEhL4gglXju3BGudTm3rFUQZ1KiGeUSz0tC0iySz2sY7uJW366WwKGE1OaY5Dn1u8ZsMugJ/G8zKl2REE11xOv/AgrZeo2yaOAZsAmIH5KfV4URTCv6qo66IaVl+0mB2wnqhBXUtjFtoLgnF+QBx2iPJyjHTe5Zy9tq1JRsO26tcZEi6RUb5MmSi5+R/Pws0oU2QVndaKhShlnDLq7SEx+Q8v/Z6outNSwPaAh1vbS1iuw//Y/XsgTF499zeup12Ovjuz2K9eEymEulwqJxR+WDpaw7WNrQx2DNn9nXYMb16+IlIPdT2TJ2Afx61uJEdY8AS9JCavwzbbdh13LUCYpA5E0Sb/pUy3TgF/FIhPOrB+TKNqcattsWrtv0itl1DT6tweL7MfhJcwW9JysY7FKNZoMjJUml5Su/29qswv7DvNQvlWfx52b1i0c+8i5y9qKgZDOEeeNY8/zx3HXpnWJxUhMFa2CjOytAZbp7bJSLuE3b9UjTWoLQsVjgBcFunQMJKWF7aXXY+Xshp0D4KUx7qaR7uceCntObsU5TkzXO2ZIo8pM+GmapPU1PivY89MvvWB5IOlnCz9DG/e4nhi/zPe5sNbzklUoFgRznbL2aC2KExTq4UcgKS1sr9m8UecAOEf/ls/epJHuF9AKe07O445rPK0MC9PDV6vPGTuTikdQ2fk67Fbyrx8pJ/gGFRb7wMKr9UydrVfVMDsnpusoCDoyd8S6L/zcJdWp2sJMalPmZieNPZB8eiYNvoGosgtO4szm9duufQTvNHp2bXLLHWJ6zZ6emcVtEwd9qxeZCOJuKZeKuPbiobYJZfY29eo03FxggkZGhMl4n2FLMb1t4mDLCLN/jX2yLrOw2GU7/sVf/p3xOKxr6nwbDDMinlfgHYsGsGTxgK8rMQzWACdIdo51nyR5u4QZbCQtr0CDbyBJQ+02kjfJpAZZ5tyWaaRijfQtI6nAwoxbu6Lliem6b/UiL7xGSgK0HLflS3eTsV08UDAafLsLzPnwmx6ek7ZiHmHcCWmmVp+L9CaV1FuBpZcPNOIkJkaHh9o62Sjpim/W6m11fsNkrzk7GPsAY2zj6sAKmEkhAL7x8bWu6c9u9XyTllegD78HmHzwTsldt2VuOtkmX6RpJBF0hBE0JSxurU4/l1DQQtZu+LmLOk2pIF01KN3EdB9ZksRB3TdeuN1DQbX5AeCuTes8B0y9SDl+OWDN4KgDTKZlpgw3F0h9Xtt8lW7L7K4aC1MlHZNRD/o6GTQlLGjZOZNkrJdLyFllKMxxWet2s2Yo0JoSuu1jaxPffrmUjsfW7T6y1CLvS8DYA3C9h4Km/FbKJWzZeWghtdpSOrUz1WVj75YibT0XlgbPnZvWLchGJA1dOj0grgFy/t7kfjK9/gYd4Qf1HwZxfzlH8Xa3kak9BIg8M9J5Dt0a4VfKJUz+VWtWRVg3hBdRfPilouD0xcEnroXFfj/V6nOJjOwBuAZKt+0+HMg1VypImwvJqXQKhLuXkmBOFROT1YX9O91eYd2pYaFLpwfErZgVx1ViCp46iVNizQ0vtw/gbpBN5xnUJWYvf9eNFExTmyWZAhqlalWlXMKWq9b0ZRqqs4RhkOdm6WAJJ9+exYwhu8dZtjCpdjEpaprWveK9ZxpjS1GrXQF06aQONxeIFUz1WxYmkOM28/eOay7C10cvalt+0/oVxmLNSWAaxR+bqgV2CXmdl70Klds5OAtf37VpHe7atK5tv1EfCK82sx8vcOq1vtLUNLLj566IEg6YqtUXjiEoglOuqV5Sq89hc3MmbdBBkiqMxh5ovRdHh4cSU6Z95+mLAq97YrrumUhQnap1pJg5R/g9IsksnX7AL7CbVMDKiektxzLOXtchSCpg3DehoEWt42AVAPeape32G2syXjdqOHcbq0Id0J23v6hEub86XsS8U/SDwe+UocoafobXvl6S7ZlEBpH9eKyC2J2+3mEVS/06JnvHGsTAdcrtkSY6HdtIirDuHc607RBegcigEgl56SziBnajtqeXKynocffimlj7tdrEFx+Lb52vaZa2X/631xyJfqU+Zy6MniaSzDKjwY9BVIE1IL5xSytenZjfBLSCS/ZQ3PY01fKNOoOx2510UNE4vxd1k7Ae4H1OaR7de8kuxJ2Vmya56SRn29LgxyDO6DFOZ5FWwnZizvVNqaJx2nPxQMFTlTQMQc8vyU4h6OjOK9XW73y93mLSKh0sALZdt9aoqKk41SbebVPAW/X5FuPulvHVK5KebcssnRiYet4gPXJcV0Ma8erEgq7vRpz2fLNWd81UimKAg5xflDoKXgQd3S0eaM/oAhrByThB5bTejzeuX4HR4SHP0oaW/tINl5xjNHQzc9pWLGfRQMHX2Ltl0CVN3GvnBkf4MYgjsJaUGmeaCNuJBTEmpYJgemYWq8Z3eY6WvdozKT98kPOI6+Zzvhm43WNuekjT9XmUCoKlgyVXiewoTExWXd1sQDgBsKQplwr4+mgjxdRPCbZWn8Oe54+jvKiIkzPt6ziF3BRwXc+OPcOnE3WN/SrQxYEj/BiY8tyDXKiwuef9QNg3HtPyoggEzYLuTa19v9FyN9ozyHlE7dxMbwZ7X3mjbQ7Bto+txdIli9u2UZ9XDC4awEtbr4g9Nd86HpN8Qq+MfakouOOa9y58ds5xcKM6VfM14mEYXDSwMIgIUp3LoiiCmwJUX+tkXIgGPyajw0N4Yvyy0A9ZnM4irYQ1uqb1v/HxtXhp6xVYsnggkJYQ0J32DHJ+Ud18pjeD+5480pKSaSlUdtolaHK3FUV8DWynWDpYcq1La9V86NYkMXsbf330Imw4b1mg382rLkx6NJUdBWB0gSYBXTo9pFcpf50irKS03/phjVqn2zPI+UV185nOyTmOtjq8TrsETcczp7pwvm6upnm0ukmSynV35qI73V/TM7Ndy6pxtvF9/+4Dvtlm1u+s9bzekKxZtp0Y6dPgk0RJ0uimMc7hd35R6ih4+crdODZVw52b1vl2LHGyhUxtL83tms7TtMx5rFbaY8VlHoATt/NyZkt1C1Pnbb8vnIJo1u8uvWB54BTXTqVpc6Yt6RlBZA+CzM51brOfJrN55bmbcsGDyFG4bdfaXpCg4MRkFZu3709M2MvvWMPMZu621IP9Ogw2pantAfPBUgGLS0VMTdddJ7IJGllFe54/Hvq4o7Q1pRVIKgkiexDGgEfpIHqNV/GaGy45x7UkZJDz8TOKQbZjKjQStui9k7id8qrxXV1z3ySVk28VdA+7lShtTWkFkkqC+OjDuIj6cTKbqQ2sAJ+pJGTU7VoEaZehDrjUkphh3i0N+6I0CrYnIb9QnaoZ29NLIpw1bUlmSNpH3+3JbEm4j/zaIGpMJIhRdGsXp2ibU74gbqpr3E55YrKKk2+3V66yqJRL+O1bs8Z4SFEE69+9FD976YRn6clSUTxn8kbh0guWu76xecU5Lr1geWL7B5iWSXwwlSVMgqRz5+PMfA5LUjNqOzV/wG27Tpzt4jynqVod0NaSjXHdY16dst+9Zh2fl3Lom7U6vvHxta5tetemdfjGx9fi5ddrvnWGBwqSeHrknueP+6QOt08C27GvmugzxxF+SklD8LHTAm9RMlq8iDPzOSxJuY+SbgO37brp+ru1i6nW8uCigbayjVExvXmcUS753mtf/eEh3wwXa2Y1ECxbyEStPp+42+jYVM0oINg4rvaylUm7JGnwU0halDS74RNPMo2zU8bTjSTdR52aP+A0+pYcgilLpxsuMVOnLALPe21isurrS7d3Ym5tumHr4z0VgtPmMTjb3k9TivLIGSctwcd+FHjr1mS2NM4RcOKmRmoZxbB6RElh6pQ3G3zl1r321R8e8txukFTTpO5bv4ppe195w1i+sDpVw9iDrYXU/Y4ryfanDz+FpMXQdtMn3m/0gxZSWPXSbp2TmxyJ173mN7q/a9M6o6yJPS5Q8JAzsChKQ4DOhNPvbp3LnZvWAQA2b9+PPc8fx43NGtFu1OcVW3ae6sC8nqdUySOLyDIR+ZGI/GPz/6Uu65wjIntE5DkROSQin4+zzzyQFkPbD0atV4wON4pfW5ooRRFce3G6pDKiSFP0St/J617zCp5WyiXfeRlWENpvJrOl43T7lWuMQV+3jsUtgL9jX9XzObEHnk0B9jTKI48D+LGqbhWR8ebnLznWmQXwRVX9uYj8DoB9IvIjVX025r4zSzeDj1500yfeb0xMVrFjX3XBiMypYse+KkbOXZaa9onioumVvpPXvWZy9wDAlqvWGL8LU7zFkjx2+taD3Pdh36ScdPM5i2vwrwbwwebf3wPwEzgMvqq+BuC15t+/FZHnAAwBoME3kCZDmzWBt6RIS5zFi7QMHIJiutdMHZfX6B4I5wJ9y5EhE+a+93qTWmoosel0G3XrOYtr8H+3adChqq+JyLu8VhaRlQCGATwVc7+Zh4a293ilxqYlzuJFVCG3NAw07Jg6Lq/RPRBuRm6cztor1fT2K9dg7KEDLZPXSkVZKKDSbXwNvoj8PYDfc/nqK2F2JCKnA9gB4BZV/Y3HejcDuBkAVqwIXlyAkCTxS43thywdINzAIS3pwE6ivvH6VcNyErWzHtu4GmMPHmibzHVypjEjeNt1a1PTicYSTxORwwA+2BzdnwngJ6ra9r4oIiUAjwDYrarfDLp9iqeRTuE3kvUTdutHoTY/gojZ9RtOqQgvKeY45zn8tcdcXTdBtpn0W1UnxdN2AvgUgK3N/3/gsnMB8LcAngtj7AnpFEFGsn4umzTFWZKiH9xUYXG+4UxMVrFl56E2eYa4sY0pQ9po0NKW3XqrimvwtwJ4QEQ+A+AIgI8BgIicBeAeVf0IgA0APgngoIjsb/7uy6r6aMx9ExKJIAHXIC6brMVZ+sVNZSfs6Ni6ZkmPqr38+Bu2Pm7cT7eD/7EMvqq+DuBDLsuPAfhI8+//A3St3CQhvgQZyfZbhksS9Ns5m0bHe195w7OACpB8Z+3WdqWC4OTM7MLbRJQ3yaThTFuSO4JMbOvlJKRe0W/nbBod3/vkkdgqpmFxa7vTTxtoixc48/O7PcmSFa9IT+hl+l8WA655JEzlq14Enk3HZ69i1Yl7kRWvSKrodfpfFgOueaRimNTkRjcCz85BjOn4nG+SQPfuRRp80nXSMEs1awHXPBLGOdHpwLPbIKZUkEAVw7p5L9KHT7pOFtP/SPd506PylZ1uBJ5NxWOWLBpIVUyEI3zSdfox/Y+kDy/phKWDJUxN17vmrjMNVt6s1bH/9mSqhSUBDT7pOv2W/pcG0qhx02vc7iMBcOP6Ffj66EVdPZZ+GcTQ4JOuw6BpOHod5E4rvbiPTB1vvwximJZJSMrJosZNP+KXQpmWtzCmZRLSxzDInQ78ssv6IfOLWTqEpJy0lLzMO1noeGnwCUk5rC2cDrLQ8dLgE5Jy+k3jJqtkoeOlD5+QPqAf/MNZJwvZZTT4hBASkH7veOnSIYSQnECDTwghOYEGnxBCcgINPiGE5AQafEIIyQk0+IQQkhOYlklID0iL0BZJjn64pjT4hHQZyh1nj365pnTpENJlvFQXSX/SL9eUBp+QLpMF1UXSSr9cUxp8QrpMFlQXSSv9ck1p8AnpMllQXSSt9Ms1ZdCWkC6TBdVF0kq/XFPWtCWEkAzhVdOWLh1CCMkJNPiEEJITYvnwRWQZgO0AVgJ4GcDHVfWEYd0igL0Aqqr60Tj7JYSQqPTDjNhOEXeEPw7gx6p6PoAfNz+b+DyA52LujxBCImPNiK1O1aA4NSN2YrLa60PrCnEN/tUAvtf8+3sARt1WEpGzAVwB4J6Y+yOEkMj0y4zYThHX4P+uqr4GAM3/32VY7y4AfwFgPub+CCEkMv0yI7ZT+PrwReTvAfyey1dfCbIDEfkogF+p6j4R+WCA9W8GcDMArFixIsguCCEkEGdVyqi6GPe0zYjtFL4jfFX9Q1X9ly7/fgDg/4rImQDQ/P9XLpvYAOAqEXkZwP0ALhORez32d7eqjqjqyPLlyyOdFCGEuNEvM2I7RVyXzk4An2r+/SkAP3CuoKq3qurZqroSwPUAHlfVm2LulxBCQjM6PIQ7rrkIQ5UyBMBQpYw7rrkoN1k6caUVtgJ4QEQ+A+AIgI8BgIicBeAeVf1IzO0TQkiijA4P5cbAO4ll8FX1dQAfcll+DECbsVfVnwD4SZx9EkIIiQZn2hJCSE6gwSeEkJxAg08IITmBBp8QQnJCqvXwReQ4gFci/vydAH6d4OH0O2yPU7AtWmF7tNLv7XGuqrpOYkq1wY+DiOw1FQHII2yPU7AtWmF7tJLl9qBLhxBCcgINPiGE5IQsG/y7e30AKYPtcQq2RStsj1Yy2x6Z9eETQghpJcsjfEIIITZo8AkhJCdkzuCLyOUiclhEXhARrxq7mUFEzhGRPSLynIgcEpHPN5cvE5Eficg/Nv9favvNrc02OiwiG3t39J1BRIoiMikijzQ/57ktKiLykIg837xHPpDz9tjcfE7+QUS+LyKn5aY9VDUz/wAUAfwSwLsBLAJwAMCFvT6uLpz3mQDe1/z7dwD8AsCFAP4jgPHm8nEAf938+8Jm2ywGsKrZZsVen0fCbfIFAP8dwCPNz3lui+8B+LPm34sAVPLaHgCGALwEoNz8/ACAT+elPbI2wn8/gBdU9UVVnUGjwtbVPT6mjqOqr6nqz5t//xbAc2jc2KYi81cDuF9V31bVlwC8gEbbZQIRORvAFQDusS3Oa1u8A8C/BvC3AKCqM6o6hZy2R5MBAGURGQAwCOAYctIeWTP4QwBetX0+2lyWG0RkJYBhAE/BXGQ+6+10F4C/ADBvW5bXtng3gOMA/kvTxXWPiCxBTttDVasA/hMaBZteA/Cmqj6GnLRH1gy+uCzLTd6piJwOYAeAW1T1N16ruizLRDuJyEcB/EpV9wX9icuyTLRFkwEA7wPwN6o6DOAkGi4LE5luj6Zv/mo03DNnAVgiIl4lVzPVHlkz+EcBnGP7fDYar2uZR0RKaBj7+1T14eZiU5H5LLfTBgBXicjLaLj0LhORe5HPtgAa53dUVZ9qfn4IjQ4gr+3xhwBeUtXjqloH8DCAP0BO2iNrBv9pAOeLyCoRWYRG0fSdPT6mjiMigoaP9jlV/abtK1OR+Z0ArheRxSKyCsD5AH7WrePtJKp6q6qeraor0bj+j6vqTchhWwCAqv4TgFdFZHVz0YcAPIuctgcarpz1IjLYfG4+hEbMKxftEbeIeapQ1VkR+RyA3Whk7HxXVQ/1+LC6wQYAnwRwUET2N5d9GYYi86p6SEQeQOPBnwXw71V1rutH3V3y3BZ/DuC+5iDoRQB/isZgL3ftoapPichDAH6OxvlNoiGlcDpy0B6UViCEkJyQNZcOIYQQAzT4hBCSE2jwCSEkJ9DgE0JITqDBJ4SQnECDTwghOYEGnxBCcsL/ByUdYgTbIt3xAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot the errors\n",
    "plt.scatter(results.index,results['error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4be18a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    900.000000\n",
       "mean       0.004406\n",
       "std        0.107960\n",
       "min       -0.396476\n",
       "25%       -0.056565\n",
       "50%       -0.002663\n",
       "75%        0.056967\n",
       "max        0.589273\n",
       "Name: error, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explore errors\n",
    "results['error'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4044bf68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    900.000000\n",
       "mean       0.076651\n",
       "std        0.076112\n",
       "min        0.000210\n",
       "25%        0.023751\n",
       "50%        0.056817\n",
       "75%        0.106214\n",
       "max        0.589273\n",
       "Name: abs_error, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#explor absolute error\n",
    "results['abs_error'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "66910f0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    900.000000\n",
       "mean       0.194706\n",
       "std        0.115863\n",
       "min        0.007141\n",
       "25%        0.112095\n",
       "50%        0.175738\n",
       "75%        0.252518\n",
       "max        0.716846\n",
       "Name: act, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#explore case percents\n",
    "results['act'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f9ffe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23e0619",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
