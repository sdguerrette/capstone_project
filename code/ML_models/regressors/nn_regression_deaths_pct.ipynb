{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6073e73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xlrd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sqlalchemy import create_engine\n",
    "from config import db_password\n",
    "import psycopg2\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c47c3d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIPS</th>\n",
       "      <th>AREA_SQMI</th>\n",
       "      <th>E_TOTPOP</th>\n",
       "      <th>E_HU</th>\n",
       "      <th>E_HH</th>\n",
       "      <th>E_POV</th>\n",
       "      <th>E_UNEMP</th>\n",
       "      <th>E_PCI</th>\n",
       "      <th>E_NOHSDP</th>\n",
       "      <th>E_AGE65</th>\n",
       "      <th>...</th>\n",
       "      <th>Gender Equality</th>\n",
       "      <th>Hopefulness</th>\n",
       "      <th>Income Per Capita</th>\n",
       "      <th>Neuroticism</th>\n",
       "      <th>Openness</th>\n",
       "      <th>Religiosity</th>\n",
       "      <th>Risk Taking</th>\n",
       "      <th>Selflessness</th>\n",
       "      <th>Tolerance</th>\n",
       "      <th>Work Ethic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>594.443459</td>\n",
       "      <td>55200</td>\n",
       "      <td>23315</td>\n",
       "      <td>21115</td>\n",
       "      <td>8422</td>\n",
       "      <td>1065</td>\n",
       "      <td>29372</td>\n",
       "      <td>4204</td>\n",
       "      <td>8050</td>\n",
       "      <td>...</td>\n",
       "      <td>77.063492</td>\n",
       "      <td>91.163142</td>\n",
       "      <td>26168.0</td>\n",
       "      <td>77.925476</td>\n",
       "      <td>78.222354</td>\n",
       "      <td>91.106719</td>\n",
       "      <td>53.333333</td>\n",
       "      <td>82.142857</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>60.380952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1009</td>\n",
       "      <td>644.830460</td>\n",
       "      <td>57645</td>\n",
       "      <td>24222</td>\n",
       "      <td>20600</td>\n",
       "      <td>8220</td>\n",
       "      <td>909</td>\n",
       "      <td>22656</td>\n",
       "      <td>7861</td>\n",
       "      <td>10233</td>\n",
       "      <td>...</td>\n",
       "      <td>64.585114</td>\n",
       "      <td>79.492703</td>\n",
       "      <td>21033.0</td>\n",
       "      <td>78.764620</td>\n",
       "      <td>78.193105</td>\n",
       "      <td>92.045455</td>\n",
       "      <td>57.603815</td>\n",
       "      <td>79.307632</td>\n",
       "      <td>64.953288</td>\n",
       "      <td>76.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1013</td>\n",
       "      <td>776.838201</td>\n",
       "      <td>20025</td>\n",
       "      <td>10026</td>\n",
       "      <td>6708</td>\n",
       "      <td>4640</td>\n",
       "      <td>567</td>\n",
       "      <td>20430</td>\n",
       "      <td>2141</td>\n",
       "      <td>3806</td>\n",
       "      <td>...</td>\n",
       "      <td>64.769089</td>\n",
       "      <td>83.523765</td>\n",
       "      <td>19011.0</td>\n",
       "      <td>78.563680</td>\n",
       "      <td>76.109761</td>\n",
       "      <td>76.623924</td>\n",
       "      <td>69.058104</td>\n",
       "      <td>79.956648</td>\n",
       "      <td>67.920284</td>\n",
       "      <td>72.773953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1015</td>\n",
       "      <td>605.867251</td>\n",
       "      <td>115098</td>\n",
       "      <td>53682</td>\n",
       "      <td>45033</td>\n",
       "      <td>20819</td>\n",
       "      <td>4628</td>\n",
       "      <td>24706</td>\n",
       "      <td>12620</td>\n",
       "      <td>19386</td>\n",
       "      <td>...</td>\n",
       "      <td>69.015332</td>\n",
       "      <td>83.365608</td>\n",
       "      <td>22231.0</td>\n",
       "      <td>79.439032</td>\n",
       "      <td>79.955121</td>\n",
       "      <td>77.918741</td>\n",
       "      <td>54.063568</td>\n",
       "      <td>76.745724</td>\n",
       "      <td>67.456150</td>\n",
       "      <td>68.292794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1017</td>\n",
       "      <td>596.560643</td>\n",
       "      <td>33826</td>\n",
       "      <td>16981</td>\n",
       "      <td>13516</td>\n",
       "      <td>5531</td>\n",
       "      <td>773</td>\n",
       "      <td>22827</td>\n",
       "      <td>4383</td>\n",
       "      <td>6409</td>\n",
       "      <td>...</td>\n",
       "      <td>69.433309</td>\n",
       "      <td>85.371517</td>\n",
       "      <td>21532.0</td>\n",
       "      <td>76.995358</td>\n",
       "      <td>78.156771</td>\n",
       "      <td>75.891100</td>\n",
       "      <td>67.343775</td>\n",
       "      <td>79.128558</td>\n",
       "      <td>66.397785</td>\n",
       "      <td>69.554441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3053</th>\n",
       "      <td>48229</td>\n",
       "      <td>4570.523160</td>\n",
       "      <td>4098</td>\n",
       "      <td>1562</td>\n",
       "      <td>900</td>\n",
       "      <td>951</td>\n",
       "      <td>101</td>\n",
       "      <td>14190</td>\n",
       "      <td>1263</td>\n",
       "      <td>639</td>\n",
       "      <td>...</td>\n",
       "      <td>67.196038</td>\n",
       "      <td>55.568966</td>\n",
       "      <td>14776.0</td>\n",
       "      <td>76.720396</td>\n",
       "      <td>79.603081</td>\n",
       "      <td>73.986415</td>\n",
       "      <td>70.917126</td>\n",
       "      <td>79.605796</td>\n",
       "      <td>75.878105</td>\n",
       "      <td>71.008448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3054</th>\n",
       "      <td>48131</td>\n",
       "      <td>1793.476183</td>\n",
       "      <td>11355</td>\n",
       "      <td>5592</td>\n",
       "      <td>3511</td>\n",
       "      <td>2751</td>\n",
       "      <td>482</td>\n",
       "      <td>17864</td>\n",
       "      <td>2386</td>\n",
       "      <td>2025</td>\n",
       "      <td>...</td>\n",
       "      <td>66.171080</td>\n",
       "      <td>77.899678</td>\n",
       "      <td>19853.0</td>\n",
       "      <td>79.125428</td>\n",
       "      <td>78.895880</td>\n",
       "      <td>76.629575</td>\n",
       "      <td>60.576045</td>\n",
       "      <td>73.670302</td>\n",
       "      <td>64.571017</td>\n",
       "      <td>68.007770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3055</th>\n",
       "      <td>48505</td>\n",
       "      <td>998.411980</td>\n",
       "      <td>14369</td>\n",
       "      <td>6388</td>\n",
       "      <td>4405</td>\n",
       "      <td>5609</td>\n",
       "      <td>621</td>\n",
       "      <td>17228</td>\n",
       "      <td>3226</td>\n",
       "      <td>1999</td>\n",
       "      <td>...</td>\n",
       "      <td>67.037410</td>\n",
       "      <td>86.586509</td>\n",
       "      <td>16007.0</td>\n",
       "      <td>79.355639</td>\n",
       "      <td>79.572483</td>\n",
       "      <td>74.378252</td>\n",
       "      <td>77.443239</td>\n",
       "      <td>76.386871</td>\n",
       "      <td>74.001471</td>\n",
       "      <td>73.609838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3056</th>\n",
       "      <td>48507</td>\n",
       "      <td>1297.406535</td>\n",
       "      <td>12131</td>\n",
       "      <td>4344</td>\n",
       "      <td>3509</td>\n",
       "      <td>4150</td>\n",
       "      <td>421</td>\n",
       "      <td>13350</td>\n",
       "      <td>2719</td>\n",
       "      <td>1665</td>\n",
       "      <td>...</td>\n",
       "      <td>65.804541</td>\n",
       "      <td>88.785822</td>\n",
       "      <td>13393.0</td>\n",
       "      <td>78.392216</td>\n",
       "      <td>76.024682</td>\n",
       "      <td>75.848196</td>\n",
       "      <td>76.967659</td>\n",
       "      <td>77.303576</td>\n",
       "      <td>70.010162</td>\n",
       "      <td>71.121990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3057</th>\n",
       "      <td>48247</td>\n",
       "      <td>1136.172598</td>\n",
       "      <td>5282</td>\n",
       "      <td>2523</td>\n",
       "      <td>1710</td>\n",
       "      <td>1431</td>\n",
       "      <td>229</td>\n",
       "      <td>17798</td>\n",
       "      <td>812</td>\n",
       "      <td>932</td>\n",
       "      <td>...</td>\n",
       "      <td>68.661277</td>\n",
       "      <td>82.727613</td>\n",
       "      <td>16637.0</td>\n",
       "      <td>80.157542</td>\n",
       "      <td>79.708963</td>\n",
       "      <td>74.399849</td>\n",
       "      <td>74.024835</td>\n",
       "      <td>77.564505</td>\n",
       "      <td>72.456881</td>\n",
       "      <td>70.160988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3058 rows × 105 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       FIPS    AREA_SQMI  E_TOTPOP   E_HU   E_HH  E_POV  E_UNEMP  E_PCI  \\\n",
       "0      1001   594.443459     55200  23315  21115   8422     1065  29372   \n",
       "1      1009   644.830460     57645  24222  20600   8220      909  22656   \n",
       "2      1013   776.838201     20025  10026   6708   4640      567  20430   \n",
       "3      1015   605.867251    115098  53682  45033  20819     4628  24706   \n",
       "4      1017   596.560643     33826  16981  13516   5531      773  22827   \n",
       "...     ...          ...       ...    ...    ...    ...      ...    ...   \n",
       "3053  48229  4570.523160      4098   1562    900    951      101  14190   \n",
       "3054  48131  1793.476183     11355   5592   3511   2751      482  17864   \n",
       "3055  48505   998.411980     14369   6388   4405   5609      621  17228   \n",
       "3056  48507  1297.406535     12131   4344   3509   4150      421  13350   \n",
       "3057  48247  1136.172598      5282   2523   1710   1431      229  17798   \n",
       "\n",
       "      E_NOHSDP  E_AGE65  ...  Gender Equality  Hopefulness  Income Per Capita  \\\n",
       "0         4204     8050  ...        77.063492    91.163142            26168.0   \n",
       "1         7861    10233  ...        64.585114    79.492703            21033.0   \n",
       "2         2141     3806  ...        64.769089    83.523765            19011.0   \n",
       "3        12620    19386  ...        69.015332    83.365608            22231.0   \n",
       "4         4383     6409  ...        69.433309    85.371517            21532.0   \n",
       "...        ...      ...  ...              ...          ...                ...   \n",
       "3053      1263      639  ...        67.196038    55.568966            14776.0   \n",
       "3054      2386     2025  ...        66.171080    77.899678            19853.0   \n",
       "3055      3226     1999  ...        67.037410    86.586509            16007.0   \n",
       "3056      2719     1665  ...        65.804541    88.785822            13393.0   \n",
       "3057       812      932  ...        68.661277    82.727613            16637.0   \n",
       "\n",
       "      Neuroticism   Openness  Religiosity  Risk Taking  Selflessness  \\\n",
       "0       77.925476  78.222354    91.106719    53.333333     82.142857   \n",
       "1       78.764620  78.193105    92.045455    57.603815     79.307632   \n",
       "2       78.563680  76.109761    76.623924    69.058104     79.956648   \n",
       "3       79.439032  79.955121    77.918741    54.063568     76.745724   \n",
       "4       76.995358  78.156771    75.891100    67.343775     79.128558   \n",
       "...           ...        ...          ...          ...           ...   \n",
       "3053    76.720396  79.603081    73.986415    70.917126     79.605796   \n",
       "3054    79.125428  78.895880    76.629575    60.576045     73.670302   \n",
       "3055    79.355639  79.572483    74.378252    77.443239     76.386871   \n",
       "3056    78.392216  76.024682    75.848196    76.967659     77.303576   \n",
       "3057    80.157542  79.708963    74.399849    74.024835     77.564505   \n",
       "\n",
       "      Tolerance  Work Ethic  \n",
       "0     70.000000   60.380952  \n",
       "1     64.953288   76.000000  \n",
       "2     67.920284   72.773953  \n",
       "3     67.456150   68.292794  \n",
       "4     66.397785   69.554441  \n",
       "...         ...         ...  \n",
       "3053  75.878105   71.008448  \n",
       "3054  64.571017   68.007770  \n",
       "3055  74.001471   73.609838  \n",
       "3056  70.010162   71.121990  \n",
       "3057  72.456881   70.160988  \n",
       "\n",
       "[3058 rows x 105 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create engine and connect to AWS RDS\n",
    "db_string = f\"postgresql://postgres:{db_password}@capstone-db.cutxgn80t57o.us-west-1.rds.amazonaws.com\"\n",
    "engine = create_engine(db_string)\n",
    "# read and check merged cases table\n",
    "df = pd.read_sql('deaths_merged_full', con = engine)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c44e314e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set index to FIPS\n",
    "df = df.set_index(df['FIPS'])\n",
    "df= df.drop(['FIPS'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e97997a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use regex to split svi columns into distinct categories\n",
    "cols = df.columns.to_list()\n",
    "col_series = pd.Series(cols)\n",
    "pct_str = r'^[ERS]P+.'\n",
    "pct_form = col_series.str.contains(pct_str)\n",
    "pct_col = col_series[pct_form].to_list()\n",
    "flag_str = r'^F+.'\n",
    "flag_form = col_series.str.contains(flag_str)\n",
    "flag_col = col_series[flag_form].to_list()\n",
    "val_str = r'^E_+.'\n",
    "val_form = col_series.str.contains(val_str)\n",
    "val_col = col_series[val_form].to_list()\n",
    "non_svi = col_series[~pct_form & ~flag_form & ~val_form].to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f26935a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter cases_df to only have svi estimated percentage columns\n",
    "cases_df = df.drop(columns = flag_col)\n",
    "cases_df = cases_df.drop(columns = val_col)\n",
    "cases_df.columns.to_list()\n",
    "cases_df = cases_df.merge(df['E_TOTPOP'], how = 'left', left_index=True, right_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3286057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AREA_SQMI',\n",
       " 'EP_POV',\n",
       " 'EP_UNEMP',\n",
       " 'EP_PCI',\n",
       " 'EP_NOHSDP',\n",
       " 'EP_AGE65',\n",
       " 'EP_AGE17',\n",
       " 'EP_DISABL',\n",
       " 'EP_SNGPNT',\n",
       " 'EP_MINRTY',\n",
       " 'EP_LIMENG',\n",
       " 'EP_MUNIT',\n",
       " 'EP_MOBILE',\n",
       " 'EP_CROWD',\n",
       " 'EP_NOVEH',\n",
       " 'EP_GROUPQ',\n",
       " 'EPL_POV',\n",
       " 'EPL_UNEMP',\n",
       " 'EPL_PCI',\n",
       " 'EPL_NOHSDP',\n",
       " 'SPL_THEME1',\n",
       " 'RPL_THEME1',\n",
       " 'EPL_AGE65',\n",
       " 'EPL_AGE17',\n",
       " 'EPL_DISABL',\n",
       " 'EPL_SNGPNT',\n",
       " 'SPL_THEME2',\n",
       " 'RPL_THEME2',\n",
       " 'EPL_MINRTY',\n",
       " 'EPL_LIMENG',\n",
       " 'SPL_THEME3',\n",
       " 'RPL_THEME3',\n",
       " 'EPL_MUNIT',\n",
       " 'EPL_MOBILE',\n",
       " 'EPL_CROWD',\n",
       " 'EPL_NOVEH',\n",
       " 'EPL_GROUPQ',\n",
       " 'SPL_THEME4',\n",
       " 'RPL_THEME4',\n",
       " 'SPL_THEMES',\n",
       " 'RPL_THEMES',\n",
       " 'EP_UNINSUR',\n",
       " 'first_yr_deaths',\n",
       " 'num_beds',\n",
       " 'dem_pct',\n",
       " 'Agreeableness',\n",
       " 'Belief In Science',\n",
       " 'Collectivism',\n",
       " 'Conflict Awareness',\n",
       " 'Conscientiousness',\n",
       " 'Empathy',\n",
       " 'Employment Rate',\n",
       " 'Entrepreneurship',\n",
       " 'Extraversion',\n",
       " 'Gender Equality',\n",
       " 'Hopefulness',\n",
       " 'Income Per Capita',\n",
       " 'Neuroticism',\n",
       " 'Openness',\n",
       " 'Religiosity',\n",
       " 'Risk Taking',\n",
       " 'Selflessness',\n",
       " 'Tolerance',\n",
       " 'Work Ethic',\n",
       " 'E_TOTPOP']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cases_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdcc35d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AREA_SQMI</th>\n",
       "      <th>EP_POV</th>\n",
       "      <th>EP_UNEMP</th>\n",
       "      <th>EP_PCI</th>\n",
       "      <th>EP_NOHSDP</th>\n",
       "      <th>EP_AGE65</th>\n",
       "      <th>EP_AGE17</th>\n",
       "      <th>EP_DISABL</th>\n",
       "      <th>EP_SNGPNT</th>\n",
       "      <th>EP_MINRTY</th>\n",
       "      <th>...</th>\n",
       "      <th>Hopefulness</th>\n",
       "      <th>Income Per Capita</th>\n",
       "      <th>Neuroticism</th>\n",
       "      <th>Openness</th>\n",
       "      <th>Religiosity</th>\n",
       "      <th>Risk Taking</th>\n",
       "      <th>Selflessness</th>\n",
       "      <th>Tolerance</th>\n",
       "      <th>Work Ethic</th>\n",
       "      <th>E_TOTPOP</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FIPS</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>594.443459</td>\n",
       "      <td>15.4</td>\n",
       "      <td>4.2</td>\n",
       "      <td>29372.0</td>\n",
       "      <td>11.3</td>\n",
       "      <td>14.6</td>\n",
       "      <td>24.2</td>\n",
       "      <td>19.3</td>\n",
       "      <td>7.5</td>\n",
       "      <td>25.0</td>\n",
       "      <td>...</td>\n",
       "      <td>91.163142</td>\n",
       "      <td>26168.0</td>\n",
       "      <td>77.925476</td>\n",
       "      <td>78.222354</td>\n",
       "      <td>91.106719</td>\n",
       "      <td>53.333333</td>\n",
       "      <td>82.142857</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>60.380952</td>\n",
       "      <td>55200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>644.830460</td>\n",
       "      <td>14.4</td>\n",
       "      <td>4.1</td>\n",
       "      <td>22656.0</td>\n",
       "      <td>19.8</td>\n",
       "      <td>17.8</td>\n",
       "      <td>23.4</td>\n",
       "      <td>14.2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>12.9</td>\n",
       "      <td>...</td>\n",
       "      <td>79.492703</td>\n",
       "      <td>21033.0</td>\n",
       "      <td>78.764620</td>\n",
       "      <td>78.193105</td>\n",
       "      <td>92.045455</td>\n",
       "      <td>57.603815</td>\n",
       "      <td>79.307632</td>\n",
       "      <td>64.953288</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>57645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>776.838201</td>\n",
       "      <td>23.5</td>\n",
       "      <td>6.7</td>\n",
       "      <td>20430.0</td>\n",
       "      <td>15.4</td>\n",
       "      <td>19.0</td>\n",
       "      <td>22.8</td>\n",
       "      <td>17.7</td>\n",
       "      <td>10.5</td>\n",
       "      <td>48.1</td>\n",
       "      <td>...</td>\n",
       "      <td>83.523765</td>\n",
       "      <td>19011.0</td>\n",
       "      <td>78.563680</td>\n",
       "      <td>76.109761</td>\n",
       "      <td>76.623924</td>\n",
       "      <td>69.058104</td>\n",
       "      <td>79.956648</td>\n",
       "      <td>67.920284</td>\n",
       "      <td>72.773953</td>\n",
       "      <td>20025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>605.867251</td>\n",
       "      <td>18.6</td>\n",
       "      <td>8.8</td>\n",
       "      <td>24706.0</td>\n",
       "      <td>15.9</td>\n",
       "      <td>16.8</td>\n",
       "      <td>21.9</td>\n",
       "      <td>20.8</td>\n",
       "      <td>10.4</td>\n",
       "      <td>27.5</td>\n",
       "      <td>...</td>\n",
       "      <td>83.365608</td>\n",
       "      <td>22231.0</td>\n",
       "      <td>79.439032</td>\n",
       "      <td>79.955121</td>\n",
       "      <td>77.918741</td>\n",
       "      <td>54.063568</td>\n",
       "      <td>76.745724</td>\n",
       "      <td>67.456150</td>\n",
       "      <td>68.292794</td>\n",
       "      <td>115098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>596.560643</td>\n",
       "      <td>16.6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>22827.0</td>\n",
       "      <td>18.6</td>\n",
       "      <td>18.9</td>\n",
       "      <td>20.7</td>\n",
       "      <td>16.7</td>\n",
       "      <td>9.7</td>\n",
       "      <td>44.2</td>\n",
       "      <td>...</td>\n",
       "      <td>85.371517</td>\n",
       "      <td>21532.0</td>\n",
       "      <td>76.995358</td>\n",
       "      <td>78.156771</td>\n",
       "      <td>75.891100</td>\n",
       "      <td>67.343775</td>\n",
       "      <td>79.128558</td>\n",
       "      <td>66.397785</td>\n",
       "      <td>69.554441</td>\n",
       "      <td>33826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48229</th>\n",
       "      <td>4570.523160</td>\n",
       "      <td>28.2</td>\n",
       "      <td>8.3</td>\n",
       "      <td>14190.0</td>\n",
       "      <td>46.1</td>\n",
       "      <td>15.6</td>\n",
       "      <td>23.9</td>\n",
       "      <td>27.2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>82.4</td>\n",
       "      <td>...</td>\n",
       "      <td>55.568966</td>\n",
       "      <td>14776.0</td>\n",
       "      <td>76.720396</td>\n",
       "      <td>79.603081</td>\n",
       "      <td>73.986415</td>\n",
       "      <td>70.917126</td>\n",
       "      <td>79.605796</td>\n",
       "      <td>75.878105</td>\n",
       "      <td>71.008448</td>\n",
       "      <td>4098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48131</th>\n",
       "      <td>1793.476183</td>\n",
       "      <td>25.6</td>\n",
       "      <td>10.6</td>\n",
       "      <td>17864.0</td>\n",
       "      <td>32.9</td>\n",
       "      <td>17.8</td>\n",
       "      <td>26.1</td>\n",
       "      <td>26.2</td>\n",
       "      <td>15.8</td>\n",
       "      <td>90.9</td>\n",
       "      <td>...</td>\n",
       "      <td>77.899678</td>\n",
       "      <td>19853.0</td>\n",
       "      <td>79.125428</td>\n",
       "      <td>78.895880</td>\n",
       "      <td>76.629575</td>\n",
       "      <td>60.576045</td>\n",
       "      <td>73.670302</td>\n",
       "      <td>64.571017</td>\n",
       "      <td>68.007770</td>\n",
       "      <td>11355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48505</th>\n",
       "      <td>998.411980</td>\n",
       "      <td>39.5</td>\n",
       "      <td>11.0</td>\n",
       "      <td>17228.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>13.9</td>\n",
       "      <td>33.6</td>\n",
       "      <td>17.3</td>\n",
       "      <td>17.1</td>\n",
       "      <td>95.7</td>\n",
       "      <td>...</td>\n",
       "      <td>86.586509</td>\n",
       "      <td>16007.0</td>\n",
       "      <td>79.355639</td>\n",
       "      <td>79.572483</td>\n",
       "      <td>74.378252</td>\n",
       "      <td>77.443239</td>\n",
       "      <td>76.386871</td>\n",
       "      <td>74.001471</td>\n",
       "      <td>73.609838</td>\n",
       "      <td>14369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48507</th>\n",
       "      <td>1297.406535</td>\n",
       "      <td>34.8</td>\n",
       "      <td>8.4</td>\n",
       "      <td>13350.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>13.7</td>\n",
       "      <td>29.5</td>\n",
       "      <td>23.3</td>\n",
       "      <td>16.1</td>\n",
       "      <td>95.0</td>\n",
       "      <td>...</td>\n",
       "      <td>88.785822</td>\n",
       "      <td>13393.0</td>\n",
       "      <td>78.392216</td>\n",
       "      <td>76.024682</td>\n",
       "      <td>75.848196</td>\n",
       "      <td>76.967659</td>\n",
       "      <td>77.303576</td>\n",
       "      <td>70.010162</td>\n",
       "      <td>71.121990</td>\n",
       "      <td>12131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48247</th>\n",
       "      <td>1136.172598</td>\n",
       "      <td>27.4</td>\n",
       "      <td>11.5</td>\n",
       "      <td>17798.0</td>\n",
       "      <td>25.8</td>\n",
       "      <td>17.6</td>\n",
       "      <td>31.4</td>\n",
       "      <td>26.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>93.0</td>\n",
       "      <td>...</td>\n",
       "      <td>82.727613</td>\n",
       "      <td>16637.0</td>\n",
       "      <td>80.157542</td>\n",
       "      <td>79.708963</td>\n",
       "      <td>74.399849</td>\n",
       "      <td>74.024835</td>\n",
       "      <td>77.564505</td>\n",
       "      <td>72.456881</td>\n",
       "      <td>70.160988</td>\n",
       "      <td>5282</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         AREA_SQMI  EP_POV  EP_UNEMP   EP_PCI  EP_NOHSDP  EP_AGE65  EP_AGE17  \\\n",
       "FIPS                                                                           \n",
       "1001    594.443459    15.4       4.2  29372.0       11.3      14.6      24.2   \n",
       "1009    644.830460    14.4       4.1  22656.0       19.8      17.8      23.4   \n",
       "1013    776.838201    23.5       6.7  20430.0       15.4      19.0      22.8   \n",
       "1015    605.867251    18.6       8.8  24706.0       15.9      16.8      21.9   \n",
       "1017    596.560643    16.6       5.0  22827.0       18.6      18.9      20.7   \n",
       "...            ...     ...       ...      ...        ...       ...       ...   \n",
       "48229  4570.523160    28.2       8.3  14190.0       46.1      15.6      23.9   \n",
       "48131  1793.476183    25.6      10.6  17864.0       32.9      17.8      26.1   \n",
       "48505   998.411980    39.5      11.0  17228.0       40.0      13.9      33.6   \n",
       "48507  1297.406535    34.8       8.4  13350.0       38.0      13.7      29.5   \n",
       "48247  1136.172598    27.4      11.5  17798.0       25.8      17.6      31.4   \n",
       "\n",
       "       EP_DISABL  EP_SNGPNT  EP_MINRTY  ...  Hopefulness  Income Per Capita  \\\n",
       "FIPS                                    ...                                   \n",
       "1001        19.3        7.5       25.0  ...    91.163142            26168.0   \n",
       "1009        14.2        7.0       12.9  ...    79.492703            21033.0   \n",
       "1013        17.7       10.5       48.1  ...    83.523765            19011.0   \n",
       "1015        20.8       10.4       27.5  ...    83.365608            22231.0   \n",
       "1017        16.7        9.7       44.2  ...    85.371517            21532.0   \n",
       "...          ...        ...        ...  ...          ...                ...   \n",
       "48229       27.2        8.0       82.4  ...    55.568966            14776.0   \n",
       "48131       26.2       15.8       90.9  ...    77.899678            19853.0   \n",
       "48505       17.3       17.1       95.7  ...    86.586509            16007.0   \n",
       "48507       23.3       16.1       95.0  ...    88.785822            13393.0   \n",
       "48247       26.0       15.2       93.0  ...    82.727613            16637.0   \n",
       "\n",
       "       Neuroticism   Openness  Religiosity  Risk Taking  Selflessness  \\\n",
       "FIPS                                                                    \n",
       "1001     77.925476  78.222354    91.106719    53.333333     82.142857   \n",
       "1009     78.764620  78.193105    92.045455    57.603815     79.307632   \n",
       "1013     78.563680  76.109761    76.623924    69.058104     79.956648   \n",
       "1015     79.439032  79.955121    77.918741    54.063568     76.745724   \n",
       "1017     76.995358  78.156771    75.891100    67.343775     79.128558   \n",
       "...            ...        ...          ...          ...           ...   \n",
       "48229    76.720396  79.603081    73.986415    70.917126     79.605796   \n",
       "48131    79.125428  78.895880    76.629575    60.576045     73.670302   \n",
       "48505    79.355639  79.572483    74.378252    77.443239     76.386871   \n",
       "48507    78.392216  76.024682    75.848196    76.967659     77.303576   \n",
       "48247    80.157542  79.708963    74.399849    74.024835     77.564505   \n",
       "\n",
       "       Tolerance  Work Ethic  E_TOTPOP  \n",
       "FIPS                                    \n",
       "1001   70.000000   60.380952     55200  \n",
       "1009   64.953288   76.000000     57645  \n",
       "1013   67.920284   72.773953     20025  \n",
       "1015   67.456150   68.292794    115098  \n",
       "1017   66.397785   69.554441     33826  \n",
       "...          ...         ...       ...  \n",
       "48229  75.878105   71.008448      4098  \n",
       "48131  64.571017   68.007770     11355  \n",
       "48505  74.001471   73.609838     14369  \n",
       "48507  70.010162   71.121990     12131  \n",
       "48247  72.456881   70.160988      5282  \n",
       "\n",
       "[3000 rows x 65 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop counties with 0 cases \n",
    "## these seem to be errors- mostly in Utah, some counties with large populations\n",
    "zeros = cases_df.loc[cases_df['first_yr_deaths']==0]\n",
    "cases_df = cases_df.drop(index = zeros.index)\n",
    "cases_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "106bfab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FIPS\n",
       "1001    0.179348\n",
       "1009    0.227253\n",
       "1013    0.329588\n",
       "1015    0.264992\n",
       "1017    0.345888\n",
       "Name: case_pct, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create variable for case% for each counties population\n",
    "cases_df['case_pct'] = cases_df['first_yr_deaths']/cases_df['E_TOTPOP']*100\n",
    "cases_df['case_pct'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c977cd6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3000.000000\n",
       "mean        0.191211\n",
       "std         0.106248\n",
       "min         0.001401\n",
       "25%         0.115764\n",
       "50%         0.176377\n",
       "75%         0.243764\n",
       "max         0.788566\n",
       "Name: case_pct, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cases_df['case_pct'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73aa10ce",
   "metadata": {},
   "source": [
    "## RF Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7384c664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 66)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cases_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8242a445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature set (x)\n",
    "X = cases_df.drop(['first_yr_deaths','case_pct'], axis=1).values\n",
    "\n",
    "#Define (y)\n",
    "y= cases_df['case_pct'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "426b3d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check columns list\n",
    "# X.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44b3e403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# frequency histogram to see how many counties are high risk and low risk\n",
    "plt.hist(cases_df[\"case_pct\"])\n",
    "plt.xlabel(\"High Risk Counties\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd26af29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data for training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3 ,random_state= 42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb794166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2100, 64)\n",
      "(900, 64)\n",
      "(2100,)\n",
      "(900,)\n"
     ]
    }
   ],
   "source": [
    "# Determine the shape of our training and testing sets.\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ab34a44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0a2871d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a StandardScaler instance.\n",
    "scaler = StandardScaler()\n",
    "# Fitting the Standard Scaler with the training data.\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scaling the data.\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8a29ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,545\n",
      "Trainable params: 12,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-17 15:36:20.642289: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Define the model - deep neural net\n",
    "number_input_features = len(X_train[0])\n",
    "hidden_nodes_layer1 = 64\n",
    "hidden_nodes_layer2 = 64\n",
    "hidden_nodes_layer3 = 64\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(\n",
    "    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\"))\n",
    "# Second hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer3, activation=\"relu\"))\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation=\"linear\"))\n",
    "\n",
    "# Check the structure of the model\n",
    "nn.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "077c762e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "nn.compile(loss=\"mean_absolute_error\", optimizer=\"adam\", metrics=[\"mean_absolute_error\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a8951c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "66/66 [==============================] - 1s 2ms/step - loss: 0.1254 - mean_absolute_error: 0.1254\n",
      "Epoch 2/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0797 - mean_absolute_error: 0.0797\n",
      "Epoch 3/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0709 - mean_absolute_error: 0.0709\n",
      "Epoch 4/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0641 - mean_absolute_error: 0.0641\n",
      "Epoch 5/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0581 - mean_absolute_error: 0.0581\n",
      "Epoch 6/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0549 - mean_absolute_error: 0.0549\n",
      "Epoch 7/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0537 - mean_absolute_error: 0.0537\n",
      "Epoch 8/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0510 - mean_absolute_error: 0.0510\n",
      "Epoch 9/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0511 - mean_absolute_error: 0.0511\n",
      "Epoch 10/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0485 - mean_absolute_error: 0.0485\n",
      "Epoch 11/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0449 - mean_absolute_error: 0.0449\n",
      "Epoch 12/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0431 - mean_absolute_error: 0.0431\n",
      "Epoch 13/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0406 - mean_absolute_error: 0.0406\n",
      "Epoch 14/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0410 - mean_absolute_error: 0.0410\n",
      "Epoch 15/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0395 - mean_absolute_error: 0.0395\n",
      "Epoch 16/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0385 - mean_absolute_error: 0.0385\n",
      "Epoch 17/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0369 - mean_absolute_error: 0.0369\n",
      "Epoch 18/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0367 - mean_absolute_error: 0.0367\n",
      "Epoch 19/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0354 - mean_absolute_error: 0.0354\n",
      "Epoch 20/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0352 - mean_absolute_error: 0.0352\n",
      "Epoch 21/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0339 - mean_absolute_error: 0.0339\n",
      "Epoch 22/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0335 - mean_absolute_error: 0.0335\n",
      "Epoch 23/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0325 - mean_absolute_error: 0.0325\n",
      "Epoch 24/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0321 - mean_absolute_error: 0.0321\n",
      "Epoch 25/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0327 - mean_absolute_error: 0.0327\n",
      "Epoch 26/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0325 - mean_absolute_error: 0.0325\n",
      "Epoch 27/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0313 - mean_absolute_error: 0.0313\n",
      "Epoch 28/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0325 - mean_absolute_error: 0.0325\n",
      "Epoch 29/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0294 - mean_absolute_error: 0.0294\n",
      "Epoch 30/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0300 - mean_absolute_error: 0.0300\n",
      "Epoch 31/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0277 - mean_absolute_error: 0.0277\n",
      "Epoch 32/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0288 - mean_absolute_error: 0.0288\n",
      "Epoch 33/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0282 - mean_absolute_error: 0.0282\n",
      "Epoch 34/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0284 - mean_absolute_error: 0.0284\n",
      "Epoch 35/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0271 - mean_absolute_error: 0.0271\n",
      "Epoch 36/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0274 - mean_absolute_error: 0.0274\n",
      "Epoch 37/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0283 - mean_absolute_error: 0.0283\n",
      "Epoch 38/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0265 - mean_absolute_error: 0.0265\n",
      "Epoch 39/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0255 - mean_absolute_error: 0.0255\n",
      "Epoch 40/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0254 - mean_absolute_error: 0.0254\n",
      "Epoch 41/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0256 - mean_absolute_error: 0.0256\n",
      "Epoch 42/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0242 - mean_absolute_error: 0.0242\n",
      "Epoch 43/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0243 - mean_absolute_error: 0.0243\n",
      "Epoch 44/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0236 - mean_absolute_error: 0.0236\n",
      "Epoch 45/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0220 - mean_absolute_error: 0.0220\n",
      "Epoch 46/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0246 - mean_absolute_error: 0.0246\n",
      "Epoch 47/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0249 - mean_absolute_error: 0.0249\n",
      "Epoch 48/300\n",
      "66/66 [==============================] - 0s 7ms/step - loss: 0.0245 - mean_absolute_error: 0.0245\n",
      "Epoch 49/300\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 0.0242 - mean_absolute_error: 0.0242\n",
      "Epoch 50/300\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 0.0223 - mean_absolute_error: 0.0223\n",
      "Epoch 51/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0231 - mean_absolute_error: 0.0231\n",
      "Epoch 52/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0215 - mean_absolute_error: 0.0215\n",
      "Epoch 53/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0226 - mean_absolute_error: 0.0226\n",
      "Epoch 54/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0222 - mean_absolute_error: 0.0222\n",
      "Epoch 55/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0216 - mean_absolute_error: 0.0216\n",
      "Epoch 56/300\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 0.0224 - mean_absolute_error: 0.0224\n",
      "Epoch 57/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0228 - mean_absolute_error: 0.0228\n",
      "Epoch 58/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0217 - mean_absolute_error: 0.0217\n",
      "Epoch 59/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0212 - mean_absolute_error: 0.0212\n",
      "Epoch 60/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0210 - mean_absolute_error: 0.0210\n",
      "Epoch 61/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0203 - mean_absolute_error: 0.0203\n",
      "Epoch 62/300\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 0.0214 - mean_absolute_error: 0.0214\n",
      "Epoch 63/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0207 - mean_absolute_error: 0.0207\n",
      "Epoch 64/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0210 - mean_absolute_error: 0.0210\n",
      "Epoch 65/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0211 - mean_absolute_error: 0.0211\n",
      "Epoch 66/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0203 - mean_absolute_error: 0.0203\n",
      "Epoch 67/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0206 - mean_absolute_error: 0.0206\n",
      "Epoch 68/300\n",
      "66/66 [==============================] - 1s 8ms/step - loss: 0.0206 - mean_absolute_error: 0.0206\n",
      "Epoch 69/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0206 - mean_absolute_error: 0.0206\n",
      "Epoch 70/300\n",
      "66/66 [==============================] - 0s 5ms/step - loss: 0.0203 - mean_absolute_error: 0.0203\n",
      "Epoch 71/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0194 - mean_absolute_error: 0.0194\n",
      "Epoch 72/300\n",
      "66/66 [==============================] - 0s 5ms/step - loss: 0.0190 - mean_absolute_error: 0.0190\n",
      "Epoch 73/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0183 - mean_absolute_error: 0.0183\n",
      "Epoch 74/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0187 - mean_absolute_error: 0.0187\n",
      "Epoch 75/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0184 - mean_absolute_error: 0.0184\n",
      "Epoch 76/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0184 - mean_absolute_error: 0.0184\n",
      "Epoch 77/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0178 - mean_absolute_error: 0.0178\n",
      "Epoch 78/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0190 - mean_absolute_error: 0.0190\n",
      "Epoch 79/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0186 - mean_absolute_error: 0.0186\n",
      "Epoch 80/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0190 - mean_absolute_error: 0.0190\n",
      "Epoch 81/300\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 0.0204 - mean_absolute_error: 0.0204\n",
      "Epoch 82/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0182 - mean_absolute_error: 0.0182\n",
      "Epoch 83/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0180 - mean_absolute_error: 0.0180\n",
      "Epoch 84/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0175 - mean_absolute_error: 0.0175\n",
      "Epoch 85/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0176 - mean_absolute_error: 0.0176\n",
      "Epoch 86/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0170 - mean_absolute_error: 0.0170\n",
      "Epoch 87/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0180 - mean_absolute_error: 0.0180\n",
      "Epoch 88/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0174 - mean_absolute_error: 0.0174\n",
      "Epoch 89/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0169 - mean_absolute_error: 0.0169\n",
      "Epoch 90/300\n",
      "66/66 [==============================] - 0s 5ms/step - loss: 0.0171 - mean_absolute_error: 0.0171\n",
      "Epoch 91/300\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 0.0168 - mean_absolute_error: 0.0168\n",
      "Epoch 92/300\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 0.0168 - mean_absolute_error: 0.0168\n",
      "Epoch 93/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0181 - mean_absolute_error: 0.0181\n",
      "Epoch 94/300\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 0.0183 - mean_absolute_error: 0.0183\n",
      "Epoch 95/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0165 - mean_absolute_error: 0.0165\n",
      "Epoch 96/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0178 - mean_absolute_error: 0.0178\n",
      "Epoch 97/300\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 0.0168 - mean_absolute_error: 0.0168\n",
      "Epoch 98/300\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 0.0166 - mean_absolute_error: 0.0166\n",
      "Epoch 99/300\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 0.0166 - mean_absolute_error: 0.0166\n",
      "Epoch 100/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0163 - mean_absolute_error: 0.0163\n",
      "Epoch 101/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0165 - mean_absolute_error: 0.0165\n",
      "Epoch 102/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0168 - mean_absolute_error: 0.0168\n",
      "Epoch 103/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0156 - mean_absolute_error: 0.0156\n",
      "Epoch 104/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0168 - mean_absolute_error: 0.0168\n",
      "Epoch 105/300\n",
      "66/66 [==============================] - 0s 6ms/step - loss: 0.0165 - mean_absolute_error: 0.0165\n",
      "Epoch 106/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0165 - mean_absolute_error: 0.0165\n",
      "Epoch 107/300\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 0.0166 - mean_absolute_error: 0.0166\n",
      "Epoch 108/300\n",
      "66/66 [==============================] - 0s 5ms/step - loss: 0.0161 - mean_absolute_error: 0.0161\n",
      "Epoch 109/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0160 - mean_absolute_error: 0.0160\n",
      "Epoch 110/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0158 - mean_absolute_error: 0.0158\n",
      "Epoch 111/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0159 - mean_absolute_error: 0.0159\n",
      "Epoch 112/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0164 - mean_absolute_error: 0.0164\n",
      "Epoch 113/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0154 - mean_absolute_error: 0.0154\n",
      "Epoch 114/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0160 - mean_absolute_error: 0.0160\n",
      "Epoch 115/300\n",
      "66/66 [==============================] - 0s 5ms/step - loss: 0.0157 - mean_absolute_error: 0.0157\n",
      "Epoch 116/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0160 - mean_absolute_error: 0.0160\n",
      "Epoch 117/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0151 - mean_absolute_error: 0.0151\n",
      "Epoch 118/300\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 0.0154 - mean_absolute_error: 0.0154\n",
      "Epoch 119/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0160 - mean_absolute_error: 0.0160\n",
      "Epoch 120/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0148 - mean_absolute_error: 0.0148\n",
      "Epoch 121/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0161 - mean_absolute_error: 0.0161\n",
      "Epoch 122/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0149 - mean_absolute_error: 0.0149\n",
      "Epoch 123/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0164 - mean_absolute_error: 0.0164\n",
      "Epoch 124/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0153 - mean_absolute_error: 0.0153\n",
      "Epoch 125/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0148 - mean_absolute_error: 0.0148\n",
      "Epoch 126/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0156 - mean_absolute_error: 0.0156\n",
      "Epoch 127/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0153 - mean_absolute_error: 0.0153\n",
      "Epoch 128/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0164 - mean_absolute_error: 0.0164\n",
      "Epoch 129/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0157 - mean_absolute_error: 0.0157\n",
      "Epoch 130/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0149 - mean_absolute_error: 0.0149\n",
      "Epoch 131/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0146 - mean_absolute_error: 0.0146\n",
      "Epoch 132/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0150 - mean_absolute_error: 0.0150\n",
      "Epoch 133/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0153 - mean_absolute_error: 0.0153\n",
      "Epoch 134/300\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 0.0149 - mean_absolute_error: 0.0149\n",
      "Epoch 135/300\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 0.0147 - mean_absolute_error: 0.0147\n",
      "Epoch 136/300\n",
      "66/66 [==============================] - 0s 5ms/step - loss: 0.0149 - mean_absolute_error: 0.0149\n",
      "Epoch 137/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0145 - mean_absolute_error: 0.0145\n",
      "Epoch 138/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0141 - mean_absolute_error: 0.0141\n",
      "Epoch 139/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0141 - mean_absolute_error: 0.0141\n",
      "Epoch 140/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0147 - mean_absolute_error: 0.0147\n",
      "Epoch 141/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0145 - mean_absolute_error: 0.0145\n",
      "Epoch 142/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0144 - mean_absolute_error: 0.0144\n",
      "Epoch 143/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0149 - mean_absolute_error: 0.0149\n",
      "Epoch 144/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0157 - mean_absolute_error: 0.0157\n",
      "Epoch 145/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0141 - mean_absolute_error: 0.0141\n",
      "Epoch 146/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0139 - mean_absolute_error: 0.0139\n",
      "Epoch 147/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0139 - mean_absolute_error: 0.0139\n",
      "Epoch 148/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0142 - mean_absolute_error: 0.0142\n",
      "Epoch 149/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0145 - mean_absolute_error: 0.0145\n",
      "Epoch 150/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0137 - mean_absolute_error: 0.0137\n",
      "Epoch 151/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0147 - mean_absolute_error: 0.0147\n",
      "Epoch 152/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0143 - mean_absolute_error: 0.0143\n",
      "Epoch 153/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0149 - mean_absolute_error: 0.0149\n",
      "Epoch 154/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0146 - mean_absolute_error: 0.0146\n",
      "Epoch 155/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0145 - mean_absolute_error: 0.0145\n",
      "Epoch 156/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0134 - mean_absolute_error: 0.0134\n",
      "Epoch 157/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0144 - mean_absolute_error: 0.0144\n",
      "Epoch 158/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0137 - mean_absolute_error: 0.0137\n",
      "Epoch 159/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0134 - mean_absolute_error: 0.0134\n",
      "Epoch 160/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0140 - mean_absolute_error: 0.0140\n",
      "Epoch 161/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0136 - mean_absolute_error: 0.0136\n",
      "Epoch 162/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0142 - mean_absolute_error: 0.0142\n",
      "Epoch 163/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0144 - mean_absolute_error: 0.0144\n",
      "Epoch 164/300\n",
      "66/66 [==============================] - 0s 5ms/step - loss: 0.0137 - mean_absolute_error: 0.0137\n",
      "Epoch 165/300\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 0.0135 - mean_absolute_error: 0.0135\n",
      "Epoch 166/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0132 - mean_absolute_error: 0.0132\n",
      "Epoch 167/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0135 - mean_absolute_error: 0.0135\n",
      "Epoch 168/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0139 - mean_absolute_error: 0.0139\n",
      "Epoch 169/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0136 - mean_absolute_error: 0.0136\n",
      "Epoch 170/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0130 - mean_absolute_error: 0.0130\n",
      "Epoch 171/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0134 - mean_absolute_error: 0.0134\n",
      "Epoch 172/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0132 - mean_absolute_error: 0.0132\n",
      "Epoch 173/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0132 - mean_absolute_error: 0.0132\n",
      "Epoch 174/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0124 - mean_absolute_error: 0.0124\n",
      "Epoch 175/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0133 - mean_absolute_error: 0.0133\n",
      "Epoch 176/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0131 - mean_absolute_error: 0.0131\n",
      "Epoch 177/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0132 - mean_absolute_error: 0.0132\n",
      "Epoch 178/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0128 - mean_absolute_error: 0.0128\n",
      "Epoch 179/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0138 - mean_absolute_error: 0.0138\n",
      "Epoch 180/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0137 - mean_absolute_error: 0.0137\n",
      "Epoch 181/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0132 - mean_absolute_error: 0.0132\n",
      "Epoch 182/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0126 - mean_absolute_error: 0.0126\n",
      "Epoch 183/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0133 - mean_absolute_error: 0.0133\n",
      "Epoch 184/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0123 - mean_absolute_error: 0.0123\n",
      "Epoch 185/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0131 - mean_absolute_error: 0.0131\n",
      "Epoch 186/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0127 - mean_absolute_error: 0.0127\n",
      "Epoch 187/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0127 - mean_absolute_error: 0.0127\n",
      "Epoch 188/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0131 - mean_absolute_error: 0.0131\n",
      "Epoch 189/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0122 - mean_absolute_error: 0.0122\n",
      "Epoch 190/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0128 - mean_absolute_error: 0.0128\n",
      "Epoch 191/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0123 - mean_absolute_error: 0.0123\n",
      "Epoch 192/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0121 - mean_absolute_error: 0.0121\n",
      "Epoch 193/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0134 - mean_absolute_error: 0.0134\n",
      "Epoch 194/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0128 - mean_absolute_error: 0.0128\n",
      "Epoch 195/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0130 - mean_absolute_error: 0.0130\n",
      "Epoch 196/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0124 - mean_absolute_error: 0.0124\n",
      "Epoch 197/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0127 - mean_absolute_error: 0.0127\n",
      "Epoch 198/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0118 - mean_absolute_error: 0.0118\n",
      "Epoch 199/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0123 - mean_absolute_error: 0.0123\n",
      "Epoch 200/300\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 0.0119 - mean_absolute_error: 0.0119\n",
      "Epoch 201/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0122 - mean_absolute_error: 0.0122\n",
      "Epoch 202/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0121 - mean_absolute_error: 0.0121\n",
      "Epoch 203/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0132 - mean_absolute_error: 0.0132\n",
      "Epoch 204/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0124 - mean_absolute_error: 0.0124\n",
      "Epoch 205/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0130 - mean_absolute_error: 0.0130\n",
      "Epoch 206/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0138 - mean_absolute_error: 0.0138\n",
      "Epoch 207/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0123 - mean_absolute_error: 0.0123\n",
      "Epoch 208/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0120 - mean_absolute_error: 0.0120\n",
      "Epoch 209/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0122 - mean_absolute_error: 0.0122\n",
      "Epoch 210/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0119 - mean_absolute_error: 0.0119\n",
      "Epoch 211/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0124 - mean_absolute_error: 0.0124\n",
      "Epoch 212/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0122 - mean_absolute_error: 0.0122\n",
      "Epoch 213/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0125 - mean_absolute_error: 0.0125\n",
      "Epoch 214/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0126 - mean_absolute_error: 0.0126\n",
      "Epoch 215/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0124 - mean_absolute_error: 0.0124\n",
      "Epoch 216/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0115 - mean_absolute_error: 0.0115\n",
      "Epoch 217/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0124 - mean_absolute_error: 0.0124\n",
      "Epoch 218/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0124 - mean_absolute_error: 0.0124\n",
      "Epoch 219/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0127 - mean_absolute_error: 0.0127\n",
      "Epoch 220/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0118 - mean_absolute_error: 0.0118\n",
      "Epoch 221/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0119 - mean_absolute_error: 0.0119\n",
      "Epoch 222/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0118 - mean_absolute_error: 0.0118\n",
      "Epoch 223/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0121 - mean_absolute_error: 0.0121\n",
      "Epoch 224/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0119 - mean_absolute_error: 0.0119\n",
      "Epoch 225/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0127 - mean_absolute_error: 0.0127\n",
      "Epoch 226/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0115 - mean_absolute_error: 0.0115\n",
      "Epoch 227/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0114 - mean_absolute_error: 0.0114\n",
      "Epoch 228/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0121 - mean_absolute_error: 0.0121\n",
      "Epoch 229/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0117 - mean_absolute_error: 0.0117\n",
      "Epoch 230/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0120 - mean_absolute_error: 0.0120\n",
      "Epoch 231/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0118 - mean_absolute_error: 0.0118\n",
      "Epoch 232/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0116 - mean_absolute_error: 0.0116\n",
      "Epoch 233/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0118 - mean_absolute_error: 0.0118\n",
      "Epoch 234/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0119 - mean_absolute_error: 0.0119\n",
      "Epoch 235/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0115 - mean_absolute_error: 0.0115\n",
      "Epoch 236/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0112 - mean_absolute_error: 0.0112\n",
      "Epoch 237/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0112 - mean_absolute_error: 0.0112\n",
      "Epoch 238/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0132 - mean_absolute_error: 0.0132\n",
      "Epoch 239/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0116 - mean_absolute_error: 0.0116\n",
      "Epoch 240/300\n",
      "66/66 [==============================] - 0s 5ms/step - loss: 0.0109 - mean_absolute_error: 0.0109\n",
      "Epoch 241/300\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 0.0117 - mean_absolute_error: 0.0117\n",
      "Epoch 242/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0121 - mean_absolute_error: 0.0121\n",
      "Epoch 243/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0112 - mean_absolute_error: 0.0112\n",
      "Epoch 244/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0114 - mean_absolute_error: 0.0114\n",
      "Epoch 245/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0106 - mean_absolute_error: 0.0106\n",
      "Epoch 246/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0114 - mean_absolute_error: 0.0114\n",
      "Epoch 247/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0112 - mean_absolute_error: 0.0112\n",
      "Epoch 248/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0117 - mean_absolute_error: 0.0117\n",
      "Epoch 249/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0117 - mean_absolute_error: 0.0117\n",
      "Epoch 250/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0118 - mean_absolute_error: 0.0118\n",
      "Epoch 251/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0109 - mean_absolute_error: 0.0109\n",
      "Epoch 252/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0110 - mean_absolute_error: 0.0110\n",
      "Epoch 253/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0120 - mean_absolute_error: 0.0120\n",
      "Epoch 254/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0111 - mean_absolute_error: 0.0111\n",
      "Epoch 255/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0117 - mean_absolute_error: 0.0117\n",
      "Epoch 256/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0116 - mean_absolute_error: 0.0116\n",
      "Epoch 257/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0114 - mean_absolute_error: 0.0114\n",
      "Epoch 258/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0110 - mean_absolute_error: 0.0110\n",
      "Epoch 259/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0107 - mean_absolute_error: 0.0107\n",
      "Epoch 260/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0111 - mean_absolute_error: 0.0111\n",
      "Epoch 261/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0103 - mean_absolute_error: 0.0103\n",
      "Epoch 262/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0109 - mean_absolute_error: 0.0109\n",
      "Epoch 263/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0114 - mean_absolute_error: 0.0114\n",
      "Epoch 264/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0113 - mean_absolute_error: 0.0113\n",
      "Epoch 265/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0108 - mean_absolute_error: 0.0108\n",
      "Epoch 266/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0107 - mean_absolute_error: 0.0107\n",
      "Epoch 267/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0112 - mean_absolute_error: 0.0112\n",
      "Epoch 268/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0111 - mean_absolute_error: 0.0111\n",
      "Epoch 269/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0109 - mean_absolute_error: 0.0109\n",
      "Epoch 270/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0110 - mean_absolute_error: 0.0110\n",
      "Epoch 271/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0107 - mean_absolute_error: 0.0107\n",
      "Epoch 272/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0106 - mean_absolute_error: 0.0106\n",
      "Epoch 273/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0110 - mean_absolute_error: 0.0110\n",
      "Epoch 274/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0105 - mean_absolute_error: 0.0105\n",
      "Epoch 275/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0107 - mean_absolute_error: 0.0107\n",
      "Epoch 276/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0098 - mean_absolute_error: 0.0098\n",
      "Epoch 277/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0107 - mean_absolute_error: 0.0107\n",
      "Epoch 278/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0105 - mean_absolute_error: 0.0105\n",
      "Epoch 279/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0104 - mean_absolute_error: 0.0104\n",
      "Epoch 280/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0105 - mean_absolute_error: 0.0105\n",
      "Epoch 281/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0105 - mean_absolute_error: 0.0105\n",
      "Epoch 282/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0104 - mean_absolute_error: 0.0104\n",
      "Epoch 283/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0100 - mean_absolute_error: 0.0100\n",
      "Epoch 284/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0107 - mean_absolute_error: 0.0107\n",
      "Epoch 285/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0105 - mean_absolute_error: 0.0105\n",
      "Epoch 286/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0100 - mean_absolute_error: 0.0100\n",
      "Epoch 287/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0101 - mean_absolute_error: 0.0101\n",
      "Epoch 288/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0106 - mean_absolute_error: 0.0106\n",
      "Epoch 289/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0109 - mean_absolute_error: 0.0109\n",
      "Epoch 290/300\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0103 - mean_absolute_error: 0.0103\n",
      "Epoch 291/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0108 - mean_absolute_error: 0.0108\n",
      "Epoch 292/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0104 - mean_absolute_error: 0.0104\n",
      "Epoch 293/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0103 - mean_absolute_error: 0.0103\n",
      "Epoch 294/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0097 - mean_absolute_error: 0.0097\n",
      "Epoch 295/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0099 - mean_absolute_error: 0.0099\n",
      "Epoch 296/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0101 - mean_absolute_error: 0.0101\n",
      "Epoch 297/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0099 - mean_absolute_error: 0.0099\n",
      "Epoch 298/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0098 - mean_absolute_error: 0.0098\n",
      "Epoch 299/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0101 - mean_absolute_error: 0.0101\n",
      "Epoch 300/300\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0102 - mean_absolute_error: 0.0102\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "fit_model = nn.fit(X_train_scaled,y_train,epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c394d261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 - 0s - loss: 0.0713 - mean_absolute_error: 0.0713 - 221ms/epoch - 8ms/step\n",
      "Loss: 0.07130537182092667, Accuracy: 0.07130537182092667\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65045f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>pred_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.487243</td>\n",
       "      <td>690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.455276</td>\n",
       "      <td>317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.418882</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.418490</td>\n",
       "      <td>580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.416480</td>\n",
       "      <td>637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>0.040331</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>0.039444</td>\n",
       "      <td>261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>0.036312</td>\n",
       "      <td>469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>0.019476</td>\n",
       "      <td>638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>0.013982</td>\n",
       "      <td>564</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         pred  pred_index\n",
       "0    0.487243         690\n",
       "1    0.455276         317\n",
       "2    0.418882         168\n",
       "3    0.418490         580\n",
       "4    0.416480         637\n",
       "..        ...         ...\n",
       "895  0.040331         164\n",
       "896  0.039444         261\n",
       "897  0.036312         469\n",
       "898  0.019476         638\n",
       "899  0.013982         564\n",
       "\n",
       "[900 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# turn model prediction into dataframe\n",
    "preds = pd.DataFrame(nn.predict(X_test_scaled), columns = ['pred'])\n",
    "preds['pred_index'] = preds.index\n",
    "preds.sort_values(by = ['pred'], ascending = False, ignore_index = True, inplace = True)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e18f381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>act</th>\n",
       "      <th>act_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.788566</td>\n",
       "      <td>566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.725468</td>\n",
       "      <td>436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.714116</td>\n",
       "      <td>239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.643863</td>\n",
       "      <td>321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.633220</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>0.020049</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>0.016108</td>\n",
       "      <td>537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>0.014004</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>0.007670</td>\n",
       "      <td>395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>0.001401</td>\n",
       "      <td>649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          act  act_index\n",
       "0    0.788566        566\n",
       "1    0.725468        436\n",
       "2    0.714116        239\n",
       "3    0.643863        321\n",
       "4    0.633220        110\n",
       "..        ...        ...\n",
       "895  0.020049        221\n",
       "896  0.016108        537\n",
       "897  0.014004         70\n",
       "898  0.007670        395\n",
       "899  0.001401        649\n",
       "\n",
       "[900 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# turn y_test into dataframe\n",
    "actuals = pd.DataFrame(y_test, columns = ['act'])\n",
    "actuals['act_index'] = actuals.index\n",
    "actuals.sort_values(by = ['act'],ascending = False, ignore_index = True, inplace =True)\n",
    "actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "442b1b05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>act</th>\n",
       "      <th>act_index</th>\n",
       "      <th>pred</th>\n",
       "      <th>pred_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.788566</td>\n",
       "      <td>566</td>\n",
       "      <td>0.487243</td>\n",
       "      <td>690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.725468</td>\n",
       "      <td>436</td>\n",
       "      <td>0.455276</td>\n",
       "      <td>317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.714116</td>\n",
       "      <td>239</td>\n",
       "      <td>0.418882</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.643863</td>\n",
       "      <td>321</td>\n",
       "      <td>0.418490</td>\n",
       "      <td>580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.633220</td>\n",
       "      <td>110</td>\n",
       "      <td>0.416480</td>\n",
       "      <td>637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.337477</td>\n",
       "      <td>528</td>\n",
       "      <td>0.295535</td>\n",
       "      <td>311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.336937</td>\n",
       "      <td>493</td>\n",
       "      <td>0.294931</td>\n",
       "      <td>476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.335092</td>\n",
       "      <td>358</td>\n",
       "      <td>0.294838</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.334303</td>\n",
       "      <td>547</td>\n",
       "      <td>0.294001</td>\n",
       "      <td>559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.333056</td>\n",
       "      <td>117</td>\n",
       "      <td>0.293128</td>\n",
       "      <td>223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         act  act_index      pred  pred_index\n",
       "0   0.788566        566  0.487243         690\n",
       "1   0.725468        436  0.455276         317\n",
       "2   0.714116        239  0.418882         168\n",
       "3   0.643863        321  0.418490         580\n",
       "4   0.633220        110  0.416480         637\n",
       "..       ...        ...       ...         ...\n",
       "86  0.337477        528  0.295535         311\n",
       "87  0.336937        493  0.294931         476\n",
       "88  0.335092        358  0.294838          97\n",
       "89  0.334303        547  0.294001         559\n",
       "90  0.333056        117  0.293128         223\n",
       "\n",
       "[91 rows x 4 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge predictions and actuals into dataframe\n",
    "results = actuals.merge(preds, how = 'inner', left_index=True, right_index=True)\n",
    "ranked = results.head(91)\n",
    "ranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "96f4e93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "correct_index = []\n",
    "\n",
    "for index, row in ranked.iterrows():\n",
    "    if row['pred_index'] in ranked['act_index'].values:\n",
    "        correct = correct+1\n",
    "        correct_index.append(row['pred_index'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b5d65c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2f03e8ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>act</th>\n",
       "      <th>act_index</th>\n",
       "      <th>pred</th>\n",
       "      <th>pred_index</th>\n",
       "      <th>error</th>\n",
       "      <th>abs_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.788566</td>\n",
       "      <td>566</td>\n",
       "      <td>0.487243</td>\n",
       "      <td>690</td>\n",
       "      <td>0.301322</td>\n",
       "      <td>0.301322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.725468</td>\n",
       "      <td>436</td>\n",
       "      <td>0.455276</td>\n",
       "      <td>317</td>\n",
       "      <td>0.270192</td>\n",
       "      <td>0.270192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.714116</td>\n",
       "      <td>239</td>\n",
       "      <td>0.418882</td>\n",
       "      <td>168</td>\n",
       "      <td>0.295234</td>\n",
       "      <td>0.295234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.643863</td>\n",
       "      <td>321</td>\n",
       "      <td>0.418490</td>\n",
       "      <td>580</td>\n",
       "      <td>0.225373</td>\n",
       "      <td>0.225373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.633220</td>\n",
       "      <td>110</td>\n",
       "      <td>0.416480</td>\n",
       "      <td>637</td>\n",
       "      <td>0.216740</td>\n",
       "      <td>0.216740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>0.020049</td>\n",
       "      <td>221</td>\n",
       "      <td>0.040331</td>\n",
       "      <td>164</td>\n",
       "      <td>-0.020282</td>\n",
       "      <td>0.020282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>0.016108</td>\n",
       "      <td>537</td>\n",
       "      <td>0.039444</td>\n",
       "      <td>261</td>\n",
       "      <td>-0.023336</td>\n",
       "      <td>0.023336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>0.014004</td>\n",
       "      <td>70</td>\n",
       "      <td>0.036312</td>\n",
       "      <td>469</td>\n",
       "      <td>-0.022308</td>\n",
       "      <td>0.022308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>0.007670</td>\n",
       "      <td>395</td>\n",
       "      <td>0.019476</td>\n",
       "      <td>638</td>\n",
       "      <td>-0.011806</td>\n",
       "      <td>0.011806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>0.001401</td>\n",
       "      <td>649</td>\n",
       "      <td>0.013982</td>\n",
       "      <td>564</td>\n",
       "      <td>-0.012581</td>\n",
       "      <td>0.012581</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          act  act_index      pred  pred_index     error  abs_error\n",
       "0    0.788566        566  0.487243         690  0.301322   0.301322\n",
       "1    0.725468        436  0.455276         317  0.270192   0.270192\n",
       "2    0.714116        239  0.418882         168  0.295234   0.295234\n",
       "3    0.643863        321  0.418490         580  0.225373   0.225373\n",
       "4    0.633220        110  0.416480         637  0.216740   0.216740\n",
       "..        ...        ...       ...         ...       ...        ...\n",
       "895  0.020049        221  0.040331         164 -0.020282   0.020282\n",
       "896  0.016108        537  0.039444         261 -0.023336   0.023336\n",
       "897  0.014004         70  0.036312         469 -0.022308   0.022308\n",
       "898  0.007670        395  0.019476         638 -0.011806   0.011806\n",
       "899  0.001401        649  0.013982         564 -0.012581   0.012581\n",
       "\n",
       "[900 rows x 6 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check how big the errors are on each prediction\n",
    "results['error'] = results['act']-results['pred']\n",
    "results['abs_error'] = abs(results['error'])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "98f83f3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>act</th>\n",
       "      <th>act_index</th>\n",
       "      <th>pred</th>\n",
       "      <th>pred_index</th>\n",
       "      <th>error</th>\n",
       "      <th>abs_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001401</td>\n",
       "      <td>649</td>\n",
       "      <td>0.013982</td>\n",
       "      <td>564</td>\n",
       "      <td>-0.012581</td>\n",
       "      <td>0.012581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.007670</td>\n",
       "      <td>395</td>\n",
       "      <td>0.019476</td>\n",
       "      <td>638</td>\n",
       "      <td>-0.011806</td>\n",
       "      <td>0.011806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.014004</td>\n",
       "      <td>70</td>\n",
       "      <td>0.036312</td>\n",
       "      <td>469</td>\n",
       "      <td>-0.022308</td>\n",
       "      <td>0.022308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.016108</td>\n",
       "      <td>537</td>\n",
       "      <td>0.039444</td>\n",
       "      <td>261</td>\n",
       "      <td>-0.023336</td>\n",
       "      <td>0.023336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.020049</td>\n",
       "      <td>221</td>\n",
       "      <td>0.040331</td>\n",
       "      <td>164</td>\n",
       "      <td>-0.020282</td>\n",
       "      <td>0.020282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>0.633220</td>\n",
       "      <td>110</td>\n",
       "      <td>0.416480</td>\n",
       "      <td>637</td>\n",
       "      <td>0.216740</td>\n",
       "      <td>0.216740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>0.643863</td>\n",
       "      <td>321</td>\n",
       "      <td>0.418490</td>\n",
       "      <td>580</td>\n",
       "      <td>0.225373</td>\n",
       "      <td>0.225373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>0.714116</td>\n",
       "      <td>239</td>\n",
       "      <td>0.418882</td>\n",
       "      <td>168</td>\n",
       "      <td>0.295234</td>\n",
       "      <td>0.295234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>0.725468</td>\n",
       "      <td>436</td>\n",
       "      <td>0.455276</td>\n",
       "      <td>317</td>\n",
       "      <td>0.270192</td>\n",
       "      <td>0.270192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>0.788566</td>\n",
       "      <td>566</td>\n",
       "      <td>0.487243</td>\n",
       "      <td>690</td>\n",
       "      <td>0.301322</td>\n",
       "      <td>0.301322</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          act  act_index      pred  pred_index     error  abs_error\n",
       "0    0.001401        649  0.013982         564 -0.012581   0.012581\n",
       "1    0.007670        395  0.019476         638 -0.011806   0.011806\n",
       "2    0.014004         70  0.036312         469 -0.022308   0.022308\n",
       "3    0.016108        537  0.039444         261 -0.023336   0.023336\n",
       "4    0.020049        221  0.040331         164 -0.020282   0.020282\n",
       "..        ...        ...       ...         ...       ...        ...\n",
       "895  0.633220        110  0.416480         637  0.216740   0.216740\n",
       "896  0.643863        321  0.418490         580  0.225373   0.225373\n",
       "897  0.714116        239  0.418882         168  0.295234   0.295234\n",
       "898  0.725468        436  0.455276         317  0.270192   0.270192\n",
       "899  0.788566        566  0.487243         690  0.301322   0.301322\n",
       "\n",
       "[900 rows x 6 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sort_values(by= ['act'], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a751ba9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fc33ad5a250>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot the errors\n",
    "plt.scatter(results.index,results['error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4be18a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    900.000000\n",
       "mean       0.005343\n",
       "std        0.036721\n",
       "min       -0.035958\n",
       "25%       -0.018593\n",
       "50%       -0.003497\n",
       "75%        0.012169\n",
       "max        0.301322\n",
       "Name: error, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explore errors\n",
    "results['error'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4044bf68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    900.000000\n",
       "mean       0.022309\n",
       "std        0.029644\n",
       "min        0.000197\n",
       "25%        0.008131\n",
       "50%        0.016669\n",
       "75%        0.024396\n",
       "max        0.301322\n",
       "Name: abs_error, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#explor absolute error\n",
    "results['abs_error'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "66910f0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    900.000000\n",
       "mean       0.194321\n",
       "std        0.110548\n",
       "min        0.001401\n",
       "25%        0.115567\n",
       "50%        0.176487\n",
       "75%        0.247742\n",
       "max        0.788566\n",
       "Name: act, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#explore case percents\n",
    "results['act'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f9ffe0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
