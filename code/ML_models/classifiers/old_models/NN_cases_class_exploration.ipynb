{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c130683c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dependencies\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from config import db_password\n",
    "import psycopg2\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5b2f2df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIPS</th>\n",
       "      <th>AREA_SQMI</th>\n",
       "      <th>E_TOTPOP</th>\n",
       "      <th>E_HU</th>\n",
       "      <th>E_HH</th>\n",
       "      <th>E_POV</th>\n",
       "      <th>E_UNEMP</th>\n",
       "      <th>E_PCI</th>\n",
       "      <th>E_NOHSDP</th>\n",
       "      <th>E_AGE65</th>\n",
       "      <th>...</th>\n",
       "      <th>Hopefulness</th>\n",
       "      <th>Income Per Capita</th>\n",
       "      <th>Neuroticism</th>\n",
       "      <th>Openness</th>\n",
       "      <th>Religiosity</th>\n",
       "      <th>Risk Taking</th>\n",
       "      <th>Selflessness</th>\n",
       "      <th>Tolerance</th>\n",
       "      <th>Work Ethic</th>\n",
       "      <th>first_yr_cases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>594.443459</td>\n",
       "      <td>55200</td>\n",
       "      <td>23315</td>\n",
       "      <td>21115</td>\n",
       "      <td>8422</td>\n",
       "      <td>1065</td>\n",
       "      <td>29372</td>\n",
       "      <td>4204</td>\n",
       "      <td>8050</td>\n",
       "      <td>...</td>\n",
       "      <td>91.163142</td>\n",
       "      <td>26168.0</td>\n",
       "      <td>77.925476</td>\n",
       "      <td>78.222354</td>\n",
       "      <td>91.106719</td>\n",
       "      <td>53.333333</td>\n",
       "      <td>82.142857</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>60.380952</td>\n",
       "      <td>6589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1009</td>\n",
       "      <td>644.830460</td>\n",
       "      <td>57645</td>\n",
       "      <td>24222</td>\n",
       "      <td>20600</td>\n",
       "      <td>8220</td>\n",
       "      <td>909</td>\n",
       "      <td>22656</td>\n",
       "      <td>7861</td>\n",
       "      <td>10233</td>\n",
       "      <td>...</td>\n",
       "      <td>79.492703</td>\n",
       "      <td>21033.0</td>\n",
       "      <td>78.764620</td>\n",
       "      <td>78.193105</td>\n",
       "      <td>92.045455</td>\n",
       "      <td>57.603815</td>\n",
       "      <td>79.307632</td>\n",
       "      <td>64.953288</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>6444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1013</td>\n",
       "      <td>776.838201</td>\n",
       "      <td>20025</td>\n",
       "      <td>10026</td>\n",
       "      <td>6708</td>\n",
       "      <td>4640</td>\n",
       "      <td>567</td>\n",
       "      <td>20430</td>\n",
       "      <td>2141</td>\n",
       "      <td>3806</td>\n",
       "      <td>...</td>\n",
       "      <td>83.523765</td>\n",
       "      <td>19011.0</td>\n",
       "      <td>78.563680</td>\n",
       "      <td>76.109761</td>\n",
       "      <td>76.623924</td>\n",
       "      <td>69.058104</td>\n",
       "      <td>79.956648</td>\n",
       "      <td>67.920284</td>\n",
       "      <td>72.773953</td>\n",
       "      <td>2097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1015</td>\n",
       "      <td>605.867251</td>\n",
       "      <td>115098</td>\n",
       "      <td>53682</td>\n",
       "      <td>45033</td>\n",
       "      <td>20819</td>\n",
       "      <td>4628</td>\n",
       "      <td>24706</td>\n",
       "      <td>12620</td>\n",
       "      <td>19386</td>\n",
       "      <td>...</td>\n",
       "      <td>83.365608</td>\n",
       "      <td>22231.0</td>\n",
       "      <td>79.439032</td>\n",
       "      <td>79.955121</td>\n",
       "      <td>77.918741</td>\n",
       "      <td>54.063568</td>\n",
       "      <td>76.745724</td>\n",
       "      <td>67.456150</td>\n",
       "      <td>68.292794</td>\n",
       "      <td>14224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1017</td>\n",
       "      <td>596.560643</td>\n",
       "      <td>33826</td>\n",
       "      <td>16981</td>\n",
       "      <td>13516</td>\n",
       "      <td>5531</td>\n",
       "      <td>773</td>\n",
       "      <td>22827</td>\n",
       "      <td>4383</td>\n",
       "      <td>6409</td>\n",
       "      <td>...</td>\n",
       "      <td>85.371517</td>\n",
       "      <td>21532.0</td>\n",
       "      <td>76.995358</td>\n",
       "      <td>78.156771</td>\n",
       "      <td>75.891100</td>\n",
       "      <td>67.343775</td>\n",
       "      <td>79.128558</td>\n",
       "      <td>66.397785</td>\n",
       "      <td>69.554441</td>\n",
       "      <td>3488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3053</th>\n",
       "      <td>48229</td>\n",
       "      <td>4570.523160</td>\n",
       "      <td>4098</td>\n",
       "      <td>1562</td>\n",
       "      <td>900</td>\n",
       "      <td>951</td>\n",
       "      <td>101</td>\n",
       "      <td>14190</td>\n",
       "      <td>1263</td>\n",
       "      <td>639</td>\n",
       "      <td>...</td>\n",
       "      <td>55.568966</td>\n",
       "      <td>14776.0</td>\n",
       "      <td>76.720396</td>\n",
       "      <td>79.603081</td>\n",
       "      <td>73.986415</td>\n",
       "      <td>70.917126</td>\n",
       "      <td>79.605796</td>\n",
       "      <td>75.878105</td>\n",
       "      <td>71.008448</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3054</th>\n",
       "      <td>48131</td>\n",
       "      <td>1793.476183</td>\n",
       "      <td>11355</td>\n",
       "      <td>5592</td>\n",
       "      <td>3511</td>\n",
       "      <td>2751</td>\n",
       "      <td>482</td>\n",
       "      <td>17864</td>\n",
       "      <td>2386</td>\n",
       "      <td>2025</td>\n",
       "      <td>...</td>\n",
       "      <td>77.899678</td>\n",
       "      <td>19853.0</td>\n",
       "      <td>79.125428</td>\n",
       "      <td>78.895880</td>\n",
       "      <td>76.629575</td>\n",
       "      <td>60.576045</td>\n",
       "      <td>73.670302</td>\n",
       "      <td>64.571017</td>\n",
       "      <td>68.007770</td>\n",
       "      <td>1214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3055</th>\n",
       "      <td>48505</td>\n",
       "      <td>998.411980</td>\n",
       "      <td>14369</td>\n",
       "      <td>6388</td>\n",
       "      <td>4405</td>\n",
       "      <td>5609</td>\n",
       "      <td>621</td>\n",
       "      <td>17228</td>\n",
       "      <td>3226</td>\n",
       "      <td>1999</td>\n",
       "      <td>...</td>\n",
       "      <td>86.586509</td>\n",
       "      <td>16007.0</td>\n",
       "      <td>79.355639</td>\n",
       "      <td>79.572483</td>\n",
       "      <td>74.378252</td>\n",
       "      <td>77.443239</td>\n",
       "      <td>76.386871</td>\n",
       "      <td>74.001471</td>\n",
       "      <td>73.609838</td>\n",
       "      <td>1760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3056</th>\n",
       "      <td>48507</td>\n",
       "      <td>1297.406535</td>\n",
       "      <td>12131</td>\n",
       "      <td>4344</td>\n",
       "      <td>3509</td>\n",
       "      <td>4150</td>\n",
       "      <td>421</td>\n",
       "      <td>13350</td>\n",
       "      <td>2719</td>\n",
       "      <td>1665</td>\n",
       "      <td>...</td>\n",
       "      <td>88.785822</td>\n",
       "      <td>13393.0</td>\n",
       "      <td>78.392216</td>\n",
       "      <td>76.024682</td>\n",
       "      <td>75.848196</td>\n",
       "      <td>76.967659</td>\n",
       "      <td>77.303576</td>\n",
       "      <td>70.010162</td>\n",
       "      <td>71.121990</td>\n",
       "      <td>1844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3057</th>\n",
       "      <td>48247</td>\n",
       "      <td>1136.172598</td>\n",
       "      <td>5282</td>\n",
       "      <td>2523</td>\n",
       "      <td>1710</td>\n",
       "      <td>1431</td>\n",
       "      <td>229</td>\n",
       "      <td>17798</td>\n",
       "      <td>812</td>\n",
       "      <td>932</td>\n",
       "      <td>...</td>\n",
       "      <td>82.727613</td>\n",
       "      <td>16637.0</td>\n",
       "      <td>80.157542</td>\n",
       "      <td>79.708963</td>\n",
       "      <td>74.399849</td>\n",
       "      <td>74.024835</td>\n",
       "      <td>77.564505</td>\n",
       "      <td>72.456881</td>\n",
       "      <td>70.160988</td>\n",
       "      <td>605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3058 rows × 105 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       FIPS    AREA_SQMI  E_TOTPOP   E_HU   E_HH  E_POV  E_UNEMP  E_PCI  \\\n",
       "0      1001   594.443459     55200  23315  21115   8422     1065  29372   \n",
       "1      1009   644.830460     57645  24222  20600   8220      909  22656   \n",
       "2      1013   776.838201     20025  10026   6708   4640      567  20430   \n",
       "3      1015   605.867251    115098  53682  45033  20819     4628  24706   \n",
       "4      1017   596.560643     33826  16981  13516   5531      773  22827   \n",
       "...     ...          ...       ...    ...    ...    ...      ...    ...   \n",
       "3053  48229  4570.523160      4098   1562    900    951      101  14190   \n",
       "3054  48131  1793.476183     11355   5592   3511   2751      482  17864   \n",
       "3055  48505   998.411980     14369   6388   4405   5609      621  17228   \n",
       "3056  48507  1297.406535     12131   4344   3509   4150      421  13350   \n",
       "3057  48247  1136.172598      5282   2523   1710   1431      229  17798   \n",
       "\n",
       "      E_NOHSDP  E_AGE65  ...  Hopefulness  Income Per Capita  Neuroticism  \\\n",
       "0         4204     8050  ...    91.163142            26168.0    77.925476   \n",
       "1         7861    10233  ...    79.492703            21033.0    78.764620   \n",
       "2         2141     3806  ...    83.523765            19011.0    78.563680   \n",
       "3        12620    19386  ...    83.365608            22231.0    79.439032   \n",
       "4         4383     6409  ...    85.371517            21532.0    76.995358   \n",
       "...        ...      ...  ...          ...                ...          ...   \n",
       "3053      1263      639  ...    55.568966            14776.0    76.720396   \n",
       "3054      2386     2025  ...    77.899678            19853.0    79.125428   \n",
       "3055      3226     1999  ...    86.586509            16007.0    79.355639   \n",
       "3056      2719     1665  ...    88.785822            13393.0    78.392216   \n",
       "3057       812      932  ...    82.727613            16637.0    80.157542   \n",
       "\n",
       "       Openness  Religiosity  Risk Taking  Selflessness  Tolerance  \\\n",
       "0     78.222354    91.106719    53.333333     82.142857  70.000000   \n",
       "1     78.193105    92.045455    57.603815     79.307632  64.953288   \n",
       "2     76.109761    76.623924    69.058104     79.956648  67.920284   \n",
       "3     79.955121    77.918741    54.063568     76.745724  67.456150   \n",
       "4     78.156771    75.891100    67.343775     79.128558  66.397785   \n",
       "...         ...          ...          ...           ...        ...   \n",
       "3053  79.603081    73.986415    70.917126     79.605796  75.878105   \n",
       "3054  78.895880    76.629575    60.576045     73.670302  64.571017   \n",
       "3055  79.572483    74.378252    77.443239     76.386871  74.001471   \n",
       "3056  76.024682    75.848196    76.967659     77.303576  70.010162   \n",
       "3057  79.708963    74.399849    74.024835     77.564505  72.456881   \n",
       "\n",
       "      Work Ethic  first_yr_cases  \n",
       "0      60.380952            6589  \n",
       "1      76.000000            6444  \n",
       "2      72.773953            2097  \n",
       "3      68.292794           14224  \n",
       "4      69.554441            3488  \n",
       "...          ...             ...  \n",
       "3053   71.008448             512  \n",
       "3054   68.007770            1214  \n",
       "3055   73.609838            1760  \n",
       "3056   71.121990            1844  \n",
       "3057   70.160988             605  \n",
       "\n",
       "[3058 rows x 105 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create engine and connect to AWS RDS\n",
    "db_string = f\"postgresql://postgres:{db_password}@capstone-db.cutxgn80t57o.us-west-1.rds.amazonaws.com\"\n",
    "engine = create_engine(db_string)\n",
    "# read and check merged cases table\n",
    "cases_df = pd.read_sql('cases_merged_full', con = engine)\n",
    "cases_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3182e6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set index to FIPS\n",
    "ccases_df = cases_df.set_index(cases_df['FIPS'])\n",
    "cases_df= cases_df.drop(columns = ['FIPS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b64c46df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    11.936594\n",
       "1    11.178767\n",
       "2    10.471910\n",
       "3    12.358164\n",
       "4    10.311595\n",
       "Name: case_pct, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create variable for case% for each counties population\n",
    "cases_df['case_pct'] = cases_df['first_yr_cases']/cases_df['E_TOTPOP']*100\n",
    "cases_df['case_pct'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78c3a477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3058.000000\n",
       "mean        9.426600\n",
       "std         3.045809\n",
       "min         0.000000\n",
       "25%         7.713422\n",
       "50%         9.466675\n",
       "75%        11.176131\n",
       "max        38.010657\n",
       "Name: case_pct, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cases_df['case_pct'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cd247c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       high\n",
       "1        low\n",
       "2        low\n",
       "3       high\n",
       "4        low\n",
       "        ... \n",
       "3053    high\n",
       "3054     low\n",
       "3055    high\n",
       "3056    high\n",
       "3057     low\n",
       "Name: case_class, Length: 3058, dtype: category\n",
       "Categories (2, object): ['low' < 'high']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bin and cut the case_pct column into 2 classifications\n",
    "q = cases_df['case_pct'].quantile(.8)\n",
    "bins = [0, q , 40]\n",
    "labels = ['low','high']\n",
    "cases_df['case_class'] = pd.cut(cases_df['case_pct'], bins, labels = labels)\n",
    "cases_df['case_class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0da9118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "low     2422\n",
       "high     612\n",
       "Name: case_class, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cases_df['case_class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0b8f650",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop unneeded columns\n",
    "cases_df = cases_df.drop('case_pct', axis =1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "664b3ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AREA_SQMI</th>\n",
       "      <th>E_TOTPOP</th>\n",
       "      <th>E_HU</th>\n",
       "      <th>E_HH</th>\n",
       "      <th>E_POV</th>\n",
       "      <th>E_UNEMP</th>\n",
       "      <th>E_PCI</th>\n",
       "      <th>E_NOHSDP</th>\n",
       "      <th>E_AGE65</th>\n",
       "      <th>E_AGE17</th>\n",
       "      <th>...</th>\n",
       "      <th>Neuroticism</th>\n",
       "      <th>Openness</th>\n",
       "      <th>Religiosity</th>\n",
       "      <th>Risk Taking</th>\n",
       "      <th>Selflessness</th>\n",
       "      <th>Tolerance</th>\n",
       "      <th>Work Ethic</th>\n",
       "      <th>first_yr_cases</th>\n",
       "      <th>case_class_low</th>\n",
       "      <th>case_class_high</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>594.443459</td>\n",
       "      <td>55200</td>\n",
       "      <td>23315</td>\n",
       "      <td>21115</td>\n",
       "      <td>8422</td>\n",
       "      <td>1065</td>\n",
       "      <td>29372</td>\n",
       "      <td>4204</td>\n",
       "      <td>8050</td>\n",
       "      <td>13369</td>\n",
       "      <td>...</td>\n",
       "      <td>77.925476</td>\n",
       "      <td>78.222354</td>\n",
       "      <td>91.106719</td>\n",
       "      <td>53.333333</td>\n",
       "      <td>82.142857</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>60.380952</td>\n",
       "      <td>6589</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>644.830460</td>\n",
       "      <td>57645</td>\n",
       "      <td>24222</td>\n",
       "      <td>20600</td>\n",
       "      <td>8220</td>\n",
       "      <td>909</td>\n",
       "      <td>22656</td>\n",
       "      <td>7861</td>\n",
       "      <td>10233</td>\n",
       "      <td>13468</td>\n",
       "      <td>...</td>\n",
       "      <td>78.764620</td>\n",
       "      <td>78.193105</td>\n",
       "      <td>92.045455</td>\n",
       "      <td>57.603815</td>\n",
       "      <td>79.307632</td>\n",
       "      <td>64.953288</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>6444</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>776.838201</td>\n",
       "      <td>20025</td>\n",
       "      <td>10026</td>\n",
       "      <td>6708</td>\n",
       "      <td>4640</td>\n",
       "      <td>567</td>\n",
       "      <td>20430</td>\n",
       "      <td>2141</td>\n",
       "      <td>3806</td>\n",
       "      <td>4566</td>\n",
       "      <td>...</td>\n",
       "      <td>78.563680</td>\n",
       "      <td>76.109761</td>\n",
       "      <td>76.623924</td>\n",
       "      <td>69.058104</td>\n",
       "      <td>79.956648</td>\n",
       "      <td>67.920284</td>\n",
       "      <td>72.773953</td>\n",
       "      <td>2097</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>605.867251</td>\n",
       "      <td>115098</td>\n",
       "      <td>53682</td>\n",
       "      <td>45033</td>\n",
       "      <td>20819</td>\n",
       "      <td>4628</td>\n",
       "      <td>24706</td>\n",
       "      <td>12620</td>\n",
       "      <td>19386</td>\n",
       "      <td>25196</td>\n",
       "      <td>...</td>\n",
       "      <td>79.439032</td>\n",
       "      <td>79.955121</td>\n",
       "      <td>77.918741</td>\n",
       "      <td>54.063568</td>\n",
       "      <td>76.745724</td>\n",
       "      <td>67.456150</td>\n",
       "      <td>68.292794</td>\n",
       "      <td>14224</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>596.560643</td>\n",
       "      <td>33826</td>\n",
       "      <td>16981</td>\n",
       "      <td>13516</td>\n",
       "      <td>5531</td>\n",
       "      <td>773</td>\n",
       "      <td>22827</td>\n",
       "      <td>4383</td>\n",
       "      <td>6409</td>\n",
       "      <td>7006</td>\n",
       "      <td>...</td>\n",
       "      <td>76.995358</td>\n",
       "      <td>78.156771</td>\n",
       "      <td>75.891100</td>\n",
       "      <td>67.343775</td>\n",
       "      <td>79.128558</td>\n",
       "      <td>66.397785</td>\n",
       "      <td>69.554441</td>\n",
       "      <td>3488</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3053</th>\n",
       "      <td>4570.523160</td>\n",
       "      <td>4098</td>\n",
       "      <td>1562</td>\n",
       "      <td>900</td>\n",
       "      <td>951</td>\n",
       "      <td>101</td>\n",
       "      <td>14190</td>\n",
       "      <td>1263</td>\n",
       "      <td>639</td>\n",
       "      <td>980</td>\n",
       "      <td>...</td>\n",
       "      <td>76.720396</td>\n",
       "      <td>79.603081</td>\n",
       "      <td>73.986415</td>\n",
       "      <td>70.917126</td>\n",
       "      <td>79.605796</td>\n",
       "      <td>75.878105</td>\n",
       "      <td>71.008448</td>\n",
       "      <td>512</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3054</th>\n",
       "      <td>1793.476183</td>\n",
       "      <td>11355</td>\n",
       "      <td>5592</td>\n",
       "      <td>3511</td>\n",
       "      <td>2751</td>\n",
       "      <td>482</td>\n",
       "      <td>17864</td>\n",
       "      <td>2386</td>\n",
       "      <td>2025</td>\n",
       "      <td>2962</td>\n",
       "      <td>...</td>\n",
       "      <td>79.125428</td>\n",
       "      <td>78.895880</td>\n",
       "      <td>76.629575</td>\n",
       "      <td>60.576045</td>\n",
       "      <td>73.670302</td>\n",
       "      <td>64.571017</td>\n",
       "      <td>68.007770</td>\n",
       "      <td>1214</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3055</th>\n",
       "      <td>998.411980</td>\n",
       "      <td>14369</td>\n",
       "      <td>6388</td>\n",
       "      <td>4405</td>\n",
       "      <td>5609</td>\n",
       "      <td>621</td>\n",
       "      <td>17228</td>\n",
       "      <td>3226</td>\n",
       "      <td>1999</td>\n",
       "      <td>4835</td>\n",
       "      <td>...</td>\n",
       "      <td>79.355639</td>\n",
       "      <td>79.572483</td>\n",
       "      <td>74.378252</td>\n",
       "      <td>77.443239</td>\n",
       "      <td>76.386871</td>\n",
       "      <td>74.001471</td>\n",
       "      <td>73.609838</td>\n",
       "      <td>1760</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3056</th>\n",
       "      <td>1297.406535</td>\n",
       "      <td>12131</td>\n",
       "      <td>4344</td>\n",
       "      <td>3509</td>\n",
       "      <td>4150</td>\n",
       "      <td>421</td>\n",
       "      <td>13350</td>\n",
       "      <td>2719</td>\n",
       "      <td>1665</td>\n",
       "      <td>3583</td>\n",
       "      <td>...</td>\n",
       "      <td>78.392216</td>\n",
       "      <td>76.024682</td>\n",
       "      <td>75.848196</td>\n",
       "      <td>76.967659</td>\n",
       "      <td>77.303576</td>\n",
       "      <td>70.010162</td>\n",
       "      <td>71.121990</td>\n",
       "      <td>1844</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3057</th>\n",
       "      <td>1136.172598</td>\n",
       "      <td>5282</td>\n",
       "      <td>2523</td>\n",
       "      <td>1710</td>\n",
       "      <td>1431</td>\n",
       "      <td>229</td>\n",
       "      <td>17798</td>\n",
       "      <td>812</td>\n",
       "      <td>932</td>\n",
       "      <td>1659</td>\n",
       "      <td>...</td>\n",
       "      <td>80.157542</td>\n",
       "      <td>79.708963</td>\n",
       "      <td>74.399849</td>\n",
       "      <td>74.024835</td>\n",
       "      <td>77.564505</td>\n",
       "      <td>72.456881</td>\n",
       "      <td>70.160988</td>\n",
       "      <td>605</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3058 rows × 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        AREA_SQMI  E_TOTPOP   E_HU   E_HH  E_POV  E_UNEMP  E_PCI  E_NOHSDP  \\\n",
       "0      594.443459     55200  23315  21115   8422     1065  29372      4204   \n",
       "1      644.830460     57645  24222  20600   8220      909  22656      7861   \n",
       "2      776.838201     20025  10026   6708   4640      567  20430      2141   \n",
       "3      605.867251    115098  53682  45033  20819     4628  24706     12620   \n",
       "4      596.560643     33826  16981  13516   5531      773  22827      4383   \n",
       "...           ...       ...    ...    ...    ...      ...    ...       ...   \n",
       "3053  4570.523160      4098   1562    900    951      101  14190      1263   \n",
       "3054  1793.476183     11355   5592   3511   2751      482  17864      2386   \n",
       "3055   998.411980     14369   6388   4405   5609      621  17228      3226   \n",
       "3056  1297.406535     12131   4344   3509   4150      421  13350      2719   \n",
       "3057  1136.172598      5282   2523   1710   1431      229  17798       812   \n",
       "\n",
       "      E_AGE65  E_AGE17  ...  Neuroticism   Openness  Religiosity  Risk Taking  \\\n",
       "0        8050    13369  ...    77.925476  78.222354    91.106719    53.333333   \n",
       "1       10233    13468  ...    78.764620  78.193105    92.045455    57.603815   \n",
       "2        3806     4566  ...    78.563680  76.109761    76.623924    69.058104   \n",
       "3       19386    25196  ...    79.439032  79.955121    77.918741    54.063568   \n",
       "4        6409     7006  ...    76.995358  78.156771    75.891100    67.343775   \n",
       "...       ...      ...  ...          ...        ...          ...          ...   \n",
       "3053      639      980  ...    76.720396  79.603081    73.986415    70.917126   \n",
       "3054     2025     2962  ...    79.125428  78.895880    76.629575    60.576045   \n",
       "3055     1999     4835  ...    79.355639  79.572483    74.378252    77.443239   \n",
       "3056     1665     3583  ...    78.392216  76.024682    75.848196    76.967659   \n",
       "3057      932     1659  ...    80.157542  79.708963    74.399849    74.024835   \n",
       "\n",
       "      Selflessness  Tolerance  Work Ethic  first_yr_cases  case_class_low  \\\n",
       "0        82.142857  70.000000   60.380952            6589               0   \n",
       "1        79.307632  64.953288   76.000000            6444               1   \n",
       "2        79.956648  67.920284   72.773953            2097               1   \n",
       "3        76.745724  67.456150   68.292794           14224               0   \n",
       "4        79.128558  66.397785   69.554441            3488               1   \n",
       "...            ...        ...         ...             ...             ...   \n",
       "3053     79.605796  75.878105   71.008448             512               0   \n",
       "3054     73.670302  64.571017   68.007770            1214               1   \n",
       "3055     76.386871  74.001471   73.609838            1760               0   \n",
       "3056     77.303576  70.010162   71.121990            1844               0   \n",
       "3057     77.564505  72.456881   70.160988             605               1   \n",
       "\n",
       "      case_class_high  \n",
       "0                   1  \n",
       "1                   0  \n",
       "2                   0  \n",
       "3                   1  \n",
       "4                   0  \n",
       "...               ...  \n",
       "3053                1  \n",
       "3054                0  \n",
       "3055                1  \n",
       "3056                1  \n",
       "3057                0  \n",
       "\n",
       "[3058 rows x 106 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#turn case % classifications into binary \n",
    "cases_df = pd.get_dummies(cases_df, columns = ['case_class'])\n",
    "cases_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfb69257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2446\n",
       "1     612\n",
       "Name: case_class_high, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cases_df['case_class_high'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3dc4e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperate targets and features\n",
    "## should i drop the number of cases?\n",
    "X = cases_df.drop(columns = ['case_class_high','case_class_low','first_yr_cases']).values\n",
    "y=cases_df['case_class_high'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe3d3924",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a2d4ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3cb693cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3de65d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 100)               10400     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 80)                8080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 81        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18,561\n",
      "Trainable params: 18,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-14 11:59:11.860514: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Define the model - deep neural net\n",
    "number_input_features = len(X_train[0])\n",
    "hidden_nodes_layer1 = 100\n",
    "hidden_nodes_layer2 = 80\n",
    "# hidden_nodes_layer1 = 100\n",
    "# hidden_nodes_layer2 = 80\n",
    "# hidden_nodes_layer3 = 80\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(\n",
    "    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\")\n",
    ")\n",
    "\n",
    "# Second hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "# nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer3, activation=\"relu\"))\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Check the structure of the model\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a209d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a670e197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "72/72 [==============================] - 1s 2ms/step - loss: 0.5010 - accuracy: 0.7737\n",
      "Epoch 2/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.4074 - accuracy: 0.8177\n",
      "Epoch 3/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.3802 - accuracy: 0.8325\n",
      "Epoch 4/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.3534 - accuracy: 0.8509\n",
      "Epoch 5/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3335 - accuracy: 0.8591\n",
      "Epoch 6/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3181 - accuracy: 0.8670\n",
      "Epoch 7/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2998 - accuracy: 0.8722\n",
      "Epoch 8/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2743 - accuracy: 0.8884\n",
      "Epoch 9/150\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.2666 - accuracy: 0.8940\n",
      "Epoch 10/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.2461 - accuracy: 0.8993\n",
      "Epoch 11/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2266 - accuracy: 0.9058\n",
      "Epoch 12/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2153 - accuracy: 0.9176\n",
      "Epoch 13/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1995 - accuracy: 0.9232\n",
      "Epoch 14/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1899 - accuracy: 0.9328\n",
      "Epoch 15/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1709 - accuracy: 0.9459\n",
      "Epoch 16/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1590 - accuracy: 0.9455\n",
      "Epoch 17/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1396 - accuracy: 0.9551\n",
      "Epoch 18/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1270 - accuracy: 0.9599\n",
      "Epoch 19/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1237 - accuracy: 0.9599\n",
      "Epoch 20/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.1065 - accuracy: 0.9677\n",
      "Epoch 21/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0888 - accuracy: 0.9760\n",
      "Epoch 22/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0844 - accuracy: 0.9765\n",
      "Epoch 23/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0714 - accuracy: 0.9808\n",
      "Epoch 24/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0722 - accuracy: 0.9830\n",
      "Epoch 25/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.0532 - accuracy: 0.9913\n",
      "Epoch 26/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0502 - accuracy: 0.9900\n",
      "Epoch 27/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.0413 - accuracy: 0.9939\n",
      "Epoch 28/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0367 - accuracy: 0.9956\n",
      "Epoch 29/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0304 - accuracy: 0.9965\n",
      "Epoch 30/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0280 - accuracy: 0.9965\n",
      "Epoch 31/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.0250 - accuracy: 0.9987\n",
      "Epoch 32/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0214 - accuracy: 0.9983\n",
      "Epoch 33/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0205 - accuracy: 0.9987\n",
      "Epoch 34/150\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0173 - accuracy: 0.9987\n",
      "Epoch 35/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0158 - accuracy: 0.9987\n",
      "Epoch 36/150\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0127 - accuracy: 0.9991\n",
      "Epoch 37/150\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0120 - accuracy: 0.9996\n",
      "Epoch 38/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0093 - accuracy: 1.0000\n",
      "Epoch 39/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0083 - accuracy: 0.9996\n",
      "Epoch 40/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.0073 - accuracy: 1.0000\n",
      "Epoch 41/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 42/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0064 - accuracy: 1.0000\n",
      "Epoch 43/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 44/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 45/150\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 46/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.0782 - accuracy: 0.9843\n",
      "Epoch 47/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.2003 - accuracy: 0.9551\n",
      "Epoch 48/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0528 - accuracy: 0.9839\n",
      "Epoch 49/150\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0183 - accuracy: 0.9978\n",
      "Epoch 50/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.0112 - accuracy: 0.9996\n",
      "Epoch 51/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.0073 - accuracy: 1.0000\n",
      "Epoch 52/150\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0055 - accuracy: 1.0000\n",
      "Epoch 53/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 54/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 55/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 56/150\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 57/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 58/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 59/150\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 60/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 61/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 62/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 63/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 64/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 65/150\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 66/150\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 67/150\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 68/150\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 69/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 70/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 71/150\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 72/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 73/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 74/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 75/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 9.3821e-04 - accuracy: 1.0000\n",
      "Epoch 76/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 8.9661e-04 - accuracy: 1.0000\n",
      "Epoch 77/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 8.4408e-04 - accuracy: 1.0000\n",
      "Epoch 78/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 7.9752e-04 - accuracy: 1.0000\n",
      "Epoch 79/150\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 7.6638e-04 - accuracy: 1.0000\n",
      "Epoch 80/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 7.2253e-04 - accuracy: 1.0000\n",
      "Epoch 81/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 6.7589e-04 - accuracy: 1.0000\n",
      "Epoch 82/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72 [==============================] - 0s 4ms/step - loss: 6.5373e-04 - accuracy: 1.0000\n",
      "Epoch 83/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 6.3929e-04 - accuracy: 1.0000\n",
      "Epoch 84/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 5.8588e-04 - accuracy: 1.0000\n",
      "Epoch 85/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 5.8521e-04 - accuracy: 1.0000\n",
      "Epoch 86/150\n",
      "72/72 [==============================] - 0s 7ms/step - loss: 5.2943e-04 - accuracy: 1.0000\n",
      "Epoch 87/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 5.0489e-04 - accuracy: 1.0000\n",
      "Epoch 88/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 4.8139e-04 - accuracy: 1.0000\n",
      "Epoch 89/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 4.5843e-04 - accuracy: 1.0000\n",
      "Epoch 90/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 4.2421e-04 - accuracy: 1.0000\n",
      "Epoch 91/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 4.0807e-04 - accuracy: 1.0000\n",
      "Epoch 92/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 4.0254e-04 - accuracy: 1.0000\n",
      "Epoch 93/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 3.6861e-04 - accuracy: 1.0000\n",
      "Epoch 94/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3.5251e-04 - accuracy: 1.0000\n",
      "Epoch 95/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3.3236e-04 - accuracy: 1.0000\n",
      "Epoch 96/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3.2875e-04 - accuracy: 1.0000\n",
      "Epoch 97/150\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 3.0126e-04 - accuracy: 1.0000\n",
      "Epoch 98/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2.8847e-04 - accuracy: 1.0000\n",
      "Epoch 99/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2.7701e-04 - accuracy: 1.0000\n",
      "Epoch 100/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2.6698e-04 - accuracy: 1.0000\n",
      "Epoch 101/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2.5697e-04 - accuracy: 1.0000\n",
      "Epoch 102/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2.3637e-04 - accuracy: 1.0000\n",
      "Epoch 103/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2.3081e-04 - accuracy: 1.0000\n",
      "Epoch 104/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2.2122e-04 - accuracy: 1.0000\n",
      "Epoch 105/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2.0949e-04 - accuracy: 1.0000\n",
      "Epoch 106/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2.0148e-04 - accuracy: 1.0000\n",
      "Epoch 107/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2.0304e-04 - accuracy: 1.0000\n",
      "Epoch 108/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.8673e-04 - accuracy: 1.0000\n",
      "Epoch 109/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.7285e-04 - accuracy: 1.0000\n",
      "Epoch 110/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1.6654e-04 - accuracy: 1.0000\n",
      "Epoch 111/150\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1.6083e-04 - accuracy: 1.0000\n",
      "Epoch 112/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.5339e-04 - accuracy: 1.0000\n",
      "Epoch 113/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.4130e-04 - accuracy: 1.0000\n",
      "Epoch 114/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.3633e-04 - accuracy: 1.0000\n",
      "Epoch 115/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.3172e-04 - accuracy: 1.0000\n",
      "Epoch 116/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.2736e-04 - accuracy: 1.0000\n",
      "Epoch 117/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.1856e-04 - accuracy: 1.0000\n",
      "Epoch 118/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.1620e-04 - accuracy: 1.0000\n",
      "Epoch 119/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.1033e-04 - accuracy: 1.0000\n",
      "Epoch 120/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.0612e-04 - accuracy: 1.0000\n",
      "Epoch 121/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1.0263e-04 - accuracy: 1.0000\n",
      "Epoch 122/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 9.6712e-05 - accuracy: 1.0000\n",
      "Epoch 123/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 9.0905e-05 - accuracy: 1.0000\n",
      "Epoch 124/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 8.8463e-05 - accuracy: 1.0000\n",
      "Epoch 125/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 8.3739e-05 - accuracy: 1.0000\n",
      "Epoch 126/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 8.0173e-05 - accuracy: 1.0000\n",
      "Epoch 127/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 7.7787e-05 - accuracy: 1.0000\n",
      "Epoch 128/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 7.4668e-05 - accuracy: 1.0000\n",
      "Epoch 129/150\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 7.0323e-05 - accuracy: 1.0000\n",
      "Epoch 130/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 6.6288e-05 - accuracy: 1.0000\n",
      "Epoch 131/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 6.5233e-05 - accuracy: 1.0000\n",
      "Epoch 132/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 6.2646e-05 - accuracy: 1.0000\n",
      "Epoch 133/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 6.0715e-05 - accuracy: 1.0000\n",
      "Epoch 134/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 5.6289e-05 - accuracy: 1.0000\n",
      "Epoch 135/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 5.4236e-05 - accuracy: 1.0000\n",
      "Epoch 136/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 5.1477e-05 - accuracy: 1.0000\n",
      "Epoch 137/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 4.9022e-05 - accuracy: 1.0000\n",
      "Epoch 138/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 4.7133e-05 - accuracy: 1.0000\n",
      "Epoch 139/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 4.4420e-05 - accuracy: 1.0000\n",
      "Epoch 140/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 4.2430e-05 - accuracy: 1.0000\n",
      "Epoch 141/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 4.1132e-05 - accuracy: 1.0000\n",
      "Epoch 142/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 4.1289e-05 - accuracy: 1.0000\n",
      "Epoch 143/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 3.7825e-05 - accuracy: 1.0000\n",
      "Epoch 144/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 3.6930e-05 - accuracy: 1.0000\n",
      "Epoch 145/150\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 3.5435e-05 - accuracy: 1.0000\n",
      "Epoch 146/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 3.4073e-05 - accuracy: 1.0000\n",
      "Epoch 147/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 3.0954e-05 - accuracy: 1.0000\n",
      "Epoch 148/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 3.0204e-05 - accuracy: 1.0000\n",
      "Epoch 149/150\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2.8958e-05 - accuracy: 1.0000\n",
      "Epoch 150/150\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2.6778e-05 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "fit_model = nn.fit(X_train_scaled,y_train,epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d122ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 - 0s - loss: 2.8825 - accuracy: 0.7725 - 261ms/epoch - 11ms/step\n",
      "Loss: 2.882539987564087, Accuracy: 0.772549033164978\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "581f5f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 1s 7ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       "array([[602,   6],\n",
       "       [154,   3]], dtype=int32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check confusion matrix\n",
    "y_pred=nn.predict(X_test_scaled)\n",
    "con_mat = tf.math.confusion_matrix(labels=y_test, predictions=y_pred)\n",
    "con_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b8dd727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.save('saved_models/aug_10_reduced_features_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0076b64c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
